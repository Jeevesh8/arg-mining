{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "long_context_am.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMZXB5agFbbM4cG+I2rrDOU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/arg_mining/blob/main/experiments/long_context_am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGdooHmfgd-"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GPY8HXWtkyE"
      },
      "source": [
        "%%capture\n",
        "#if running on colab, install below 4\n",
        "#!git clone https://github.com/Jeevesh8/arg_mining\n",
        "#!pip install transformers\n",
        "#!pip install seqeval datasets allennlp\n",
        "#!pip install flax\n",
        "\n",
        "#if connected to local runtime, run the next command too\n",
        "#pip install bs4 tensorflow torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8QzJCbx9lO"
      },
      "source": [
        "\n",
        "\n",
        "*   Update ``arg_mining/datasets/cmv_modes/configs.py`` as per your requirements, all experiments considered till now, set ``batch_size`` to 2, and all other variables with their default value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glbajGinKG4"
      },
      "source": [
        "#Run to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pCBwYZjfkHw"
      },
      "source": [
        "### Load Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKUqJo4e_Rc"
      },
      "source": [
        "%%capture\n",
        "from datasets import load_metric\n",
        "metric = load_metric('seqeval')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kJb8EE0Fm-"
      },
      "source": [
        "### Krippendorff's Alpha Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1XOXUu0FOw"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class krip_alpha():\n",
        "    \"\"\"A module for computing sentence level Krippendorff's Alpha,\n",
        "    for argumentative components  annotated at the token level. Must use\n",
        "    labels [\"B-C\", \"B-P\"].\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"See self.compute_metric() for what each of these data actually mean.\n",
        "        \"\"\"\n",
        "        self.pred_has_claim = 0\n",
        "        self.ref_has_claim = 0\n",
        "        self.pred_has_premise = 0\n",
        "        self.ref_has_premise = 0\n",
        "        \n",
        "        self.claim_wise_agreement = 0\n",
        "        self.premise_wise_agreement = 0\n",
        "        \n",
        "        self.claim_wise_disagreement = 0\n",
        "        self.premise_wise_disagreement = 0\n",
        "    \n",
        "        self.total_sentences = 0\n",
        "        \n",
        "        self.has_both_ref = 0\n",
        "        self.has_both_pred = 0\n",
        "        self.has_none_ref = 0\n",
        "        self.has_none_pred = 0\n",
        "\n",
        "    def preprocess(self, threads: List[List[int]]) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            threads:    A list of all threads in a batch. A thread is a list of \n",
        "                        integers corresponding to token_ids of the tokens in the \n",
        "                        thread.\n",
        "        Returns:\n",
        "            A List with all the threads, where each thread now consists of \n",
        "            sentence lists. Where, a sentence list in a thread list is the list \n",
        "            of token_ids corresponding to a sentence in a thread. \n",
        "        \"\"\"\n",
        "        threads_lis = []\n",
        "\n",
        "        for i, thread in enumerate(threads):\n",
        "            sentence = []\n",
        "            threads_lis.append([])\n",
        "            for j, token_id in enumerate(thread):\n",
        "                if token_id==tokenizer.pad_token_id:\n",
        "                    break\n",
        "                \n",
        "                sentence.append(token_id)\n",
        "                token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "                #print(\"appended token:\", token)\n",
        "\n",
        "                next_token = 'None' if j==len(thread) else tokenizer.convert_ids_to_tokens(thread[j+1])\n",
        "\n",
        "                if (token.count('.')+token.count('?')+token.count('!')>=1 and \n",
        "                    next_token.count('.')+next_token.count('?')+next_token.count('!')==0):\n",
        "\n",
        "                    threads_lis[i].append(sentence)\n",
        "                    #print(\"Sample sentence: \", tokenizer.decode(sentence))\n",
        "                    sentence = []\n",
        "                \n",
        "                elif re.findall(r\"\\[USER\\d+\\]|\\[UNU\\]\", token)!=[]:\n",
        "                    prev_part = tokenizer.decode(sentence[:-1])[1:-1]\n",
        "                    if re.search(r'[a-zA-Z]', prev_part) is not None:\n",
        "                        threads_lis[i].append(sentence[:-1])\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(sentence[:-1]))\n",
        "                        sentence = [sentence[-1]]\n",
        "                    else:\n",
        "                        k=len(sentence)-2\n",
        "                        while k>=0 and sentence[k]==tokenizer.convert_tokens_to_ids('Ġ'):\n",
        "                            k-=1\n",
        "                        sentence = sentence[k+1:]\n",
        "                        threads_lis[i][-1] += sentence[:k]\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(threads_lis[i][-1]))\n",
        "                \n",
        "            has_rem_token = False\n",
        "            for elem in sentence:\n",
        "                if (elem!=tokenizer.convert_tokens_to_ids('Ġ') and\n",
        "                    elem!=tokenizer.eos_token_id):\n",
        "                    has_rem_token = True\n",
        "                    break\n",
        "            \n",
        "            if has_rem_token:\n",
        "                threads_lis[i].append(sentence)\n",
        "                #print(\"Sample sentence at end of thread: \", tokenizer.decode(sentence))\n",
        "                sentence = []\n",
        "\n",
        "        return threads_lis\n",
        "\n",
        "    def get_sentence_wise_preds(self, threads: List[List[List[int]]], \n",
        "                                      predictions: List[List[str]]) -> List[List[List[str]]]:\n",
        "        \"\"\"Splits the prediction corresponding to each thread, into predictions\n",
        "        for each sentence in the corresponding thread in \"threads\" list.\n",
        "        Args:\n",
        "            threads:      A list of threads, where each thread consists of further \n",
        "                          lists corresponding to the various sentences in the\n",
        "                          thread. [As output by self.preprocess()]\n",
        "            predictions:  A list of predictions for each thread, in the threads\n",
        "                          list. Each prediciton consists of a list of componenet \n",
        "                          types corresponding to each token in a thread.\n",
        "        Returns:\n",
        "            The predictions list, with each prediction split into predictions \n",
        "            corresponding to the sentences in the corresponding thread specified\n",
        "            in the threads list. \n",
        "        \"\"\"\n",
        "        sentence_wise_preds = []\n",
        "        for i, thread in enumerate(threads):\n",
        "            next_sentence_beg = 0\n",
        "            sentence_wise_preds.append([])\n",
        "            for sentence in thread:\n",
        "                sentence_wise_preds[i].append(\n",
        "                    predictions[i][next_sentence_beg:next_sentence_beg+len(sentence)])\n",
        "                next_sentence_beg += len(sentence)\n",
        "        return sentence_wise_preds\n",
        "    \n",
        "    def update_state(self, pred_sentence: List[str], ref_sentence: List[str]) -> None:\n",
        "        \"\"\"Updates the various information maintained for the computation of\n",
        "        Krippendorff's alpha, based on the predictions(pred_sentence) and \n",
        "        references(ref_sentence) provided for a particular sentence, in some \n",
        "        thread.\n",
        "        \"\"\"\n",
        "        self.total_sentences += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence:\n",
        "            self.pred_has_claim += 1\n",
        "            if 'B-C' in ref_sentence:\n",
        "                self.ref_has_claim += 1\n",
        "                self.claim_wise_agreement += 1\n",
        "            else:\n",
        "                self.claim_wise_disagreement += 1\n",
        "            \n",
        "        elif 'B-C' in ref_sentence:\n",
        "            self.ref_has_claim += 1\n",
        "            self.claim_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.claim_wise_agreement += 1\n",
        "        \n",
        "        if 'B-P' in pred_sentence:\n",
        "            self.pred_has_premise += 1\n",
        "            if 'B-P' in ref_sentence:\n",
        "                self.ref_has_premise += 1\n",
        "                self.premise_wise_agreement += 1\n",
        "            else:\n",
        "                self.premise_wise_disagreement += 1\n",
        "\n",
        "        elif 'B-P' in ref_sentence:\n",
        "            self.ref_has_premise += 1\n",
        "            self.premise_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.premise_wise_agreement += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence and 'B-P' in pred_sentence:\n",
        "            self.has_both_pred += 1\n",
        "        \n",
        "        if 'B-C' in ref_sentence and 'B-P' in ref_sentence:\n",
        "            self.has_both_ref += 1\n",
        "        \n",
        "        if 'B-C' not in pred_sentence and 'B-P' not in pred_sentence:\n",
        "            self.has_none_pred += 1\n",
        "        \n",
        "        if 'B-C' not in ref_sentence and 'B-P' not in ref_sentence:\n",
        "            self.has_none_ref += 1\n",
        "        return\n",
        "\n",
        "    def add_batch(self, predictions: List[List[str]], \n",
        "                  references: List[List[str]], \n",
        "                  tokenized_threads: List[List[int]]) -> None:\n",
        "        \"\"\"Add a batch of predictions and references for the computation of \n",
        "        Krippendorff's alpha.\n",
        "        Args:\n",
        "            predictions:      A list of predictions for each thread, in the \n",
        "                              threads list. Each prediciton consists of a list \n",
        "                              of component types corresponding to each token in \n",
        "                              a thread.\n",
        "            references:       Same structure as predictions, but consisting of \n",
        "                              acutal gold labels, instead of predicted ones.\n",
        "            tokenized_thread: A list of all threads in a batch. A thread is a \n",
        "                              list of integers corresponding to token_ids of the\n",
        "                              tokens in the thread.\n",
        "        \"\"\"\n",
        "        threads = self.preprocess(tokenized_threads)\n",
        "        \n",
        "        sentence_wise_preds = self.get_sentence_wise_preds(threads, predictions)\n",
        "        sentence_wise_refs = self.get_sentence_wise_preds(threads, references)\n",
        "\n",
        "        for pred_thread, ref_thread in zip(sentence_wise_preds, sentence_wise_refs):\n",
        "            for pred_sentence, ref_sentence in zip(pred_thread, ref_thread):\n",
        "                self.update_state(pred_sentence, ref_sentence)\n",
        "\n",
        "    def compute(self, print_additional: bool=True) -> None:\n",
        "        \"\"\"Prints out the metric, for the batched added till now. And then \n",
        "        resets all data being maintained by the metric. \n",
        "        Args:\n",
        "            print_additional:   If True, will print all the data being \n",
        "                                maintained instead of just the Krippendorff's \n",
        "                                alphas for claims and premises.\n",
        "        \"\"\"\n",
        "        print(\"Sentence level Krippendorff's alpha for Claims: \", 1-(self.claim_wise_disagreement/(self.claim_wise_agreement+self.claim_wise_disagreement))/0.5)\n",
        "        print(\"Sentence level Krippendorff's alpha for Premises: \", 1-(self.premise_wise_disagreement/(self.premise_wise_agreement+self.premise_wise_disagreement))/0.5)\n",
        "        \n",
        "        if print_additional:\n",
        "            print(\"Additional attributes: \")\n",
        "            print(\"\\tTotal Sentences:\", self.total_sentences)\n",
        "            print(\"\\tPrediction setences having claims:\", self.pred_has_claim)\n",
        "            print(\"\\tPrediction sentences having premises:\", self.pred_has_premise)\n",
        "            print(\"\\tReference setences having claims:\", self.ref_has_claim)\n",
        "            print(\"\\tReference sentences having premises:\", self.ref_has_premise)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tPrediction Sentence having both claim and premise:\", self.has_both_pred)\n",
        "            print(\"\\tPrediction Sentence having neither claim nor premise:\", self.has_none_pred)\n",
        "            print(\"\\tReference Sentence having both claim and premise:\", self.has_both_ref)\n",
        "            print(\"\\tReference Sentence having neither claim nor premise:\", self.has_none_ref)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tSentences having claim in both reference and prediction:\", self.claim_wise_agreement)\n",
        "            print(\"\\tSentences having claim in only one of reference or prediction:\", self.claim_wise_disagreement)\n",
        "            print(\"\\tSentences having premise in both reference and prediction:\", self.premise_wise_agreement)\n",
        "            print(\"\\tSentences having premise in only one of reference or prediction:\", self.premise_wise_disagreement)\n",
        "        self.__init__()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRNv0wBWtFd"
      },
      "source": [
        "metric = krip_alpha()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6BxMkfm3R"
      },
      "source": [
        "### Define & Load Tokenizer, Model, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wcsqmllnfRB"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DeXxCDS_id",
        "outputId": "72231c9e-6149-4789-9d8a-76d789009267"
      },
      "source": [
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v938tu5rydoT"
      },
      "source": [
        "#### Load Model/Tokenizer from HF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRkCQOZu6HS"
      },
      "source": [
        "model_version = 'allenai/longformer-base-4096'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cB-7M9t0HP"
      },
      "source": [
        "%%capture\n",
        "from transformers import LongformerTokenizer, AutoModel\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_version)\n",
        "transformer_model = AutoModel.from_pretrained(model_version).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4IWXMOymZk"
      },
      "source": [
        "#### Or load them from pretrained files..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNf-8LQVyliT",
        "outputId": "e36ff58f-b32b-463e-c2f8-9e23582e2837"
      },
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "\n",
        "tokenizer = LongformerTokenizer.from_pretrained('./2epoch_complete/tokenizer/')\n",
        "transformer_model = LongformerModel.from_pretrained('./2epoch_complete/model/').to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./2epoch_complete/model/ were not used when initializing LongformerModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at ./2epoch_complete/model/ and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEV4yTy11zUm"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4PwtV9zai9"
      },
      "source": [
        "#### To add extra token type embeddings..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajTzrbzkwbT"
      },
      "source": [
        "def resize_token_type_embeddings(transformer_model, new_size):\n",
        "    old_embeddings = transformer_model.embeddings.token_type_embeddings.weight\n",
        "    old_size, hidden_dim = old_embeddings.shape\n",
        "    transformer_model.embeddings.token_type_embeddings = nn.Embedding(new_size, hidden_dim, device=transformer_model.device)\n",
        "    with torch.no_grad():\n",
        "        transformer_model.embeddings.token_type_embeddings.weight[:old_size] = old_embeddings\n",
        "\n",
        "resize_token_type_embeddings(transformer_model, 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHJiT0sjXid"
      },
      "source": [
        "transformer_model.config.type_vocab_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZ89akNzoMo"
      },
      "source": [
        "#### Load in discourse markers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p6dGA83cGVS"
      },
      "source": [
        "with open('./Discourse_Markers.txt') as f:\n",
        "    discourse_markers = [dm.strip() for dm in f.readlines()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bQWcCd1p1G"
      },
      "source": [
        "%%capture\n",
        "from arg_mining.datasets.cmv_modes import load_dataset, data_config"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5O1Wm7Dzqqe"
      },
      "source": [
        "#### Add special tokens to tokenizer and model vocab, if not already there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkOZsiCzl1k"
      },
      "source": [
        "tokenizer.add_tokens(data_config[\"special_tokens\"])\n",
        "\n",
        "transformer_model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh32PtIEz1mF"
      },
      "source": [
        "#### Function to get train, test data (80/20 split currently)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCeCgbLxVyQ"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_8gVbufzoX"
      },
      "source": [
        "### Define layers for a Linear-Chain-CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seBwkZdByesM"
      },
      "source": [
        "from allennlp.modules.conditional_random_field import ConditionalRandomField as crf\n",
        "\n",
        "ac_dict = data_config[\"arg_components\"]\n",
        "\n",
        "allowed_transitions =([(ac_dict[\"B-C\"], ac_dict[\"I-C\"]), \n",
        "                       (ac_dict[\"B-P\"], ac_dict[\"I-P\"])] + \n",
        "                      [(ac_dict[\"I-C\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-C\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"I-P\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-P\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"O\"], ac_dict[ct]) \n",
        "                        for ct in [\"O\", \"B-C\", \"B-P\"]])\n",
        "                    \n",
        "linear_layer = nn.Linear(transformer_model.config.hidden_size,\n",
        "                         len(ac_dict)).to(device)\n",
        "\n",
        "crf_layer = crf(num_tags=len(ac_dict),\n",
        "                constraints=allowed_transitions,\n",
        "                include_start_end_transitions=False).to(device)\n",
        "\n",
        "cross_entropy_layer = nn.CrossEntropyLoss(weight=torch.log(torch.tensor([3.3102, 61.4809, 3.6832, 49.6827, 2.5639], \n",
        "                                                                        device=device)), reduction='none')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUicsK33f9d7"
      },
      "source": [
        "### Global Attention Mask Utility for Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSo7p2I5mOi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n",
        "    0 elsewhere.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(tokenized_threads)\n",
        "    for user_token in [\"UNU\"]+[f\"[USER{i}]\" for i in range(data_config[\"max_users\"])]:\n",
        "        user_token_id = tokenizer.encode(user_token)[1:-1]\n",
        "        mask = np.where(tokenized_threads==user_token_id, 1, mask)\n",
        "    return np.array(mask, dtype=bool)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4PLQihf5CT"
      },
      "source": [
        "### Loss and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3u8eH1rjZe"
      },
      "source": [
        "from typing import Tuple"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuJ5aryW9tUC"
      },
      "source": [
        "def compute(batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            preds: bool=False, cross_entropy: bool=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n",
        "                component type labels of shape [batch_size, seq_len], and a global\n",
        "                attention mask for Longformer, of the same shape.\n",
        "        \n",
        "        preds:  If True, returns a List(of batch_size size) of Tuples of form \n",
        "                (tag_sequence, viterbi_score) where the tag_sequence is the \n",
        "                viterbi-decoded sequence, for the corresponding sample in the batch.\n",
        "        \n",
        "        cross_entropy:  This argument will only be used if preds=False, i.e., if \n",
        "                        loss is being calculated. If True, then cross entropy loss\n",
        "                        will also be added to the output loss.\n",
        "    \n",
        "    Returns:\n",
        "        Either the predicted sequences with their scores for each element in the batch\n",
        "        (if preds is True), or the loss value summed over all elements of the batch\n",
        "        (if preds is False).\n",
        "    \"\"\"\n",
        "    tokenized_threads, token_type_ids, comp_type_labels, global_attention_mask = batch\n",
        "    \n",
        "    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n",
        "    \n",
        "    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n",
        "                                            attention_mask=pad_mask,\n",
        "                                            global_attention_mask=global_attention_mask).last_hidden_state)\n",
        "    \n",
        "    if preds:\n",
        "        return crf_layer.viterbi_tags(logits, pad_mask)\n",
        "    \n",
        "    log_likelihood = crf_layer(logits, comp_type_labels, pad_mask)\n",
        "    \n",
        "    if cross_entropy:\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        \n",
        "        pad_mask, comp_type_labels = pad_mask.reshape(-1), comp_type_labels.reshape(-1)\n",
        "        \n",
        "        ce_loss = torch.sum(pad_mask*cross_entropy_layer(logits, comp_type_labels))\n",
        "        \n",
        "        return ce_loss - log_likelihood\n",
        "\n",
        "    return -log_likelihood"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lkPCsgEY4"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yfpzEMBGra"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n",
        "                                      linear_layer.parameters(),\n",
        "                                      crf_layer.parameters()),\n",
        "                       lr = 2e-5,)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSWnkO8gLD6"
      },
      "source": [
        "### Training And Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTNaP3kLaN2X"
      },
      "source": [
        "def train(dataset):\n",
        "    accumulate_over = 4\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (tokenized_threads, masked_threads, comp_type_labels, _ ) in enumerate(dataset):\n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads),\n",
        "                                             device=device, dtype=torch.int32)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0), \n",
        "                                         device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0), \n",
        "                                      device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0), \n",
        "                                        device=device, dtype=torch.long)\n",
        "        \n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        loss = compute((tokenized_threads,\n",
        "                        torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                        comp_type_labels, \n",
        "                        global_attention_mask))/data_config[\"batch_size\"]\n",
        "        \n",
        "        print(\"Loss: \", loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        if i%accumulate_over==accumulate_over-1:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BpSI83cU24"
      },
      "source": [
        "def evaluate(dataset, metric):\n",
        "    \n",
        "    int_to_labels = {v:k for k, v in ac_dict.items()}\n",
        "    \n",
        "    for tokenized_threads, masked_threads, comp_type_labels, _ in dataset:\n",
        "        \n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads), \n",
        "                                             device=device)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0),\n",
        "                                        device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0),\n",
        "                                     device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0),\n",
        "                                        device=device)\n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        preds = compute((tokenized_threads,\n",
        "                         torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                         comp_type_labels,\n",
        "                         global_attention_mask),\n",
        "                        preds=True)\n",
        "        \n",
        "        lengths = torch.sum(torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0), \n",
        "                            axis=-1)\n",
        "        \n",
        "        preds = [ [int_to_labels[pred] for pred in pred[0][:lengths[i]]]\n",
        "                  for i, pred in enumerate(preds)\n",
        "                ]\n",
        "        \n",
        "        refs = [ [int_to_labels[ref] for ref in labels[:lengths[i]]]\n",
        "                 for i, labels in enumerate(comp_type_labels.cpu().tolist())\n",
        "               ]\n",
        "        \n",
        "        metric.add_batch(predictions=preds, \n",
        "                         references=refs,)\n",
        "                         #tokenized_threads=tokenized_threads.cpu().tolist())\n",
        "    \n",
        "    print(metric.compute())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZV6rnIQgOYA"
      },
      "source": [
        "### Final Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2O5qCufGwA"
      },
      "source": [
        "n_epochs = 35"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xcK9sOgX44",
        "outputId": "65acb861-300c-4bb8-9c48-e05ae1752c17"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    print(f\"------------EPOCH {epoch+1}---------------\")\n",
        "    train_dataset, _, test_dataset = get_datasets()\n",
        "    train(train_dataset)\n",
        "    evaluate(test_dataset, metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------EPOCH 1---------------\n",
            "Loss:  tensor(2448.9033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2127.0803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1877.1439, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2450.6768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3395.0059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2608.2446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3188.6816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3253.7073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2926.0967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2772.6130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2560.4561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2901.6284, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3756.4526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2358.6245, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2952.6650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(926.2906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1164.3552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1930.9231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1156.4286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1849.2424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2218.5906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1843.4905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2775.5940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6682.9712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3943.1436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1396.8740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1317.7366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2023.4672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3000.7495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3362.0054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1426.0632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4300.8525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3827.7817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1415.8374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1956.0986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2731.0483, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1963.0138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3922.1396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1084.7892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1995.4417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1855.2936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1672.7037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2913.5791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1971.4705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2819.7227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1954.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 289}, 'P': {'precision': 0.11090909090909092, 'recall': 0.18541033434650456, 'f1': 0.13879408418657566, 'number': 329}, 'overall_precision': 0.11090909090909092, 'overall_recall': 0.09870550161812297, 'overall_f1': 0.10445205479452055, 'overall_accuracy': 0.5104773644105782}\n",
            "------------EPOCH 2---------------\n",
            "Loss:  tensor(1592.2214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1662.2294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1468.6550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1886.9351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2320.7700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1850.8242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2335.0344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2649.3564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2384.4360, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2131.0791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1885.6643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2182.3706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3062.8613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1741.0811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2193.7432, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(698.6473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(925.8162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1558.0355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(982.6189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1509.9409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1823.3501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1500.3182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2208.2720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5235.3633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2933.8550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1138.6362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1076.2747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1653.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2465.5757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2671.8179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1164.2112, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3632.5300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3402.8362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1140.7291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1690.5364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2415.6377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1684.8726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3354.8113, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(948.9041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1750.0259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1542.4609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1445.2358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2396.4219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1552.5408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2436.5916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1639.2910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.21348314606741572, 'recall': 0.0657439446366782, 'f1': 0.10052910052910052, 'number': 289}, 'P': {'precision': 0.21196581196581196, 'recall': 0.3768996960486322, 'f1': 0.2713347921225383, 'number': 329}, 'overall_precision': 0.21216617210682492, 'overall_recall': 0.2313915857605178, 'overall_f1': 0.22136222910216716, 'overall_accuracy': 0.5465598386373823}\n",
            "------------EPOCH 3---------------\n",
            "Loss:  tensor(1327.5579, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1326.5586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1137.3463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1508.4915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1809.6279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1476.2562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1902.2523, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2109.6750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1939.1046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1774.6271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1497.9492, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1684.6998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2527.9666, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1441.6689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1808.7639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(583.0543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(770.4323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1353.4885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(842.2374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1217.8871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1550.9852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1287.2361, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1848.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4626.1743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2507.0554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(994.1711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(917.6641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1432.5341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2020.0903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1977.3188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(970.2986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3066.8945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2811.5039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(926.7845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1408.9717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2010.9414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1316.5930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2750.7070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(845.2018, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1511.2788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1282.4392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1157.3174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1956.7650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1271.3132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1950.4111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1469.7153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2507936507936508, 'recall': 0.27335640138408307, 'f1': 0.26158940397350994, 'number': 289}, 'P': {'precision': 0.33260869565217394, 'recall': 0.46504559270516715, 'f1': 0.38783269961977185, 'number': 329}, 'overall_precision': 0.29935483870967744, 'overall_recall': 0.37540453074433655, 'overall_f1': 0.33309404163675527, 'overall_accuracy': 0.5897579560735097}\n",
            "------------EPOCH 4---------------\n",
            "Loss:  tensor(998.2607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(768.1893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(604.1332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(873.1363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1440.2616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1110.5951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1621.7643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1680.2212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1541.9741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1452.1592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1212.9438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1334.5273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2092.3818, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1179.8171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1599.0940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(444.8608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(612.8031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1085.3196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(602.4160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(897.0887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1205.1821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(884.2932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1378.4424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3840.3604, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1736.4376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(782.1221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(739.3608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1194.8093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1676.6998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1461.2662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(795.9165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2489.8660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2160.3831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(767.7067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1129.0898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1592.1403, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(975.5664, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2088.7400, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(643.9727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1120.5159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1031.0168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(696.5484, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1226.1133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(745.4985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1305.9396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1074.4762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3172043010752688, 'recall': 0.2041522491349481, 'f1': 0.24842105263157893, 'number': 289}, 'P': {'precision': 0.36699029126213595, 'recall': 0.574468085106383, 'f1': 0.44786729857819907, 'number': 329}, 'overall_precision': 0.3537803138373752, 'overall_recall': 0.40129449838187703, 'overall_f1': 0.37604245640636846, 'overall_accuracy': 0.624719856566562}\n",
            "------------EPOCH 5---------------\n",
            "Loss:  tensor(700.8549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(601.2236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(419.5282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(667.0223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1281.2053, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(826.1942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1230.3721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1481.3298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1289.9688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1144.0693, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1004.1588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1106.4325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1613.4658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(989.9019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1389.1770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(376.8624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(518.6979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(877.1196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(468.2581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(572.4562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(868.7612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(606.5405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1139.5605, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2980.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1164.0281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(511.9524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(463.0944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(860.6759, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1292.1620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1143.8281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(629.4575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1834.9562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1607.0859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(599.1600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(683.2036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1249.5024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(785.4526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1617.4963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(452.8354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(808.8691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(851.9097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(455.1747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(863.2068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(539.4332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1097.0188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1090.1979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.32537960954446854, 'recall': 0.5190311418685121, 'f1': 0.4, 'number': 289}, 'P': {'precision': 0.47023809523809523, 'recall': 0.48024316109422494, 'f1': 0.47518796992481205, 'number': 329}, 'overall_precision': 0.3864491844416562, 'overall_recall': 0.49838187702265374, 'overall_f1': 0.4353356890459364, 'overall_accuracy': 0.6280255490811295}\n",
            "------------EPOCH 6---------------\n",
            "Loss:  tensor(687.3277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(386.1195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(283.7337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(525.2460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1002.1009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(524.6224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1206.1847, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1092.7706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1031.0999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(838.6313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(753.3795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(847.1688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1222.4980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(777.8292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1000.1871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(307.0402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(487.1255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(622.8766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(355.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(566.8547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(638.5486, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(424.8331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(769.7711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2570.4121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(646.8794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(306.5868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(285.7536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(605.3522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1108.9736, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(844.4427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(384.8910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1342.1687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1220.7583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(419.6762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(528.8556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(759.3967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(559.1320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1069.5103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.1341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(514.3332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(645.5863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(250.5804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(543.6118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(379.7240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(753.7321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(746.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3684210526315789, 'recall': 0.3633217993079585, 'f1': 0.3658536585365853, 'number': 289}, 'P': {'precision': 0.5449591280653951, 'recall': 0.60790273556231, 'f1': 0.574712643678161, 'number': 329}, 'overall_precision': 0.4677914110429448, 'overall_recall': 0.4935275080906149, 'overall_f1': 0.48031496062992124, 'overall_accuracy': 0.6883684446436575}\n",
            "------------EPOCH 7---------------\n",
            "Loss:  tensor(334.1320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(300.5513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(180.9476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(337.0690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(858.2942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(410.5274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(843.1314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1047.9749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(762.7770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(578.5851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(549.9501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(530.9041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(888.6201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(585.9235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(773.6514, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(233.8069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(315.4845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(429.3167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(277.3646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(412.5023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(457.8483, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(276.5250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(804.1636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1666.9008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(433.6499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(232.9612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(204.2866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(471.1999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(720.7863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(587.0025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(220.5408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(928.7719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(737.5579, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(279.4140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(309.8372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(440.6530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(419.8439, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(772.8442, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(196.8603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(313.4436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(465.7328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.8329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(370.7600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(257.7366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(563.5927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(514.8336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4109090909090909, 'recall': 0.39100346020761245, 'f1': 0.40070921985815605, 'number': 289}, 'P': {'precision': 0.5427872860635696, 'recall': 0.6747720364741642, 'f1': 0.6016260162601625, 'number': 329}, 'overall_precision': 0.489766081871345, 'overall_recall': 0.5420711974110033, 'overall_f1': 0.5145929339477727, 'overall_accuracy': 0.6996302106678619}\n",
            "------------EPOCH 8---------------\n",
            "Loss:  tensor(198.8395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(195.6159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(115.9860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(200.4491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(734.5984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(175.7561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.4846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(616.0905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(565.7070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(279.0158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(308.0607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(360.5112, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(668.3855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(484.4788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(566.9661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(178.2424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(231.3574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(205.8070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.7916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(261.3130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(287.8859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(168.5879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(386.3598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1346.3124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(399.6900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(197.3854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.6207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(389.3546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(624.8845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(484.1897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(236.0000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(612.3621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(522.4935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(221.9051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.1153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(382.9679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(443.9168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(619.5219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(184.6795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(224.4196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(401.3160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.7072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(341.5935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(261.4477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(755.8427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.5563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39617486338797814, 'recall': 0.5017301038062284, 'f1': 0.44274809160305345, 'number': 289}, 'P': {'precision': 0.624031007751938, 'recall': 0.48936170212765956, 'f1': 0.5485519591141398, 'number': 329}, 'overall_precision': 0.49038461538461536, 'overall_recall': 0.49514563106796117, 'overall_f1': 0.4927536231884057, 'overall_accuracy': 0.7157664724338861}\n",
            "------------EPOCH 9---------------\n",
            "Loss:  tensor(221.9588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(146.4410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(115.8474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(174.5632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(598.1839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.3186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(496.0225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(605.9253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(441.0033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(299.7728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(280.6791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(332.3170, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1020.4414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(570.2247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(698.7670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(164.5926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(197.7684, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.6062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(158.4170, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(170.1265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.8688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.5682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.7663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(756.8356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(193.3502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.9425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(100.8804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(269.3806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.0808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(450.5786, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.8209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(490.0121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(445.7144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(165.9127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(155.0447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(409.6668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(319.7241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(497.2309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(178.6942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(219.7710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(236.8412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.6565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(180.4485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(154.0277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(332.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(263.3564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37623762376237624, 'recall': 0.657439446366782, 'f1': 0.47858942065491183, 'number': 289}, 'P': {'precision': 0.6944444444444444, 'recall': 0.22796352583586627, 'f1': 0.34324942791762014, 'number': 329}, 'overall_precision': 0.43230016313213704, 'overall_recall': 0.42880258899676377, 'overall_f1': 0.43054427294882214, 'overall_accuracy': 0.6613626176602421}\n",
            "------------EPOCH 10---------------\n",
            "Loss:  tensor(591.2917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(181.7690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(121.3811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(196.2300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1312.9568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(479.1287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1151.9554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(868.4117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.4136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(150.1061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(194.1453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(318.9918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(541.7283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(294.6315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(454.2317, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.1879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(615.4631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1068.7731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(658.3142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(617.7338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(922.4977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(969.2855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1449.3748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3975.7998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(689.4561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(350.5240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(200.7475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(564.5093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.6539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(489.7158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(171.3555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(551.2210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(770.2461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(290.9890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(558.4368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(511.9629, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(489.6965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(985.7969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(245.9074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(535.9290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(506.1040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(387.9268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(829.9205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(453.9041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(845.6108, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(852.4114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.35879218472468916, 'recall': 0.698961937716263, 'f1': 0.4741784037558686, 'number': 289}, 'P': {'precision': 0.7391304347826086, 'recall': 0.3100303951367781, 'f1': 0.43683083511777304, 'number': 329}, 'overall_precision': 0.43366619115549215, 'overall_recall': 0.4919093851132686, 'overall_f1': 0.4609552691432904, 'overall_accuracy': 0.6476916181084715}\n",
            "------------EPOCH 11---------------\n",
            "Loss:  tensor(486.6422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.2115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(148.2700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(187.2907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(610.7052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(299.7764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(888.4120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(559.8530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(835.8787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(868.4159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(547.2476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(796.7123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(809.4039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(674.2208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(551.2363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(239.5835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(184.1365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(334.3153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(208.1815, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(211.5453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(318.9426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(260.5427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(502.1339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1367.5310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(506.0202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(229.9703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(179.1670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.2206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1307.5455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(964.3178, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.4217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1626.2228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1736.8789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(814.7821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(799.9596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1604.8076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(497.2670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2088.6953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.9222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(693.4192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(831.4268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(298.5544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(718.5975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(308.1207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(436.2955, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(343.3418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4068627450980392, 'recall': 0.5743944636678201, 'f1': 0.4763271162123386, 'number': 289}, 'P': {'precision': 0.6333333333333333, 'recall': 0.5197568389057751, 'f1': 0.5709515859766277, 'number': 329}, 'overall_precision': 0.4970501474926254, 'overall_recall': 0.5453074433656958, 'overall_f1': 0.5200617283950617, 'overall_accuracy': 0.7019273868220529}\n",
            "------------EPOCH 12---------------\n",
            "Loss:  tensor(141.2191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(166.5740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.3756, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(158.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(365.6599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(119.0426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(449.8495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(307.5799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(563.0603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(650.9211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(251.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(444.6617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1395.8530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(982.7591, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1053.9696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(533.2811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(435.4627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(939.8171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(418.8804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(700.8647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1172.3120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(567.1041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(862.6641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2575.5154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(379.1769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.2687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(236.6679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(435.1346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(552.0978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(499.6051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(207.6082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(672.7913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(617.5040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(309.2454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(278.2234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(540.5655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(274.2609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1016.6378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(271.3241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(375.5142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(851.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(244.2884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(420.8739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(417.3246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(917.1398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(744.3820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45255474452554745, 'recall': 0.21453287197231835, 'f1': 0.29107981220657275, 'number': 289}, 'P': {'precision': 0.4351395730706076, 'recall': 0.8054711246200608, 'f1': 0.5650319829424306, 'number': 329}, 'overall_precision': 0.4383378016085791, 'overall_recall': 0.529126213592233, 'overall_f1': 0.4794721407624634, 'overall_accuracy': 0.6089757956073509}\n",
            "------------EPOCH 13---------------\n",
            "Loss:  tensor(516.8595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(752.4764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(499.8295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(747.7759, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1190.0759, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(686.2183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1039.4075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1693.5717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1110.2688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(833.1290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(638.2832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(834.7068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(682.1846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(402.5754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(685.7153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(181.8066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(178.0824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(257.9470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.3607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(242.2028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(288.1783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(170.4590, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(371.4579, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(783.0724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(308.6008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(148.0218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(150.2766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(279.8833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(432.3913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(554.8037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(238.5044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(659.9951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(554.5660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(248.6279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(329.5767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(416.8891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(361.6299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.2421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(179.1681, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(301.3665, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(331.8612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.4849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(341.1485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(201.3052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(374.0895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(302.5156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4183168316831683, 'recall': 0.5847750865051903, 'f1': 0.4877344877344877, 'number': 289}, 'P': {'precision': 0.6222222222222222, 'recall': 0.5106382978723404, 'f1': 0.5609348914858097, 'number': 329}, 'overall_precision': 0.5, 'overall_recall': 0.5453074433656958, 'overall_f1': 0.5216718266253869, 'overall_accuracy': 0.7053451367099955}\n",
            "------------EPOCH 14---------------\n",
            "Loss:  tensor(170.2716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(113.4852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.7720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.8152, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(325.3964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(115.7279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(356.3612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(258.6893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.7433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(205.9992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.3930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(277.7690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(319.8550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(229.3478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(328.1439, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.8362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(119.2271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(211.1311, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(115.0690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(131.7483, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(262.9611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.3886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(277.0471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(564.7545, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.2613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(197.6806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(208.9679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(307.7874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.2479, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(303.6985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(267.9976, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.8032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.9283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.5882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.1121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(258.5805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.7495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.3463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.7058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.5133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.5639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.5086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(266.1898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(192.2599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.5035211267605634, 'recall': 0.49480968858131485, 'f1': 0.49912739965095987, 'number': 289}, 'P': {'precision': 0.5856777493606138, 'recall': 0.6960486322188449, 'f1': 0.6361111111111112, 'number': 329}, 'overall_precision': 0.5511111111111111, 'overall_recall': 0.6019417475728155, 'overall_f1': 0.5754060324825986, 'overall_accuracy': 0.7218175705961453}\n",
            "------------EPOCH 15---------------\n",
            "Loss:  tensor(64.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(165.1435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.6171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.9554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(340.1616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.4994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(276.1302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(308.1037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(177.6978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(138.1130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.1750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(205.2028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(206.7734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.4654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(278.0269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.4193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.3036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(123.4127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.3136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.0487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.0210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.7518, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.3343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(259.4208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.7247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.1909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.4898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.8732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(135.6138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.5845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.8493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(185.2160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(190.9032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.1754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.1290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(135.5564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(110.9490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(194.1673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.6052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.6174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(128.8829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.8060, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.4748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.4185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(204.7755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.1745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.46153846153846156, 'recall': 0.5813148788927336, 'f1': 0.5145482388973965, 'number': 289}, 'P': {'precision': 0.5723270440251572, 'recall': 0.5531914893617021, 'f1': 0.5625965996908809, 'number': 329}, 'overall_precision': 0.5131964809384164, 'overall_recall': 0.5663430420711975, 'overall_f1': 0.5384615384615385, 'overall_accuracy': 0.7169430748543254}\n",
            "------------EPOCH 16---------------\n",
            "Loss:  tensor(45.7302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.3144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.3961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(75.2371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(192.8530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.5703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(187.9338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.9834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.5119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.6656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.3478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(150.8297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.4071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(112.7044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.8718, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.1297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.5670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.3757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.4399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.8082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.4906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.5442, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.0531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(175.9970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.9873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.3349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.8008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(83.0853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(110.8211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(221.9885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.0052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(168.9602, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(157.6635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.9117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.7990, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.0125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.5514, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(169.4823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.3059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.3920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.3390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.5965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.0110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(180.3877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(83.3518, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.48695652173913045, 'recall': 0.5813148788927336, 'f1': 0.529968454258675, 'number': 289}, 'P': {'precision': 0.5963302752293578, 'recall': 0.5927051671732523, 'f1': 0.5945121951219511, 'number': 329}, 'overall_precision': 0.5401785714285714, 'overall_recall': 0.587378640776699, 'overall_f1': 0.5627906976744186, 'overall_accuracy': 0.718343792021515}\n",
            "------------EPOCH 17---------------\n",
            "Loss:  tensor(43.3992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.0752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(163.7367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.0721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.7307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.5845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.0240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.2934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.2054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.9946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.0107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.2636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(101.3055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.4043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.7791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.6150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.8655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.1764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.5914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.0448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.4592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.1649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.1530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.3627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.7121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.8325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.1450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.5081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.0462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.6978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.4760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.0782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.9160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.4562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.6307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(151.3273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.3841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.4839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.9119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.6476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.6137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.0838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(166.5946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.6291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4678362573099415, 'recall': 0.5536332179930796, 'f1': 0.5071315372424723, 'number': 289}, 'P': {'precision': 0.5735735735735735, 'recall': 0.5805471124620061, 'f1': 0.5770392749244713, 'number': 329}, 'overall_precision': 0.52, 'overall_recall': 0.5679611650485437, 'overall_f1': 0.54292343387471, 'overall_accuracy': 0.723050201703272}\n",
            "------------EPOCH 18---------------\n",
            "Loss:  tensor(38.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.4055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.5282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.3264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.4169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.9713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.8365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.0509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.4059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.9377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(123.5548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.4119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(75.6222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.8555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.6494, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.9823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.1568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.7496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.2599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.9740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.7696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.8864, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.5631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.8817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.0175, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(75.0197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(187.8223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.0124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.9769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.7269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6467, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.4199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.5373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.5736, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(135.1261, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.6190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.0090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.7295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.3020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(144.5009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.5601, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4697406340057637, 'recall': 0.5640138408304498, 'f1': 0.5125786163522013, 'number': 289}, 'P': {'precision': 0.5909090909090909, 'recall': 0.5927051671732523, 'f1': 0.5918057663125948, 'number': 329}, 'overall_precision': 0.5288035450516987, 'overall_recall': 0.5792880258899676, 'overall_f1': 0.5528957528957529, 'overall_accuracy': 0.721033168982519}\n",
            "------------EPOCH 19---------------\n",
            "Loss:  tensor(35.8739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.4613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.7933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.7953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(101.9178, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.9714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.6893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.4406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.1374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.5522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(101.8583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.1049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.4552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.4654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.9363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.0995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.8302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.5826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.1804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.1978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.0077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.6138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.3350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.5412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.2287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.0694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(175.9911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.4459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.9490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.6747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.7970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.3629, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.0203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.8696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.6601, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.2520, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.1270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.3490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.0317, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.9318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.6931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4685714285714286, 'recall': 0.5674740484429066, 'f1': 0.513302034428795, 'number': 289}, 'P': {'precision': 0.599388379204893, 'recall': 0.5957446808510638, 'f1': 0.597560975609756, 'number': 329}, 'overall_precision': 0.5317577548005908, 'overall_recall': 0.5825242718446602, 'overall_f1': 0.5559845559845559, 'overall_accuracy': 0.7239466606902735}\n",
            "------------EPOCH 20---------------\n",
            "Loss:  tensor(34.2255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.0629, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.2291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.8561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.1876, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.8693, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.6129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.8593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.4342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.7900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.6329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.1356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.4368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.9804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.4335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.4062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.7123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.2909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.3446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.2758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.4998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.3637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.3169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(164.3183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.3193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.0014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.4119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.6525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.0185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.7416, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.6519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.6399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.9067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.8979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.3943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.7884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.46175637393767704, 'recall': 0.5640138408304498, 'f1': 0.5077881619937694, 'number': 289}, 'P': {'precision': 0.5945121951219512, 'recall': 0.5927051671732523, 'f1': 0.5936073059360729, 'number': 329}, 'overall_precision': 0.5256975036710719, 'overall_recall': 0.5792880258899676, 'overall_f1': 0.5511932255581217, 'overall_accuracy': 0.7205849394890184}\n",
            "------------EPOCH 21---------------\n",
            "Loss:  tensor(32.9667, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.9292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.6007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.3012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.8699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.7339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.8791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9304, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.8033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.6926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.2362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.4627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.4274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.9820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.1437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.4186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.3276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.7862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.9393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.0314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.5657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.3691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.6888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.3116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.4182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(151.9888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3876, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.9684, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.9737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.2689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.9094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(112.9090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.2753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.9090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.5692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.2268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.9782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.4337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4619718309859155, 'recall': 0.5674740484429066, 'f1': 0.5093167701863354, 'number': 289}, 'P': {'precision': 0.5975609756097561, 'recall': 0.5957446808510638, 'f1': 0.5966514459665143, 'number': 329}, 'overall_precision': 0.527086383601757, 'overall_recall': 0.5825242718446602, 'overall_f1': 0.5534204458109147, 'overall_accuracy': 0.7209771402958315}\n",
            "------------EPOCH 22---------------\n",
            "Loss:  tensor(32.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.2320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.9906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.8638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.7303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.1430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.0261, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.2910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.4181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.0337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.3315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.4410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.2797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.8839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.7223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.1861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.3194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.8960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.7048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.1082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.6800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.3621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.9960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.1072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.5658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.3055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(139.7224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.4618, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(105.4100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.4365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.2687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.4219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.7609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.9507, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.1200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.6522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.1974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.6617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.4327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4581005586592179, 'recall': 0.5674740484429066, 'f1': 0.5069551777434312, 'number': 289}, 'P': {'precision': 0.600609756097561, 'recall': 0.5987841945288754, 'f1': 0.5996955859969559, 'number': 329}, 'overall_precision': 0.5262390670553936, 'overall_recall': 0.5841423948220065, 'overall_f1': 0.5536809815950919, 'overall_accuracy': 0.7200806813088301}\n",
            "------------EPOCH 23---------------\n",
            "Loss:  tensor(31.3895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.5414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.1989, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.3993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.3390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.9882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.5162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.4386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.8460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.8993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.5422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.0877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.7131, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.1822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.6949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.1626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.7112, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.8561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.7223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.4062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.6568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.0956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.5720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.7158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.9142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.8501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.5890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.1297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.2699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.0054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.6298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.6607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.7314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45786516853932585, 'recall': 0.5640138408304498, 'f1': 0.5054263565891473, 'number': 289}, 'P': {'precision': 0.5969696969696969, 'recall': 0.5987841945288754, 'f1': 0.5978755690440061, 'number': 329}, 'overall_precision': 0.5247813411078717, 'overall_recall': 0.5825242718446602, 'overall_f1': 0.5521472392638036, 'overall_accuracy': 0.7199125952487674}\n",
            "------------EPOCH 24---------------\n",
            "Loss:  tensor(30.8497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.9134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2001, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.3913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.8709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.7429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.6821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.7388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.9979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.5542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.9882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.0096, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.9045, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.4425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.0681, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.4554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.9659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.8359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.3472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.1876, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.4493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.0937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.3100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(101.7563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.6415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.4250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.5080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.8638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.3990, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.4229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.4370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4550561797752809, 'recall': 0.5605536332179931, 'f1': 0.5023255813953489, 'number': 289}, 'P': {'precision': 0.5933734939759037, 'recall': 0.5987841945288754, 'f1': 0.5960665658093798, 'number': 329}, 'overall_precision': 0.5218023255813954, 'overall_recall': 0.580906148867314, 'overall_f1': 0.5497702909647779, 'overall_accuracy': 0.7183998207082026}\n",
            "------------EPOCH 25---------------\n",
            "Loss:  tensor(30.4287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.0099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.7255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1507, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.4991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.8140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.7972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.0115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.8767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.6243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.0711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.8052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.5833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.6566, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.4837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.6175, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.8914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.9888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.9352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.2567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.3822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.9565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(116.8789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.7505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.8188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(100.2308, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.4355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.6147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0990, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.9905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.8463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.6676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.8296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.6571, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45251396648044695, 'recall': 0.5605536332179931, 'f1': 0.500772797527048, 'number': 289}, 'P': {'precision': 0.5873493975903614, 'recall': 0.5927051671732523, 'f1': 0.5900151285930408, 'number': 329}, 'overall_precision': 0.5173913043478261, 'overall_recall': 0.5776699029126213, 'overall_f1': 0.5458715596330276, 'overall_accuracy': 0.716326759300762}\n",
            "------------EPOCH 26---------------\n",
            "Loss:  tensor(30.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.0951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.1434, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.9043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.5714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.7720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.3806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.1668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.8614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.8620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.2679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.5954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.7766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.3209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.9106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.0199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.7399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.4658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.8468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.7953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.0666, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.5266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(101.0318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.2169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.8628, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.4324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.7864, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.2794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.9431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.9303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.1107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45125348189415043, 'recall': 0.5605536332179931, 'f1': 0.5, 'number': 289}, 'P': {'precision': 0.5898203592814372, 'recall': 0.5987841945288754, 'f1': 0.5942684766214179, 'number': 329}, 'overall_precision': 0.5180375180375181, 'overall_recall': 0.580906148867314, 'overall_f1': 0.547673531655225, 'overall_accuracy': 0.7171671896010757}\n",
            "------------EPOCH 27---------------\n",
            "Loss:  tensor(29.7151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.2134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.0136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.6275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.3948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.5755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.9588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.6924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.7487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.7312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.0581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.6575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.9035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.5679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.0123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.1045, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.5394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.1678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.1023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.3138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.3499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.1765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.3295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.8339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.6853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.3504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.8791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.2179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.7378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.9093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.4230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.5137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.4516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.8940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.3592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.7395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45251396648044695, 'recall': 0.5605536332179931, 'f1': 0.500772797527048, 'number': 289}, 'P': {'precision': 0.5898203592814372, 'recall': 0.5987841945288754, 'f1': 0.5942684766214179, 'number': 329}, 'overall_precision': 0.5187861271676301, 'overall_recall': 0.580906148867314, 'overall_f1': 0.5480916030534352, 'overall_accuracy': 0.7173352756611385}\n",
            "------------EPOCH 28---------------\n",
            "Loss:  tensor(29.3719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.7608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.0161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.4489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.1459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.1193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.2917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.6869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.2729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.4445, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.8674, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.2972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.5515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.0849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6306, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.1731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.0270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.9529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.5891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.7932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.2456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(105.0378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.6676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.5343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.2350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.3382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.1402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.1447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.2668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.3237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.9150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.0552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.0343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.3258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45251396648044695, 'recall': 0.5605536332179931, 'f1': 0.500772797527048, 'number': 289}, 'P': {'precision': 0.5880597014925373, 'recall': 0.5987841945288754, 'f1': 0.5933734939759036, 'number': 329}, 'overall_precision': 0.5180375180375181, 'overall_recall': 0.580906148867314, 'overall_f1': 0.547673531655225, 'overall_accuracy': 0.7163827879874496}\n",
            "------------EPOCH 29---------------\n",
            "Loss:  tensor(29.1068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.1556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.2814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.5707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.2002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.1963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.6911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.8991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.8773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.3660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.1036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5239, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.6931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.5891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.1797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.5898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.2860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.7521, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.6829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.6333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.4345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.8354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.3713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.3724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.5525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.5105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.4733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.5386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.9825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.0671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.4408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.1712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.9031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.7117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4613259668508287, 'recall': 0.5778546712802768, 'f1': 0.5130568356374808, 'number': 289}, 'P': {'precision': 0.5903614457831325, 'recall': 0.5957446808510638, 'f1': 0.5930408472012103, 'number': 329}, 'overall_precision': 0.5230547550432276, 'overall_recall': 0.587378640776699, 'overall_f1': 0.5533536585365854, 'overall_accuracy': 0.7175593904078889}\n",
            "------------EPOCH 30---------------\n",
            "Loss:  tensor(28.2233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.1853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.5969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.0638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.8099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.9898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.5896, device='cuda:0', grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mi7LRmfIYbR"
      },
      "source": [
        "### Rough -- Checking dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9kGw_svbXM",
        "outputId": "b2838b14-a51e-4129-bb8c-20b274bb7a1c"
      },
      "source": [
        "\" \".join(\" mY name is \".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mY name is'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-oVkplUCJh"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qMGQ5GOzbn"
      },
      "source": [
        "train_dataset, _, test_dataset = get_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdpyzvKIfM8",
        "outputId": "8531f009-4bed-4ea9-daa4-d76d023d52e9"
      },
      "source": [
        "for tokenized_threads, masked_threads, comp_type_labels, _ in test_dataset:\n",
        "    tokenized_threads, masked_threads, comp_type_labels = tokenized_threads[0], masked_threads[0], comp_type_labels[0]\n",
        "    for tokenized_thread, masked_thread, comp_type_label in zip(tokenized_threads, masked_threads, comp_type_labels):\n",
        "        print(comp_type_label[:100])\n",
        "        print(tokenized_thread[:100])\n",
        "        print(tokenizer.decode(tokenized_thread[:500]))\n",
        "        start, end = 0, 0\n",
        "        prev_type = \"other\"\n",
        "        i = 0\n",
        "        while i<tokenized_thread.shape[0]:\n",
        "            if comp_type_label[i]==ac_dict[\"O\"]:\n",
        "                if prev_type==\"other\":\n",
        "                    end += 1\n",
        "                else:\n",
        "                    print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                    print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                    start = i\n",
        "                    end = i\n",
        "                    prev_type=\"other\"\n",
        "                \n",
        "            if comp_type_label[i] in [ac_dict[\"B-C\"], ac_dict[\"B-P\"]]:\n",
        "                print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                start = i\n",
        "                end = i\n",
        "                prev_type = \"Claim\" if comp_type_label[i]==ac_dict[\"B-C\"] else \"Premise\"\n",
        "            \n",
        "            if comp_type_label[i] in [ac_dict[\"I-C\"], ac_dict[\"I-P\"]]:\n",
        "                end += 1\n",
        "            \n",
        "            i+=1\n",
        "        break\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
            " 2 2 2 2 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4]\n",
            "[    0 18814   846    35  7978     9  1901    16   145   551   350   444\n",
            " 50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268\n",
            "   100  1819   923    84   481  1901    53     7   162  1437  8585    16\n",
            "    10   699   516   227 20203   110    78  8322   235    36  1437    22\n",
            "   270  1284 29384   328]\n",
            "<s>CMV: Freedom of speech is being taken too far [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE] I certainly value our free speech but to me there is a clear line between exercising your first amendment right (  \" President Obama sucks! \" etc ) and doing things that are known to be offensive to other cultures (  Satirical cartoons of prophets, assassinating leaders, etc ). [NEWLINE] [NEWLINE]  Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE] Sure, but that doesn't mean we condone the bully's actions and don't punish the bullies for acting. [NEWLINE] We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments. [NEWLINE]  Complete freedom in the expression of any idea, offensive or not, is a major element of that world. [NEWLINE] [NEWLINE]  If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us. [NEWLINE] [NEWLINE] [USER0] [NEWLINE] I certainly agree with your points - I didn't mean to imply that I was only for * * some * * freedom of speech. [NEWLINE] I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset. [NEWLINE] [NEWLINE]  Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"  [NEWLINE] Is that so hard? [NEWLINE] [NEWLINE] </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Component:  <s>CM  of type:  other [    0 18814]\n",
            "Masked Component:  <s>CM  of type:  other [    0 18814]\n",
            "Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Masked Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Masked Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Masked Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Component:   but to me   of type:  other [  53    7  162 1437]\n",
            "Masked Component:  <mask> to me   of type:  other [50264     7   162  1437]\n",
            "Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Masked Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Masked Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Masked Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Component:   (   of type:  other [  36 1437]\n",
            "Masked Component:   (   of type:  other [  36 1437]\n",
            "Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Masked Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Masked Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Component:   Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111    53   114    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Masked Component:   Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111 50264 50264    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            "    53   114    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Masked Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            " 50264 50264    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Component:  Sure  of type:  Claim [32541]\n",
            "Masked Component:  Sure  of type:  Claim [32541]\n",
            "Component:  , but   of type:  other [   6   53 1437]\n",
            "Masked Component:  ,<mask>   of type:  other [    6 50264  1437]\n",
            "Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Masked Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Masked Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Masked Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [ 318   47  236    7 1744    5 3519    7 1994   13  143    9  201    6\n",
            "   47   33    7 1744    5 3519    7 1994   13   70    9  201]\n",
            "Masked Component:  <mask> you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [50264    47   236     7  1744     5  3519     7  1994    13   143     9\n",
            "   201     6    47    33     7  1744     5  3519     7  1994    13    70\n",
            "     9   201]\n",
            "Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Masked Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Component:   -   of type:  other [ 111 1437]\n",
            "Masked Component:   -   of type:  other [ 111 1437]\n",
            "Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Masked Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901    77  1402  1134\n",
            "   120  4904]\n",
            "Masked Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech<mask> certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901 50264  1402  1134\n",
            "   120  4904]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"   of type:  Premise [ 2612    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45   142\n",
            "    52   214  6023     9    47    53   142    52  2098   110  2728     4\n",
            "    22  1437]\n",
            "Masked Component:  <mask> can't America be the bigger person and say \" Ok, we won't publish certain types of material, not<mask> we're afraid of you<mask><mask> we respect your views. \"   of type:  Premise [50264    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45 50264\n",
            "    52   214  6023     9    47 50264 50264    52  2098   110  2728     4\n",
            "    22  1437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZnb1Nz-MX4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2601bebd-3989-40a5-af7d-91d578651823"
      },
      "source": [
        "import re\n",
        "re.sub(r\"\\s*</claim>([^\\s])\", r\"</claim> \\1\", \"<claim>my name is </claim>jeevesh.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<claim>my name is</claim> jeevesh.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UVJb5jy178"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}