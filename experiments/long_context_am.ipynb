{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "long_context_am.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "35kJb8EE0Fm-",
        "-Mi7LRmfIYbR"
      ],
      "authorship_tag": "ABX9TyP7IRmAAsa3oPSsG375mz6r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/arg_mining/blob/main/experiments/long_context_am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGdooHmfgd-"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GPY8HXWtkyE"
      },
      "source": [
        "%%capture\n",
        "#if running on colab, install below 4\n",
        "#!git clone https://github.com/Jeevesh8/arg_mining\n",
        "#!pip install transformers\n",
        "#!pip install seqeval datasets allennlp\n",
        "#!pip install flax\n",
        "\n",
        "#if connected to local runtime, run the next command too\n",
        "#pip install bs4 tensorflow torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8QzJCbx9lO"
      },
      "source": [
        "\n",
        "\n",
        "*   Update ``arg_mining/datasets/cmv_modes/configs.py`` as per your requirements, all experiments considered till now, set ``batch_size`` to 2, and all other variables with their default value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glbajGinKG4"
      },
      "source": [
        "#Run to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pCBwYZjfkHw"
      },
      "source": [
        "### Load Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKUqJo4e_Rc"
      },
      "source": [
        "%%capture\n",
        "from datasets import load_metric\n",
        "metric = load_metric('seqeval')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kJb8EE0Fm-"
      },
      "source": [
        "### Krippendorff's Alpha Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1XOXUu0FOw"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class krip_alpha():\n",
        "    \"\"\"A module for computing sentence level Krippendorff's Alpha,\n",
        "    for argumentative components  annotated at the token level. Must use\n",
        "    labels [\"B-C\", \"B-P\"].\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"See self.compute_metric() for what each of these data actually mean.\n",
        "        \"\"\"\n",
        "        self.pred_has_claim = 0\n",
        "        self.ref_has_claim = 0\n",
        "        self.pred_has_premise = 0\n",
        "        self.ref_has_premise = 0\n",
        "        \n",
        "        self.claim_wise_agreement = 0\n",
        "        self.premise_wise_agreement = 0\n",
        "        \n",
        "        self.claim_wise_disagreement = 0\n",
        "        self.premise_wise_disagreement = 0\n",
        "    \n",
        "        self.total_sentences = 0\n",
        "        \n",
        "        self.has_both_ref = 0\n",
        "        self.has_both_pred = 0\n",
        "        self.has_none_ref = 0\n",
        "        self.has_none_pred = 0\n",
        "\n",
        "    def preprocess(self, threads: List[List[int]]) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            threads:    A list of all threads in a batch. A thread is a list of \n",
        "                        integers corresponding to token_ids of the tokens in the \n",
        "                        thread.\n",
        "        Returns:\n",
        "            A List with all the threads, where each thread now consists of \n",
        "            sentence lists. Where, a sentence list in a thread list is the list \n",
        "            of token_ids corresponding to a sentence in a thread. \n",
        "        \"\"\"\n",
        "        threads_lis = []\n",
        "\n",
        "        for i, thread in enumerate(threads):\n",
        "            sentence = []\n",
        "            threads_lis.append([])\n",
        "            for j, token_id in enumerate(thread):\n",
        "                if token_id==tokenizer.pad_token_id:\n",
        "                    break\n",
        "                \n",
        "                sentence.append(token_id)\n",
        "                token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "                #print(\"appended token:\", token)\n",
        "\n",
        "                next_token = 'None' if j==len(thread) else tokenizer.convert_ids_to_tokens(thread[j+1])\n",
        "\n",
        "                if (token.count('.')+token.count('?')+token.count('!')>=1 and \n",
        "                    next_token.count('.')+next_token.count('?')+next_token.count('!')==0):\n",
        "\n",
        "                    threads_lis[i].append(sentence)\n",
        "                    #print(\"Sample sentence: \", tokenizer.decode(sentence))\n",
        "                    sentence = []\n",
        "                \n",
        "                elif re.findall(r\"\\[USER\\d+\\]|\\[UNU\\]\", token)!=[]:\n",
        "                    prev_part = tokenizer.decode(sentence[:-1])[1:-1]\n",
        "                    if re.search(r'[a-zA-Z]', prev_part) is not None:\n",
        "                        threads_lis[i].append(sentence[:-1])\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(sentence[:-1]))\n",
        "                        sentence = [sentence[-1]]\n",
        "                    else:\n",
        "                        k=len(sentence)-2\n",
        "                        while k>=0 and sentence[k]==tokenizer.convert_tokens_to_ids('Ġ'):\n",
        "                            k-=1\n",
        "                        sentence = sentence[k+1:]\n",
        "                        threads_lis[i][-1] += sentence[:k]\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(threads_lis[i][-1]))\n",
        "                \n",
        "            has_rem_token = False\n",
        "            for elem in sentence:\n",
        "                if (elem!=tokenizer.convert_tokens_to_ids('Ġ') and\n",
        "                    elem!=tokenizer.eos_token_id):\n",
        "                    has_rem_token = True\n",
        "                    break\n",
        "            \n",
        "            if has_rem_token:\n",
        "                threads_lis[i].append(sentence)\n",
        "                #print(\"Sample sentence at end of thread: \", tokenizer.decode(sentence))\n",
        "                sentence = []\n",
        "\n",
        "        return threads_lis\n",
        "\n",
        "    def get_sentence_wise_preds(self, threads: List[List[List[int]]], \n",
        "                                      predictions: List[List[str]]) -> List[List[List[str]]]:\n",
        "        \"\"\"Splits the prediction corresponding to each thread, into predictions\n",
        "        for each sentence in the corresponding thread in \"threads\" list.\n",
        "        Args:\n",
        "            threads:      A list of threads, where each thread consists of further \n",
        "                          lists corresponding to the various sentences in the\n",
        "                          thread. [As output by self.preprocess()]\n",
        "            predictions:  A list of predictions for each thread, in the threads\n",
        "                          list. Each prediciton consists of a list of componenet \n",
        "                          types corresponding to each token in a thread.\n",
        "        Returns:\n",
        "            The predictions list, with each prediction split into predictions \n",
        "            corresponding to the sentences in the corresponding thread specified\n",
        "            in the threads list. \n",
        "        \"\"\"\n",
        "        sentence_wise_preds = []\n",
        "        for i, thread in enumerate(threads):\n",
        "            next_sentence_beg = 0\n",
        "            sentence_wise_preds.append([])\n",
        "            for sentence in thread:\n",
        "                sentence_wise_preds[i].append(\n",
        "                    predictions[i][next_sentence_beg:next_sentence_beg+len(sentence)])\n",
        "                next_sentence_beg += len(sentence)\n",
        "        return sentence_wise_preds\n",
        "    \n",
        "    def update_state(self, pred_sentence: List[str], ref_sentence: List[str]) -> None:\n",
        "        \"\"\"Updates the various information maintained for the computation of\n",
        "        Krippendorff's alpha, based on the predictions(pred_sentence) and \n",
        "        references(ref_sentence) provided for a particular sentence, in some \n",
        "        thread.\n",
        "        \"\"\"\n",
        "        self.total_sentences += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence:\n",
        "            self.pred_has_claim += 1\n",
        "            if 'B-C' in ref_sentence:\n",
        "                self.ref_has_claim += 1\n",
        "                self.claim_wise_agreement += 1\n",
        "            else:\n",
        "                self.claim_wise_disagreement += 1\n",
        "            \n",
        "        elif 'B-C' in ref_sentence:\n",
        "            self.ref_has_claim += 1\n",
        "            self.claim_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.claim_wise_agreement += 1\n",
        "        \n",
        "        if 'B-P' in pred_sentence:\n",
        "            self.pred_has_premise += 1\n",
        "            if 'B-P' in ref_sentence:\n",
        "                self.ref_has_premise += 1\n",
        "                self.premise_wise_agreement += 1\n",
        "            else:\n",
        "                self.premise_wise_disagreement += 1\n",
        "\n",
        "        elif 'B-P' in ref_sentence:\n",
        "            self.ref_has_premise += 1\n",
        "            self.premise_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.premise_wise_agreement += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence and 'B-P' in pred_sentence:\n",
        "            self.has_both_pred += 1\n",
        "        \n",
        "        if 'B-C' in ref_sentence and 'B-P' in ref_sentence:\n",
        "            self.has_both_ref += 1\n",
        "        \n",
        "        if 'B-C' not in pred_sentence and 'B-P' not in pred_sentence:\n",
        "            self.has_none_pred += 1\n",
        "        \n",
        "        if 'B-C' not in ref_sentence and 'B-P' not in ref_sentence:\n",
        "            self.has_none_ref += 1\n",
        "        return\n",
        "\n",
        "    def add_batch(self, predictions: List[List[str]], \n",
        "                  references: List[List[str]], \n",
        "                  tokenized_threads: List[List[int]]) -> None:\n",
        "        \"\"\"Add a batch of predictions and references for the computation of \n",
        "        Krippendorff's alpha.\n",
        "        Args:\n",
        "            predictions:      A list of predictions for each thread, in the \n",
        "                              threads list. Each prediciton consists of a list \n",
        "                              of component types corresponding to each token in \n",
        "                              a thread.\n",
        "            references:       Same structure as predictions, but consisting of \n",
        "                              acutal gold labels, instead of predicted ones.\n",
        "            tokenized_thread: A list of all threads in a batch. A thread is a \n",
        "                              list of integers corresponding to token_ids of the\n",
        "                              tokens in the thread.\n",
        "        \"\"\"\n",
        "        threads = self.preprocess(tokenized_threads)\n",
        "        \n",
        "        sentence_wise_preds = self.get_sentence_wise_preds(threads, predictions)\n",
        "        sentence_wise_refs = self.get_sentence_wise_preds(threads, references)\n",
        "\n",
        "        for pred_thread, ref_thread in zip(sentence_wise_preds, sentence_wise_refs):\n",
        "            for pred_sentence, ref_sentence in zip(pred_thread, ref_thread):\n",
        "                self.update_state(pred_sentence, ref_sentence)\n",
        "\n",
        "    def compute(self, print_additional: bool=True) -> None:\n",
        "        \"\"\"Prints out the metric, for the batched added till now. And then \n",
        "        resets all data being maintained by the metric. \n",
        "        Args:\n",
        "            print_additional:   If True, will print all the data being \n",
        "                                maintained instead of just the Krippendorff's \n",
        "                                alphas for claims and premises.\n",
        "        \"\"\"\n",
        "        print(\"Sentence level Krippendorff's alpha for Claims: \", 1-(self.claim_wise_disagreement/(self.claim_wise_agreement+self.claim_wise_disagreement))/0.5)\n",
        "        print(\"Sentence level Krippendorff's alpha for Premises: \", 1-(self.premise_wise_disagreement/(self.premise_wise_agreement+self.premise_wise_disagreement))/0.5)\n",
        "        \n",
        "        if print_additional:\n",
        "            print(\"Additional attributes: \")\n",
        "            print(\"\\tTotal Sentences:\", self.total_sentences)\n",
        "            print(\"\\tPrediction setences having claims:\", self.pred_has_claim)\n",
        "            print(\"\\tPrediction sentences having premises:\", self.pred_has_premise)\n",
        "            print(\"\\tReference setences having claims:\", self.ref_has_claim)\n",
        "            print(\"\\tReference sentences having premises:\", self.ref_has_premise)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tPrediction Sentence having both claim and premise:\", self.has_both_pred)\n",
        "            print(\"\\tPrediction Sentence having neither claim nor premise:\", self.has_none_pred)\n",
        "            print(\"\\tReference Sentence having both claim and premise:\", self.has_both_ref)\n",
        "            print(\"\\tReference Sentence having neither claim nor premise:\", self.has_none_ref)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tSentences having claim in both reference and prediction:\", self.claim_wise_agreement)\n",
        "            print(\"\\tSentences having claim in only one of reference or prediction:\", self.claim_wise_disagreement)\n",
        "            print(\"\\tSentences having premise in both reference and prediction:\", self.premise_wise_agreement)\n",
        "            print(\"\\tSentences having premise in only one of reference or prediction:\", self.premise_wise_disagreement)\n",
        "        self.__init__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRNv0wBWtFd"
      },
      "source": [
        "metric = krip_alpha()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6BxMkfm3R"
      },
      "source": [
        "### Define & Load Tokenizer, Model, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wcsqmllnfRB"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DeXxCDS_id",
        "outputId": "b77c12dc-d30c-4c98-edac-c5b2e787247a"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v938tu5rydoT"
      },
      "source": [
        "#### Load Model/Tokenizer from HF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRkCQOZu6HS"
      },
      "source": [
        "model_version = 'allenai/longformer-base-4096'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cB-7M9t0HP"
      },
      "source": [
        "%%capture\n",
        "from transformers import LongformerTokenizer, AutoModel\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_version)\n",
        "transformer_model = AutoModel.from_pretrained(model_version).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4IWXMOymZk"
      },
      "source": [
        "#### Or load them from pretrained files..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNf-8LQVyliT",
        "outputId": "b7178e86-6229-425b-9813-75a956d0dc3e"
      },
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "\n",
        "tokenizer = LongformerTokenizer.from_pretrained('./2epoch_complete/tokenizer/')\n",
        "transformer_model = LongformerModel.from_pretrained('./2epoch_complete/model/').to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./2epoch_complete/model/ were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at ./2epoch_complete/model/ and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEV4yTy11zUm"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4PwtV9zai9"
      },
      "source": [
        "#### To add extra token type embeddings..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajTzrbzkwbT"
      },
      "source": [
        "def resize_token_type_embeddings(transformer_model, new_size):\n",
        "    old_embeddings = transformer_model.embeddings.token_type_embeddings.weight\n",
        "    old_size, hidden_dim = old_embeddings.shape\n",
        "    transformer_model.embeddings.token_type_embeddings = nn.Embedding(new_size, hidden_dim, device=transformer_model.device)\n",
        "    with torch.no_grad():\n",
        "        transformer_model.embeddings.token_type_embeddings.weight[:old_size] = old_embeddings\n",
        "\n",
        "resize_token_type_embeddings(transformer_model, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHJiT0sjXid"
      },
      "source": [
        "transformer_model.config.type_vocab_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZ89akNzoMo"
      },
      "source": [
        "#### Load in discourse markers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p6dGA83cGVS"
      },
      "source": [
        "with open('./Discourse_Markers.txt') as f:\n",
        "    discourse_markers = [dm.strip() for dm in f.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bQWcCd1p1G"
      },
      "source": [
        "%%capture\n",
        "from arg_mining.datasets.cmv_modes import load_dataset, data_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5O1Wm7Dzqqe"
      },
      "source": [
        "#### Add special tokens to tokenizer and model vocab, if not already there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkOZsiCzl1k"
      },
      "source": [
        "tokenizer.add_tokens(data_config[\"special_tokens\"])\n",
        "\n",
        "transformer_model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh32PtIEz1mF"
      },
      "source": [
        "#### Function to get train, test data (80/20 split currently)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCeCgbLxVyQ"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=50,\n",
        "                                                              test_sz=50,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_8gVbufzoX"
      },
      "source": [
        "### Define layers for a Linear-Chain-CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seBwkZdByesM"
      },
      "source": [
        "from allennlp.modules.conditional_random_field import ConditionalRandomField as crf\n",
        "\n",
        "ac_dict = data_config[\"arg_components\"]\n",
        "\n",
        "allowed_transitions =([(ac_dict[\"B-C\"], ac_dict[\"I-C\"]), \n",
        "                       (ac_dict[\"B-P\"], ac_dict[\"I-P\"])] + \n",
        "                      [(ac_dict[\"I-C\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-C\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"I-P\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-P\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"O\"], ac_dict[ct]) \n",
        "                        for ct in [\"O\", \"B-C\", \"B-P\"]])\n",
        "                    \n",
        "linear_layer = nn.Linear(transformer_model.config.hidden_size,\n",
        "                         len(ac_dict)).to(device)\n",
        "\n",
        "crf_layer = crf(num_tags=len(ac_dict),\n",
        "                constraints=allowed_transitions,\n",
        "                include_start_end_transitions=False).to(device)\n",
        "\n",
        "cross_entropy_layer = nn.CrossEntropyLoss(weight=torch.log(torch.tensor([3.3102, 61.4809, 3.6832, 49.6827, 2.5639], \n",
        "                                                                        device=device)), reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUicsK33f9d7"
      },
      "source": [
        "### Global Attention Mask Utility for Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSo7p2I5mOi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n",
        "    0 elsewhere.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(tokenized_threads)\n",
        "    for user_token in [\"UNU\"]+[f\"[USER{i}]\" for i in range(data_config[\"max_users\"])]:\n",
        "        user_token_id = tokenizer.encode(user_token)[1:-1]\n",
        "        mask = np.where(tokenized_threads==user_token_id, 1, mask)\n",
        "    return np.array(mask, dtype=bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4PLQihf5CT"
      },
      "source": [
        "### Loss and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3u8eH1rjZe"
      },
      "source": [
        "from typing import Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuJ5aryW9tUC"
      },
      "source": [
        "def compute(batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            preds: bool=False, cross_entropy: bool=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n",
        "                component type labels of shape [batch_size, seq_len], and a global\n",
        "                attention mask for Longformer, of the same shape.\n",
        "        \n",
        "        preds:  If True, returns a List(of batch_size size) of Tuples of form \n",
        "                (tag_sequence, viterbi_score) where the tag_sequence is the \n",
        "                viterbi-decoded sequence, for the corresponding sample in the batch.\n",
        "        \n",
        "        cross_entropy:  This argument will only be used if preds=False, i.e., if \n",
        "                        loss is being calculated. If True, then cross entropy loss\n",
        "                        will also be added to the output loss.\n",
        "    \n",
        "    Returns:\n",
        "        Either the predicted sequences with their scores for each element in the batch\n",
        "        (if preds is True), or the loss value summed over all elements of the batch\n",
        "        (if preds is False).\n",
        "    \"\"\"\n",
        "    tokenized_threads, token_type_ids, comp_type_labels, global_attention_mask = batch\n",
        "    \n",
        "    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n",
        "    \n",
        "    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n",
        "                                            attention_mask=pad_mask,\n",
        "                                            global_attention_mask=global_attention_mask).last_hidden_state)\n",
        "    \n",
        "    if preds:\n",
        "        return crf_layer.viterbi_tags(logits, pad_mask)\n",
        "    \n",
        "    log_likelihood = crf_layer(logits, comp_type_labels, pad_mask)\n",
        "    \n",
        "    if cross_entropy:\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        \n",
        "        pad_mask, comp_type_labels = pad_mask.reshape(-1), comp_type_labels.reshape(-1)\n",
        "        \n",
        "        ce_loss = torch.sum(pad_mask*cross_entropy_layer(logits, comp_type_labels))\n",
        "        \n",
        "        return ce_loss - log_likelihood\n",
        "\n",
        "    return -log_likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lkPCsgEY4"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yfpzEMBGra"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n",
        "                                      linear_layer.parameters(),\n",
        "                                      crf_layer.parameters()),\n",
        "                       lr = 2e-5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSWnkO8gLD6"
      },
      "source": [
        "### Training And Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTNaP3kLaN2X"
      },
      "source": [
        "def train(dataset):\n",
        "    accumulate_over = 4\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (tokenized_threads, masked_threads, comp_type_labels, _ ) in enumerate(dataset):\n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads),\n",
        "                                             device=device, dtype=torch.int32)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0), \n",
        "                                         device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0), \n",
        "                                      device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0), \n",
        "                                        device=device, dtype=torch.long)\n",
        "        \n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        loss = compute((tokenized_threads,\n",
        "                        torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                        comp_type_labels, \n",
        "                        global_attention_mask))/data_config[\"batch_size\"]\n",
        "        \n",
        "        print(\"Loss: \", loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        if i%accumulate_over==accumulate_over-1:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BpSI83cU24"
      },
      "source": [
        "def evaluate(dataset, metric):\n",
        "    \n",
        "    int_to_labels = {v:k for k, v in ac_dict.items()}\n",
        "    \n",
        "    for tokenized_threads, masked_threads, comp_type_labels, _ in dataset:\n",
        "        \n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads), \n",
        "                                             device=device)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0),\n",
        "                                        device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0),\n",
        "                                     device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0),\n",
        "                                        device=device)\n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        preds = compute((tokenized_threads,\n",
        "                         torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                         comp_type_labels,\n",
        "                         global_attention_mask),\n",
        "                        preds=True)\n",
        "        \n",
        "        lengths = torch.sum(torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0), \n",
        "                            axis=-1)\n",
        "        \n",
        "        preds = [ [int_to_labels[pred] for pred in pred[0][:lengths[i]]]\n",
        "                  for i, pred in enumerate(preds)\n",
        "                ]\n",
        "        \n",
        "        refs = [ [int_to_labels[ref] for ref in labels[:lengths[i]]]\n",
        "                 for i, labels in enumerate(comp_type_labels.cpu().tolist())\n",
        "               ]\n",
        "        \n",
        "        metric.add_batch(predictions=preds, \n",
        "                         references=refs,)\n",
        "                         #tokenized_threads=tokenized_threads.cpu().tolist())\n",
        "    \n",
        "    print(metric.compute())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZV6rnIQgOYA"
      },
      "source": [
        "### Final Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2O5qCufGwA"
      },
      "source": [
        "n_epochs = 35"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xcK9sOgX44",
        "outputId": "7729a18b-ebe8-42af-f4a8-77f82375476c"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    print(f\"------------EPOCH {epoch+1}---------------\")\n",
        "    train_dataset, _, test_dataset = get_datasets()\n",
        "    train(train_dataset)\n",
        "    evaluate(test_dataset, metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------EPOCH 1---------------\n",
            "Loss:  tensor(3426.5449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2321.1104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2124.8677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2661.7747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3997.3401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3106.0842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3692.3784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3471.5649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3212.0410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3130.8801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2790.0083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3067.5820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4018.3713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2659.7090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3250.7080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(994.2097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1221.6023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2081.8564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1116.3726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2000.1370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2395.6313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1930.5344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2959.5190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6864.7612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4079.9182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1480.9335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1420.0356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2105.1006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3285.7144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.03758389261744966, 'recall': 0.0374331550802139, 'f1': 0.03750837240455458, 'number': 748}, 'P': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1036}, 'overall_precision': 0.037333333333333336, 'overall_recall': 0.01569506726457399, 'overall_f1': 0.022099447513812157, 'overall_accuracy': 0.299279114630927}\n",
            "------------EPOCH 2---------------\n",
            "Loss:  tensor(2544.6746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1726.8668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1578.6436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2023.7524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2875.3481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2145.7859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2798.8367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2510.6814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2466.7549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2419.7241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2271.9922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2535.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3401.1709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2206.2883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2672.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(846.3611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1031.7850, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1762.9154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(962.9022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1710.2556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2130.2502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1704.5205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2579.8994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5858.2803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3342.5381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1278.4812, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1211.1941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1904.9087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2871.0703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.05615826419910657, 'recall': 0.11764705882352941, 'f1': 0.07602591792656588, 'number': 748}, 'P': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 1036}, 'overall_precision': 0.054287476866132015, 'overall_recall': 0.04932735426008968, 'overall_f1': 0.051688693098384725, 'overall_accuracy': 0.3768910549294345}\n",
            "------------EPOCH 3---------------\n",
            "Loss:  tensor(2018.9897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1704.9385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1550.4617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1977.5425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2343.1697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1874.2698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2485.1079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2342.4104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2177.3381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2149.4062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1947.9956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2215.0242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3027.8384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1877.1819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2284.7473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(741.9941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(911.7042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1635.5388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(864.3918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1527.4639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1934.9348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1435.6167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2333.9492, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5121.6470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2866.1392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1196.0835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1127.9703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1695.1499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2509.3027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.07100977198697069, 'recall': 0.14572192513368984, 'f1': 0.09548839246605344, 'number': 748}, 'P': {'precision': 0.08641975308641975, 'recall': 0.02702702702702703, 'f1': 0.041176470588235294, 'number': 1036}, 'overall_precision': 0.07369553523399677, 'overall_recall': 0.07679372197309417, 'overall_f1': 0.07521273675542135, 'overall_accuracy': 0.4394760889430399}\n",
            "------------EPOCH 4---------------\n",
            "Loss:  tensor(1680.2059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1198.9688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1078.3215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1405.3323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1996.8391, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1660.4254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2219.9722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2159.1570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1983.7393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1997.6213, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1601.1476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1776.4075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2695.2849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1633.3866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1958.1682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(680.6375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(819.8568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1484.8287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(772.4424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1306.1982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1744.9043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1213.7478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2055.6509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4599.9268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2417.6309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1053.3716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(990.4617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1482.9562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2193.3784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.14687714481811942, 'recall': 0.28609625668449196, 'f1': 0.19410430839002266, 'number': 748}, 'P': {'precision': 0.13827655310621242, 'recall': 0.06660231660231661, 'f1': 0.08990228013029317, 'number': 1036}, 'overall_precision': 0.14468302658486706, 'overall_recall': 0.1586322869955157, 'overall_f1': 0.15133689839572192, 'overall_accuracy': 0.49635495989440553}\n",
            "------------EPOCH 5---------------\n",
            "Loss:  tensor(1234.1322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(879.3831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(803.1817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1047.2069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1680.2495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1398.3951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1961.4740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1835.1809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1742.2529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1699.7446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1291.6006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1426.4966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2364.8755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1420.9720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1675.1122, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(623.1471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(729.5723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1292.3870, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(668.6379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1056.8396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1502.7371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(977.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1707.9727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3870.6821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1888.0088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(870.7301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(773.5488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1244.3450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1846.8152, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.23853211009174313, 'recall': 0.48663101604278075, 'f1': 0.32014072119613013, 'number': 748}, 'P': {'precision': 0.2125, 'recall': 0.13127413127413126, 'f1': 0.16229116945107397, 'number': 1036}, 'overall_precision': 0.23084025854108955, 'overall_recall': 0.2802690582959641, 'overall_f1': 0.25316455696202533, 'overall_accuracy': 0.5402782008325718}\n",
            "------------EPOCH 6---------------\n",
            "Loss:  tensor(876.6380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(613.4686, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(543.2471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(747.3819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1400.7649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1067.7198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1690.9675, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1516.5552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1492.7212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1305.3787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1038.2075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1250.0906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1975.0239, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1176.4741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1461.9060, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(481.0336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(630.4578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1064.1586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(549.9121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(861.8942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1251.9646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(734.2712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1307.6780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3171.2329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1312.6815, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(666.6706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(552.5576, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(998.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1594.9786, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.24451827242524918, 'recall': 0.4919786096256685, 'f1': 0.32667554371948515, 'number': 748}, 'P': {'precision': 0.4501466275659824, 'recall': 0.29633204633204635, 'f1': 0.35739231664726434, 'number': 1036}, 'overall_precision': 0.30864197530864196, 'overall_recall': 0.3783632286995516, 'overall_f1': 0.33996474439687735, 'overall_accuracy': 0.5442989135952888}\n",
            "------------EPOCH 7---------------\n",
            "Loss:  tensor(716.8098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(492.7969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(412.9967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(653.5599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1117.0999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(763.9895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1532.7400, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1405.1454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1316.0079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1040.9050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(801.6578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1055.1031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1869.5398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1091.9868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1300.3970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(417.2460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(544.7006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(991.0862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(515.5272, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(719.2800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1161.4613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(661.5376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1250.5339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3467.4526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(843.0684, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(458.7114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(341.9719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(802.1456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1197.7446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.24570337364735836, 'recall': 0.516042780748663, 'f1': 0.3329021129797326, 'number': 748}, 'P': {'precision': 0.4773462783171521, 'recall': 0.28474903474903474, 'f1': 0.3567110036275695, 'number': 1036}, 'overall_precision': 0.31110095934216536, 'overall_recall': 0.38172645739910316, 'overall_f1': 0.34281399446262273, 'overall_accuracy': 0.5410092395167022}\n",
            "------------EPOCH 8---------------\n",
            "Loss:  tensor(588.8948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(365.4763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.9003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(559.2725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1020.3267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(694.0690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1613.7921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1316.6924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1311.4908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1059.9083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(738.5330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1058.7531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1883.4760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(980.1637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1223.2888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(337.2851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(457.3840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(907.0070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(456.4897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(661.7200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1199.7534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(637.2631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1176.5065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3069.8047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(769.1235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(424.1034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(298.9032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(793.5065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1510.1835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2625570776255708, 'recall': 0.1537433155080214, 'f1': 0.19392917369308602, 'number': 748}, 'P': {'precision': 0.3533266129032258, 'recall': 0.6766409266409267, 'f1': 0.4642384105960265, 'number': 1036}, 'overall_precision': 0.33691164327002476, 'overall_recall': 0.45739910313901344, 'overall_f1': 0.3880171184022824, 'overall_accuracy': 0.6244491826581379}\n",
            "------------EPOCH 9---------------\n",
            "Loss:  tensor(946.8443, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1058.7783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(860.8094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1410.4895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2121.5505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1598.3250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2252.9429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2933.4231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1592.7670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1189.7717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(937.9093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1101.0176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1456.1838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(834.4789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1092.5081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(324.0730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(547.6044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1120.3625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(463.2722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(750.7449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1967.2793, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(895.4550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1771.9927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4611.3682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1839.9697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(932.5663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(866.5057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1458.7642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2309.0979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2626518218623482, 'recall': 0.6938502673796791, 'f1': 0.381057268722467, 'number': 748}, 'P': {'precision': 0.08303249097472924, 'recall': 0.0222007722007722, 'f1': 0.035034272658035034, 'number': 1036}, 'overall_precision': 0.24056813138038172, 'overall_recall': 0.3038116591928251, 'overall_f1': 0.2685162249194947, 'overall_accuracy': 0.45909229363387144}\n",
            "------------EPOCH 10---------------\n",
            "Loss:  tensor(1532.3837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(613.1852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(568.8846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(766.7669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1288.8538, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(915.8673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1396.7693, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1162.0630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1145.3260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(957.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(823.4569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(949.3426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1573.5868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(925.4780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1214.4844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(354.3046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(493.8253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(881.0710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(482.6169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(703.4077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(975.9967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(589.9066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1400.6600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2486.5522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1029.5430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(471.8916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(337.6286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(774.6541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1052.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.33617021276595743, 'recall': 0.42245989304812837, 'f1': 0.3744075829383886, 'number': 748}, 'P': {'precision': 0.39784117193523516, 'recall': 0.4980694980694981, 'f1': 0.44234890698671236, 'number': 1036}, 'overall_precision': 0.3719266875279392, 'overall_recall': 0.4663677130044843, 'overall_f1': 0.41382740611788116, 'overall_accuracy': 0.6558838460757437}\n",
            "------------EPOCH 11---------------\n",
            "Loss:  tensor(611.8019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(366.1135, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(358.3553, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(460.0132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(846.9113, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(533.9066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1054.2872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1277.2206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1012.6776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(719.4294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(573.2679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(655.9236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1199.4902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(671.4528, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1013.4744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(265.3871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(363.1208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(620.6323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(312.8827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(415.5478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(806.7418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(412.9658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1020.9189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1748.8446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(467.2101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(343.3777, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(266.1796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(696.6228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(843.7105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2715477293790547, 'recall': 0.3917112299465241, 'f1': 0.32074438970990693, 'number': 748}, 'P': {'precision': 0.3594507269789984, 'recall': 0.4295366795366795, 'f1': 0.3913808267370273, 'number': 1036}, 'overall_precision': 0.31851532153646955, 'overall_recall': 0.413677130044843, 'overall_f1': 0.35991221653255306, 'overall_accuracy': 0.6540968626256473}\n",
            "------------EPOCH 12---------------\n",
            "Loss:  tensor(449.7624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(237.6114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(224.4329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(333.3835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(707.2863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(280.5246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(852.4906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1044.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(776.3463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(455.2266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(450.4903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(503.4909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(921.2015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(554.8107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(900.8893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(180.1545, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(250.4699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(421.2017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(208.9811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(314.3346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(439.5778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(219.5163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(629.0723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1221.7458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(298.2109, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.3372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(177.6908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(477.8535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(645.0338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3082811412665275, 'recall': 0.5922459893048129, 'f1': 0.40549199084668197, 'number': 748}, 'P': {'precision': 0.4574468085106383, 'recall': 0.3735521235521235, 'f1': 0.41126461211477155, 'number': 1036}, 'overall_precision': 0.36355672360928604, 'overall_recall': 0.46524663677130046, 'overall_f1': 0.40816326530612246, 'overall_accuracy': 0.6128134836023962}\n",
            "------------EPOCH 13---------------\n",
            "Loss:  tensor(320.8551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(152.0643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(138.6406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.4361, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(640.8996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(165.7699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(674.4860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(595.9832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(539.1476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(315.6536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(252.3137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(388.7318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.0809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(358.0449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(521.7330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(132.8943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(178.6771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(297.1131, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(171.3956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(203.5547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(308.9895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(145.6003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(417.2241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(914.0405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(229.9262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(213.1558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(155.3821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(321.7755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(363.5808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.36065573770491804, 'recall': 0.5, 'f1': 0.41904761904761906, 'number': 748}, 'P': {'precision': 0.48711554447215294, 'recall': 0.5656370656370656, 'f1': 0.5234479678427869, 'number': 1036}, 'overall_precision': 0.42857142857142855, 'overall_recall': 0.5381165919282511, 'overall_f1': 0.4771371769383697, 'overall_accuracy': 0.6690019291298609}\n",
            "------------EPOCH 14---------------\n",
            "Loss:  tensor(135.1302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.7791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.6789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.8986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(406.7974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(128.4993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(439.2063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(456.6817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(377.8696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(237.8295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(154.2524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(313.5657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(387.4315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(244.7503, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(434.9213, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.0622, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.6623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(219.4513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.2104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.4061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(210.0324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.2209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(254.6031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(591.0082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.1188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(132.8616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.3828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(245.0887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(287.7154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.35932560590094836, 'recall': 0.45588235294117646, 'f1': 0.4018856806128462, 'number': 748}, 'P': {'precision': 0.46296296296296297, 'recall': 0.5308880308880309, 'f1': 0.4946043165467626, 'number': 1036}, 'overall_precision': 0.4169396350023397, 'overall_recall': 0.4994394618834081, 'overall_f1': 0.4544758990053558, 'overall_accuracy': 0.6833181033607473}\n",
            "------------EPOCH 15---------------\n",
            "Loss:  tensor(110.6014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.3960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.4943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(158.3458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(342.0789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.5537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(367.7673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(418.2175, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(280.2711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.6040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.5222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(246.2678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(227.7604, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.2589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(233.0452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(117.2539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(138.1810, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.6368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.6041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(139.0937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(178.7641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.1552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(396.6808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(558.8431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.2639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.6239, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(104.4643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.0283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(233.1577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3089500860585198, 'recall': 0.4799465240641711, 'f1': 0.3759162303664922, 'number': 748}, 'P': {'precision': 0.4014962593516209, 'recall': 0.46621621621621623, 'f1': 0.43144260830728, 'number': 1036}, 'overall_precision': 0.35602536997885836, 'overall_recall': 0.47197309417040356, 'overall_f1': 0.40588093516510004, 'overall_accuracy': 0.667093105899076}\n",
            "------------EPOCH 16---------------\n",
            "Loss:  tensor(73.7757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.8841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.4108, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.9941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.9983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.5202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(230.2658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(254.1962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(273.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.1074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.5383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(132.2982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(175.6974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(194.4095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(190.7860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.3379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.8838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.7319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.3633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(116.0688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.1491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.2164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(144.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(315.1124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.9851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.4147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.7272, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(139.2885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(207.5986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3357487922705314, 'recall': 0.5574866310160428, 'f1': 0.41909547738693465, 'number': 748}, 'P': {'precision': 0.5113851992409867, 'recall': 0.5202702702702703, 'f1': 0.5157894736842106, 'number': 1036}, 'overall_precision': 0.4163763066202091, 'overall_recall': 0.5358744394618834, 'overall_f1': 0.4686274509803922, 'overall_accuracy': 0.6534673570920906}\n",
            "------------EPOCH 17---------------\n",
            "Loss:  tensor(37.5661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.5404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.3656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(155.2219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.0093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.5495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.6967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.2745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(217.6959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(100.9192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.0414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.7700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.9708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.3271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(139.4581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.6417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.3277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.7947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.4863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.3750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.3649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.3104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(273.3129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.7994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.1369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.7533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.3748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(100.9825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.35408163265306125, 'recall': 0.46390374331550804, 'f1': 0.4016203703703704, 'number': 748}, 'P': {'precision': 0.4667176740627391, 'recall': 0.5888030888030888, 'f1': 0.5206999573196757, 'number': 1036}, 'overall_precision': 0.4184521206821163, 'overall_recall': 0.5364349775784754, 'overall_f1': 0.47015475313190863, 'overall_accuracy': 0.6832977967306325}\n",
            "------------EPOCH 18---------------\n",
            "Loss:  tensor(34.3245, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.7931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.0116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.0707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.0763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.1866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.8856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.1102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.9663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.5337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.9166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.9202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.2395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.3906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.0287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.8381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.2556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.1232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(191.0777, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.3991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.7162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.1480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.9567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3446153846153846, 'recall': 0.5989304812834224, 'f1': 0.4375, 'number': 748}, 'P': {'precision': 0.5123152709359606, 'recall': 0.5019305019305019, 'f1': 0.5070697220867869, 'number': 1036}, 'overall_precision': 0.41814254859611233, 'overall_recall': 0.5426008968609866, 'overall_f1': 0.4723103195901439, 'overall_accuracy': 0.6572443902934308}\n",
            "------------EPOCH 19---------------\n",
            "Loss:  tensor(26.2477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.4411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.1100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.6197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.6825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.6279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.7473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.5160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.7165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.5785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.4756, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.6794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.7694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.7103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.5394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.3411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.8610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.3501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.7816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.8789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.7333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.3312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3755496921723835, 'recall': 0.570855614973262, 'f1': 0.45305039787798407, 'number': 748}, 'P': {'precision': 0.5137111517367459, 'recall': 0.5424710424710425, 'f1': 0.5276995305164319, 'number': 1036}, 'overall_precision': 0.44329896907216493, 'overall_recall': 0.554372197309417, 'overall_f1': 0.4926525529265255, 'overall_accuracy': 0.6746065590415271}\n",
            "------------EPOCH 20---------------\n",
            "Loss:  tensor(22.4734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.5231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.7427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.4113, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.5147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.0375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.6216, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.9898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.7712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.6673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.9733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.1700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.0254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.3380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.2333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.6489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.3287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38461538461538464, 'recall': 0.5481283422459893, 'f1': 0.45203969128996696, 'number': 748}, 'P': {'precision': 0.4962089300758214, 'recall': 0.5685328185328186, 'f1': 0.52991452991453, 'number': 1036}, 'overall_precision': 0.4434087882822903, 'overall_recall': 0.5599775784753364, 'overall_f1': 0.4949219717612089, 'overall_accuracy': 0.6789521778860798}\n",
            "------------EPOCH 21---------------\n",
            "Loss:  tensor(15.8287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.6301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.5280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.6866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.5620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5756, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.4516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.0275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.4314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.5250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.8654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.372236958443855, 'recall': 0.5628342245989305, 'f1': 0.4481106971793507, 'number': 748}, 'P': {'precision': 0.4966044142614601, 'recall': 0.5646718146718147, 'f1': 0.5284552845528456, 'number': 1036}, 'overall_precision': 0.4356864443482027, 'overall_recall': 0.5639013452914798, 'overall_f1': 0.49157097483508433, 'overall_accuracy': 0.67590618336887}\n",
            "------------EPOCH 22---------------\n",
            "Loss:  tensor(10.4628, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.5138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.5890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.2608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.1246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.3421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.8325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8510, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9112, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37646001796945194, 'recall': 0.5601604278074866, 'f1': 0.4502955400322407, 'number': 748}, 'P': {'precision': 0.5181347150259067, 'recall': 0.5791505791505791, 'f1': 0.5469462169553327, 'number': 1036}, 'overall_precision': 0.448701012769705, 'overall_recall': 0.5711883408071748, 'overall_f1': 0.5025893958076448, 'overall_accuracy': 0.6795207635292924}\n",
            "------------EPOCH 23---------------\n",
            "Loss:  tensor(8.0873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.1663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3005, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.9982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9178, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.5198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.8765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.7607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3786, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.4249, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4360, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38009049773755654, 'recall': 0.5614973262032086, 'f1': 0.4533189422558014, 'number': 748}, 'P': {'precision': 0.5276073619631901, 'recall': 0.581081081081081, 'f1': 0.5530546623794211, 'number': 1036}, 'overall_precision': 0.45503116651825465, 'overall_recall': 0.5728699551569507, 'overall_f1': 0.5071960297766749, 'overall_accuracy': 0.6806376281856026}\n",
            "------------EPOCH 24---------------\n",
            "Loss:  tensor(6.9695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8416, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3261, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2955, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.4346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.4543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.5090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.4257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.9284, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3768888888888889, 'recall': 0.5668449197860963, 'f1': 0.45274959957287775, 'number': 748}, 'P': {'precision': 0.5255281690140845, 'recall': 0.5762548262548263, 'f1': 0.5497237569060772, 'number': 1036}, 'overall_precision': 0.4515701017249005, 'overall_recall': 0.5723094170403588, 'overall_f1': 0.5048207663782448, 'overall_accuracy': 0.6795207635292924}\n",
            "------------EPOCH 25---------------\n",
            "Loss:  tensor(6.1744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.1946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.7359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.0928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.3002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.7259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.5866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37710736468500444, 'recall': 0.5681818181818182, 'f1': 0.4533333333333334, 'number': 748}, 'P': {'precision': 0.5305580159433126, 'recall': 0.5781853281853282, 'f1': 0.5533487297921479, 'number': 1036}, 'overall_precision': 0.45390070921985815, 'overall_recall': 0.5739910313901345, 'overall_f1': 0.5069306930693068, 'overall_accuracy': 0.6809016143770941}\n",
            "------------EPOCH 26---------------\n",
            "Loss:  tensor(5.5668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.2117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.2095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.8279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7443, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8112, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2175, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.9877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3783303730017762, 'recall': 0.56951871657754, 'f1': 0.45464247598719315, 'number': 748}, 'P': {'precision': 0.5282685512367491, 'recall': 0.5772200772200772, 'f1': 0.551660516605166, 'number': 1036}, 'overall_precision': 0.4534986713906112, 'overall_recall': 0.5739910313901345, 'overall_f1': 0.5066798614547254, 'overall_accuracy': 0.6808000812265205}\n",
            "------------EPOCH 27---------------\n",
            "Loss:  tensor(5.0659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6540, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6580, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6517, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.8879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.1253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4675, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.9548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2665, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6566, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3806970509383378, 'recall': 0.56951871657754, 'f1': 0.4563470808784146, 'number': 748}, 'P': {'precision': 0.5259911894273128, 'recall': 0.5762548262548263, 'f1': 0.5499769691386457, 'number': 1036}, 'overall_precision': 0.4538598047914818, 'overall_recall': 0.5734304932735426, 'overall_f1': 0.5066864784546804, 'overall_accuracy': 0.6812468270890446}\n",
            "------------EPOCH 28---------------\n",
            "Loss:  tensor(4.6556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.6960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9651, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4810, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2443, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37980339588918677, 'recall': 0.5681818181818182, 'f1': 0.45527584359935724, 'number': 748}, 'P': {'precision': 0.5228471001757469, 'recall': 0.5743243243243243, 'f1': 0.547378104875805, 'number': 1036}, 'overall_precision': 0.4519273371732388, 'overall_recall': 0.5717488789237668, 'overall_f1': 0.5048255382331106, 'overall_accuracy': 0.6813889734998477}\n",
            "------------EPOCH 29---------------\n",
            "Loss:  tensor(4.3053, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4440, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.0625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9793, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.7348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37890974084003576, 'recall': 0.5668449197860963, 'f1': 0.4542046063203, 'number': 748}, 'P': {'precision': 0.5183887915936952, 'recall': 0.5714285714285714, 'f1': 0.5436179981634528, 'number': 1036}, 'overall_precision': 0.44935869084475893, 'overall_recall': 0.5695067264573991, 'overall_f1': 0.5023485784919653, 'overall_accuracy': 0.681287440349274}\n",
            "------------EPOCH 30---------------\n",
            "Loss:  tensor(3.9821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.5032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.7336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.4488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0527, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5170, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3775784753363229, 'recall': 0.5628342245989305, 'f1': 0.45195920558239405, 'number': 748}, 'P': {'precision': 0.5182926829268293, 'recall': 0.5743243243243243, 'f1': 0.5448717948717948, 'number': 1036}, 'overall_precision': 0.4489615554573575, 'overall_recall': 0.5695067264573991, 'overall_f1': 0.5021003212255991, 'overall_accuracy': 0.681368666869733}\n",
            "------------EPOCH 31---------------\n",
            "Loss:  tensor(3.6991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.3235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.7862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.6543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.7858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.5873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.375, 'recall': 0.5614973262032086, 'f1': 0.44967880085653106, 'number': 748}, 'P': {'precision': 0.5200348432055749, 'recall': 0.5762548262548263, 'f1': 0.5467032967032968, 'number': 1036}, 'overall_precision': 0.44841269841269843, 'overall_recall': 0.570067264573991, 'overall_f1': 0.501974333662389, 'overall_accuracy': 0.6810031475276678}\n",
            "------------EPOCH 32---------------\n",
            "Loss:  tensor(3.4459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.1620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.2843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.4115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3170, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5742, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.3795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37344028520499106, 'recall': 0.5601604278074866, 'f1': 0.4481283422459893, 'number': 748}, 'P': {'precision': 0.5213600697471665, 'recall': 0.5772200772200772, 'f1': 0.5478699038021072, 'number': 1036}, 'overall_precision': 0.4482150727192596, 'overall_recall': 0.570067264573991, 'overall_f1': 0.5018504811250926, 'overall_accuracy': 0.6809016143770941}\n",
            "------------EPOCH 33---------------\n",
            "Loss:  tensor(3.2208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.0156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5252, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8152, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7545, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.2041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.1934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.4917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.4822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37488789237668163, 'recall': 0.5588235294117647, 'f1': 0.44873859366612995, 'number': 748}, 'P': {'precision': 0.522154648132059, 'recall': 0.5801158301158301, 'f1': 0.5496113397347965, 'number': 1036}, 'overall_precision': 0.4496910856134157, 'overall_recall': 0.5711883408071748, 'overall_f1': 0.5032098765432098, 'overall_accuracy': 0.6809625342674384}\n",
            "------------EPOCH 34---------------\n",
            "Loss:  tensor(3.0168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.8853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4216, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.0606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.0210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.0267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.2758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3770343580470163, 'recall': 0.5574866310160428, 'f1': 0.4498381877022654, 'number': 748}, 'P': {'precision': 0.5268630849220104, 'recall': 0.5868725868725869, 'f1': 0.5552511415525114, 'number': 1036}, 'overall_precision': 0.45353982300884954, 'overall_recall': 0.5745515695067265, 'overall_f1': 0.506923837784372, 'overall_accuracy': 0.6820184790334044}\n",
            "------------EPOCH 35---------------\n",
            "Loss:  tensor(2.8339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.6683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.7696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.8617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.4968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.8790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.0921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3770343580470163, 'recall': 0.5574866310160428, 'f1': 0.4498381877022654, 'number': 748}, 'P': {'precision': 0.5259067357512953, 'recall': 0.5878378378378378, 'f1': 0.5551504102096627, 'number': 1036}, 'overall_precision': 0.4531802120141343, 'overall_recall': 0.5751121076233184, 'overall_f1': 0.5069169960474308, 'overall_accuracy': 0.6824652248959285}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mi7LRmfIYbR"
      },
      "source": [
        "### Rough -- Checking dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9kGw_svbXM",
        "outputId": "b2838b14-a51e-4129-bb8c-20b274bb7a1c"
      },
      "source": [
        "\" \".join(\" mY name is \".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mY name is'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-oVkplUCJh"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qMGQ5GOzbn"
      },
      "source": [
        "train_dataset, _, test_dataset = get_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdpyzvKIfM8",
        "outputId": "8531f009-4bed-4ea9-daa4-d76d023d52e9"
      },
      "source": [
        "for tokenized_threads, masked_threads, comp_type_labels, _ in test_dataset:\n",
        "    tokenized_threads, masked_threads, comp_type_labels = tokenized_threads[0], masked_threads[0], comp_type_labels[0]\n",
        "    for tokenized_thread, masked_thread, comp_type_label in zip(tokenized_threads, masked_threads, comp_type_labels):\n",
        "        print(comp_type_label[:100])\n",
        "        print(tokenized_thread[:100])\n",
        "        print(tokenizer.decode(tokenized_thread[:500]))\n",
        "        start, end = 0, 0\n",
        "        prev_type = \"other\"\n",
        "        i = 0\n",
        "        while i<tokenized_thread.shape[0]:\n",
        "            if comp_type_label[i]==ac_dict[\"O\"]:\n",
        "                if prev_type==\"other\":\n",
        "                    end += 1\n",
        "                else:\n",
        "                    print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                    print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                    start = i\n",
        "                    end = i\n",
        "                    prev_type=\"other\"\n",
        "                \n",
        "            if comp_type_label[i] in [ac_dict[\"B-C\"], ac_dict[\"B-P\"]]:\n",
        "                print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                start = i\n",
        "                end = i\n",
        "                prev_type = \"Claim\" if comp_type_label[i]==ac_dict[\"B-C\"] else \"Premise\"\n",
        "            \n",
        "            if comp_type_label[i] in [ac_dict[\"I-C\"], ac_dict[\"I-P\"]]:\n",
        "                end += 1\n",
        "            \n",
        "            i+=1\n",
        "        break\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
            " 2 2 2 2 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4]\n",
            "[    0 18814   846    35  7978     9  1901    16   145   551   350   444\n",
            " 50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268\n",
            "   100  1819   923    84   481  1901    53     7   162  1437  8585    16\n",
            "    10   699   516   227 20203   110    78  8322   235    36  1437    22\n",
            "   270  1284 29384   328]\n",
            "<s>CMV: Freedom of speech is being taken too far [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE] I certainly value our free speech but to me there is a clear line between exercising your first amendment right (  \" President Obama sucks! \" etc ) and doing things that are known to be offensive to other cultures (  Satirical cartoons of prophets, assassinating leaders, etc ). [NEWLINE] [NEWLINE]  Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE] Sure, but that doesn't mean we condone the bully's actions and don't punish the bullies for acting. [NEWLINE] We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments. [NEWLINE]  Complete freedom in the expression of any idea, offensive or not, is a major element of that world. [NEWLINE] [NEWLINE]  If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us. [NEWLINE] [NEWLINE] [USER0] [NEWLINE] I certainly agree with your points - I didn't mean to imply that I was only for * * some * * freedom of speech. [NEWLINE] I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset. [NEWLINE] [NEWLINE]  Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"  [NEWLINE] Is that so hard? [NEWLINE] [NEWLINE] </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Component:  <s>CM  of type:  other [    0 18814]\n",
            "Masked Component:  <s>CM  of type:  other [    0 18814]\n",
            "Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Masked Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Masked Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Masked Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Component:   but to me   of type:  other [  53    7  162 1437]\n",
            "Masked Component:  <mask> to me   of type:  other [50264     7   162  1437]\n",
            "Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Masked Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Masked Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Masked Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Component:   (   of type:  other [  36 1437]\n",
            "Masked Component:   (   of type:  other [  36 1437]\n",
            "Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Masked Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Masked Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Component:   Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111    53   114    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Masked Component:   Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111 50264 50264    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            "    53   114    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Masked Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            " 50264 50264    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Component:  Sure  of type:  Claim [32541]\n",
            "Masked Component:  Sure  of type:  Claim [32541]\n",
            "Component:  , but   of type:  other [   6   53 1437]\n",
            "Masked Component:  ,<mask>   of type:  other [    6 50264  1437]\n",
            "Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Masked Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Masked Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Masked Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [ 318   47  236    7 1744    5 3519    7 1994   13  143    9  201    6\n",
            "   47   33    7 1744    5 3519    7 1994   13   70    9  201]\n",
            "Masked Component:  <mask> you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [50264    47   236     7  1744     5  3519     7  1994    13   143     9\n",
            "   201     6    47    33     7  1744     5  3519     7  1994    13    70\n",
            "     9   201]\n",
            "Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Masked Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Component:   -   of type:  other [ 111 1437]\n",
            "Masked Component:   -   of type:  other [ 111 1437]\n",
            "Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Masked Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901    77  1402  1134\n",
            "   120  4904]\n",
            "Masked Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech<mask> certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901 50264  1402  1134\n",
            "   120  4904]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"   of type:  Premise [ 2612    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45   142\n",
            "    52   214  6023     9    47    53   142    52  2098   110  2728     4\n",
            "    22  1437]\n",
            "Masked Component:  <mask> can't America be the bigger person and say \" Ok, we won't publish certain types of material, not<mask> we're afraid of you<mask><mask> we respect your views. \"   of type:  Premise [50264    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45 50264\n",
            "    52   214  6023     9    47 50264 50264    52  2098   110  2728     4\n",
            "    22  1437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZnb1Nz-MX4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2601bebd-3989-40a5-af7d-91d578651823"
      },
      "source": [
        "import re\n",
        "re.sub(r\"\\s*</claim>([^\\s])\", r\"</claim> \\1\", \"<claim>my name is </claim>jeevesh.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<claim>my name is</claim> jeevesh.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UVJb5jy178"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}