{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "long_context_am.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMy1sLMWMbl5QZmWJ3/9Noq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/arg_mining/blob/main/experiments/long_context_am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGdooHmfgd-"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GPY8HXWtkyE"
      },
      "source": [
        "%%capture\n",
        "#if running on colab, install below 4\n",
        "#!git clone https://github.com/Jeevesh8/arg_mining\n",
        "#!pip install transformers\n",
        "#!pip install seqeval datasets allennlp\n",
        "#!pip install flax\n",
        "\n",
        "#if connected to local runtime, run the next command too\n",
        "#pip install bs4 tensorflow torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8QzJCbx9lO"
      },
      "source": [
        "\n",
        "\n",
        "*   Update ``arg_mining/datasets/cmv_modes/configs.py`` as per your requirements, all experiments considered till now, set ``batch_size`` to 2, and all other variables with their default value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glbajGinKG4"
      },
      "source": [
        "#Run to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pCBwYZjfkHw"
      },
      "source": [
        "### Load Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKUqJo4e_Rc"
      },
      "source": [
        "%%capture\n",
        "from datasets import load_metric\n",
        "metric = load_metric('seqeval')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kJb8EE0Fm-"
      },
      "source": [
        "### Krippendorff's Alpha Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1XOXUu0FOw"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class krip_alpha():\n",
        "    \"\"\"A module for computing sentence level Krippendorff's Alpha,\n",
        "    for argumentative components  annotated at the token level. Must use\n",
        "    labels [\"B-C\", \"B-P\"].\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"See self.compute_metric() for what each of these data actually mean.\n",
        "        \"\"\"\n",
        "        self.pred_has_claim = 0\n",
        "        self.ref_has_claim = 0\n",
        "        self.pred_has_premise = 0\n",
        "        self.ref_has_premise = 0\n",
        "        \n",
        "        self.claim_wise_agreement = 0\n",
        "        self.premise_wise_agreement = 0\n",
        "        \n",
        "        self.claim_wise_disagreement = 0\n",
        "        self.premise_wise_disagreement = 0\n",
        "    \n",
        "        self.total_sentences = 0\n",
        "        \n",
        "        self.has_both_ref = 0\n",
        "        self.has_both_pred = 0\n",
        "        self.has_none_ref = 0\n",
        "        self.has_none_pred = 0\n",
        "\n",
        "    def preprocess(self, threads: List[List[int]]) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            threads:    A list of all threads in a batch. A thread is a list of \n",
        "                        integers corresponding to token_ids of the tokens in the \n",
        "                        thread.\n",
        "        Returns:\n",
        "            A List with all the threads, where each thread now consists of \n",
        "            sentence lists. Where, a sentence list in a thread list is the list \n",
        "            of token_ids corresponding to a sentence in a thread. \n",
        "        \"\"\"\n",
        "        threads_lis = []\n",
        "\n",
        "        for i, thread in enumerate(threads):\n",
        "            sentence = []\n",
        "            threads_lis.append([])\n",
        "            for j, token_id in enumerate(thread):\n",
        "                if token_id==tokenizer.pad_token_id:\n",
        "                    break\n",
        "                \n",
        "                sentence.append(token_id)\n",
        "                token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "                #print(\"appended token:\", token)\n",
        "\n",
        "                next_token = 'None' if j==len(thread) else tokenizer.convert_ids_to_tokens(thread[j+1])\n",
        "\n",
        "                if (token.count('.')+token.count('?')+token.count('!')>=1 and \n",
        "                    next_token.count('.')+next_token.count('?')+next_token.count('!')==0):\n",
        "\n",
        "                    threads_lis[i].append(sentence)\n",
        "                    #print(\"Sample sentence: \", tokenizer.decode(sentence))\n",
        "                    sentence = []\n",
        "                \n",
        "                elif re.findall(r\"\\[USER\\d+\\]|\\[UNU\\]\", token)!=[]:\n",
        "                    prev_part = tokenizer.decode(sentence[:-1])[1:-1]\n",
        "                    if re.search(r'[a-zA-Z]', prev_part) is not None:\n",
        "                        threads_lis[i].append(sentence[:-1])\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(sentence[:-1]))\n",
        "                        sentence = [sentence[-1]]\n",
        "                    else:\n",
        "                        k=len(sentence)-2\n",
        "                        while k>=0 and sentence[k]==tokenizer.convert_tokens_to_ids('Ġ'):\n",
        "                            k-=1\n",
        "                        sentence = sentence[k+1:]\n",
        "                        threads_lis[i][-1] += sentence[:k]\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(threads_lis[i][-1]))\n",
        "                \n",
        "            has_rem_token = False\n",
        "            for elem in sentence:\n",
        "                if (elem!=tokenizer.convert_tokens_to_ids('Ġ') and\n",
        "                    elem!=tokenizer.eos_token_id):\n",
        "                    has_rem_token = True\n",
        "                    break\n",
        "            \n",
        "            if has_rem_token:\n",
        "                threads_lis[i].append(sentence)\n",
        "                #print(\"Sample sentence at end of thread: \", tokenizer.decode(sentence))\n",
        "                sentence = []\n",
        "\n",
        "        return threads_lis\n",
        "\n",
        "    def get_sentence_wise_preds(self, threads: List[List[List[int]]], \n",
        "                                      predictions: List[List[str]]) -> List[List[List[str]]]:\n",
        "        \"\"\"Splits the prediction corresponding to each thread, into predictions\n",
        "        for each sentence in the corresponding thread in \"threads\" list.\n",
        "        Args:\n",
        "            threads:      A list of threads, where each thread consists of further \n",
        "                          lists corresponding to the various sentences in the\n",
        "                          thread. [As output by self.preprocess()]\n",
        "            predictions:  A list of predictions for each thread, in the threads\n",
        "                          list. Each prediciton consists of a list of componenet \n",
        "                          types corresponding to each token in a thread.\n",
        "        Returns:\n",
        "            The predictions list, with each prediction split into predictions \n",
        "            corresponding to the sentences in the corresponding thread specified\n",
        "            in the threads list. \n",
        "        \"\"\"\n",
        "        sentence_wise_preds = []\n",
        "        for i, thread in enumerate(threads):\n",
        "            next_sentence_beg = 0\n",
        "            sentence_wise_preds.append([])\n",
        "            for sentence in thread:\n",
        "                sentence_wise_preds[i].append(\n",
        "                    predictions[i][next_sentence_beg:next_sentence_beg+len(sentence)])\n",
        "                next_sentence_beg += len(sentence)\n",
        "        return sentence_wise_preds\n",
        "    \n",
        "    def update_state(self, pred_sentence: List[str], ref_sentence: List[str]) -> None:\n",
        "        \"\"\"Updates the various information maintained for the computation of\n",
        "        Krippendorff's alpha, based on the predictions(pred_sentence) and \n",
        "        references(ref_sentence) provided for a particular sentence, in some \n",
        "        thread.\n",
        "        \"\"\"\n",
        "        self.total_sentences += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence:\n",
        "            self.pred_has_claim += 1\n",
        "            if 'B-C' in ref_sentence:\n",
        "                self.ref_has_claim += 1\n",
        "                self.claim_wise_agreement += 1\n",
        "            else:\n",
        "                self.claim_wise_disagreement += 1\n",
        "            \n",
        "        elif 'B-C' in ref_sentence:\n",
        "            self.ref_has_claim += 1\n",
        "            self.claim_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.claim_wise_agreement += 1\n",
        "        \n",
        "        if 'B-P' in pred_sentence:\n",
        "            self.pred_has_premise += 1\n",
        "            if 'B-P' in ref_sentence:\n",
        "                self.ref_has_premise += 1\n",
        "                self.premise_wise_agreement += 1\n",
        "            else:\n",
        "                self.premise_wise_disagreement += 1\n",
        "\n",
        "        elif 'B-P' in ref_sentence:\n",
        "            self.ref_has_premise += 1\n",
        "            self.premise_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.premise_wise_agreement += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence and 'B-P' in pred_sentence:\n",
        "            self.has_both_pred += 1\n",
        "        \n",
        "        if 'B-C' in ref_sentence and 'B-P' in ref_sentence:\n",
        "            self.has_both_ref += 1\n",
        "        \n",
        "        if 'B-C' not in pred_sentence and 'B-P' not in pred_sentence:\n",
        "            self.has_none_pred += 1\n",
        "        \n",
        "        if 'B-C' not in ref_sentence and 'B-P' not in ref_sentence:\n",
        "            self.has_none_ref += 1\n",
        "        return\n",
        "\n",
        "    def add_batch(self, predictions: List[List[str]], \n",
        "                  references: List[List[str]], \n",
        "                  tokenized_threads: List[List[int]]) -> None:\n",
        "        \"\"\"Add a batch of predictions and references for the computation of \n",
        "        Krippendorff's alpha.\n",
        "        Args:\n",
        "            predictions:      A list of predictions for each thread, in the \n",
        "                              threads list. Each prediciton consists of a list \n",
        "                              of component types corresponding to each token in \n",
        "                              a thread.\n",
        "            references:       Same structure as predictions, but consisting of \n",
        "                              acutal gold labels, instead of predicted ones.\n",
        "            tokenized_thread: A list of all threads in a batch. A thread is a \n",
        "                              list of integers corresponding to token_ids of the\n",
        "                              tokens in the thread.\n",
        "        \"\"\"\n",
        "        threads = self.preprocess(tokenized_threads)\n",
        "        \n",
        "        sentence_wise_preds = self.get_sentence_wise_preds(threads, predictions)\n",
        "        sentence_wise_refs = self.get_sentence_wise_preds(threads, references)\n",
        "\n",
        "        for pred_thread, ref_thread in zip(sentence_wise_preds, sentence_wise_refs):\n",
        "            for pred_sentence, ref_sentence in zip(pred_thread, ref_thread):\n",
        "                self.update_state(pred_sentence, ref_sentence)\n",
        "\n",
        "    def compute(self, print_additional: bool=True) -> None:\n",
        "        \"\"\"Prints out the metric, for the batched added till now. And then \n",
        "        resets all data being maintained by the metric. \n",
        "        Args:\n",
        "            print_additional:   If True, will print all the data being \n",
        "                                maintained instead of just the Krippendorff's \n",
        "                                alphas for claims and premises.\n",
        "        \"\"\"\n",
        "        print(\"Sentence level Krippendorff's alpha for Claims: \", 1-(self.claim_wise_disagreement/(self.claim_wise_agreement+self.claim_wise_disagreement))/0.5)\n",
        "        print(\"Sentence level Krippendorff's alpha for Premises: \", 1-(self.premise_wise_disagreement/(self.premise_wise_agreement+self.premise_wise_disagreement))/0.5)\n",
        "        \n",
        "        if print_additional:\n",
        "            print(\"Additional attributes: \")\n",
        "            print(\"\\tTotal Sentences:\", self.total_sentences)\n",
        "            print(\"\\tPrediction setences having claims:\", self.pred_has_claim)\n",
        "            print(\"\\tPrediction sentences having premises:\", self.pred_has_premise)\n",
        "            print(\"\\tReference setences having claims:\", self.ref_has_claim)\n",
        "            print(\"\\tReference sentences having premises:\", self.ref_has_premise)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tPrediction Sentence having both claim and premise:\", self.has_both_pred)\n",
        "            print(\"\\tPrediction Sentence having neither claim nor premise:\", self.has_none_pred)\n",
        "            print(\"\\tReference Sentence having both claim and premise:\", self.has_both_ref)\n",
        "            print(\"\\tReference Sentence having neither claim nor premise:\", self.has_none_ref)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tSentences having claim in both reference and prediction:\", self.claim_wise_agreement)\n",
        "            print(\"\\tSentences having claim in only one of reference or prediction:\", self.claim_wise_disagreement)\n",
        "            print(\"\\tSentences having premise in both reference and prediction:\", self.premise_wise_agreement)\n",
        "            print(\"\\tSentences having premise in only one of reference or prediction:\", self.premise_wise_disagreement)\n",
        "        self.__init__()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRNv0wBWtFd"
      },
      "source": [
        "metric = krip_alpha()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6BxMkfm3R"
      },
      "source": [
        "### Define & Load Tokenizer, Model, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wcsqmllnfRB"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DeXxCDS_id",
        "outputId": "747ee16d-6020-4867-975c-ee5b9a889924"
      },
      "source": [
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v938tu5rydoT"
      },
      "source": [
        "#### Load Model/Tokenizer from HF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRkCQOZu6HS"
      },
      "source": [
        "model_version = 'allenai/longformer-base-4096'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cB-7M9t0HP"
      },
      "source": [
        "%%capture\n",
        "from transformers import LongformerTokenizer, AutoModel\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_version)\n",
        "transformer_model = AutoModel.from_pretrained(model_version).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4IWXMOymZk"
      },
      "source": [
        "#### Or load them from pretrained files..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNf-8LQVyliT",
        "outputId": "edcc92cf-2aef-482a-8780-882ab8a4290f"
      },
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "\n",
        "tokenizer = LongformerTokenizer.from_pretrained('./4epoch_complete/tokenizer/')\n",
        "transformer_model = LongformerModel.from_pretrained('./4epoch_complete/model/').to(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./4epoch_complete/model/ were not used when initializing LongformerModel: ['lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.dense.weight']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at ./4epoch_complete/model/ and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEV4yTy11zUm"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4PwtV9zai9"
      },
      "source": [
        "#### To add extra token type embeddings..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajTzrbzkwbT"
      },
      "source": [
        "def resize_token_type_embeddings(transformer_model, new_size):\n",
        "    old_embeddings = transformer_model.embeddings.token_type_embeddings.weight\n",
        "    old_size, hidden_dim = old_embeddings.shape\n",
        "    transformer_model.embeddings.token_type_embeddings = nn.Embedding(new_size, hidden_dim, device=transformer_model.device)\n",
        "    with torch.no_grad():\n",
        "        transformer_model.embeddings.token_type_embeddings.weight[:old_size] = old_embeddings\n",
        "\n",
        "resize_token_type_embeddings(transformer_model, 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHJiT0sjXid"
      },
      "source": [
        "transformer_model.config.type_vocab_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZ89akNzoMo"
      },
      "source": [
        "#### Load in discourse markers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p6dGA83cGVS"
      },
      "source": [
        "with open('./Discourse_Markers.txt') as f:\n",
        "    discourse_markers = [dm.strip() for dm in f.readlines()]"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bQWcCd1p1G"
      },
      "source": [
        "%%capture\n",
        "from arg_mining.datasets.cmv_modes import load_dataset, data_config"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5O1Wm7Dzqqe"
      },
      "source": [
        "#### Add special tokens to tokenizer and model vocab, if not already there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkOZsiCzl1k"
      },
      "source": [
        "tokenizer.add_tokens(data_config[\"special_tokens\"])\n",
        "\n",
        "transformer_model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh32PtIEz1mF"
      },
      "source": [
        "#### Function to get train, test data (80/20 split currently)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCeCgbLxVyQ"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=50,\n",
        "                                                              test_sz=50,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_8gVbufzoX"
      },
      "source": [
        "### Define layers for a Linear-Chain-CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seBwkZdByesM"
      },
      "source": [
        "from allennlp.modules.conditional_random_field import ConditionalRandomField as crf\n",
        "\n",
        "ac_dict = data_config[\"arg_components\"]\n",
        "\n",
        "allowed_transitions =([(ac_dict[\"B-C\"], ac_dict[\"I-C\"]), \n",
        "                       (ac_dict[\"B-P\"], ac_dict[\"I-P\"])] + \n",
        "                      [(ac_dict[\"I-C\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-C\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"I-P\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-P\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"O\"], ac_dict[ct]) \n",
        "                        for ct in [\"O\", \"B-C\", \"B-P\"]])\n",
        "                    \n",
        "linear_layer = nn.Linear(transformer_model.config.hidden_size,\n",
        "                         len(ac_dict)).to(device)\n",
        "\n",
        "crf_layer = crf(num_tags=len(ac_dict),\n",
        "                constraints=allowed_transitions,\n",
        "                include_start_end_transitions=False).to(device)\n",
        "\n",
        "cross_entropy_layer = nn.CrossEntropyLoss(weight=torch.log(torch.tensor([3.3102, 61.4809, 3.6832, 49.6827, 2.5639], \n",
        "                                                                        device=device)), reduction='none')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUicsK33f9d7"
      },
      "source": [
        "### Global Attention Mask Utility for Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSo7p2I5mOi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n",
        "    0 elsewhere.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(tokenized_threads)\n",
        "    for user_token in [\"UNU\"]+[f\"[USER{i}]\" for i in range(data_config[\"max_users\"])]:\n",
        "        user_token_id = tokenizer.encode(user_token)[1:-1]\n",
        "        mask = np.where(tokenized_threads==user_token_id, 1, mask)\n",
        "    return np.array(mask, dtype=bool)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4PLQihf5CT"
      },
      "source": [
        "### Loss and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3u8eH1rjZe"
      },
      "source": [
        "from typing import Tuple"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuJ5aryW9tUC"
      },
      "source": [
        "def compute(batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            preds: bool=False, cross_entropy: bool=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n",
        "                component type labels of shape [batch_size, seq_len], and a global\n",
        "                attention mask for Longformer, of the same shape.\n",
        "        \n",
        "        preds:  If True, returns a List(of batch_size size) of Tuples of form \n",
        "                (tag_sequence, viterbi_score) where the tag_sequence is the \n",
        "                viterbi-decoded sequence, for the corresponding sample in the batch.\n",
        "        \n",
        "        cross_entropy:  This argument will only be used if preds=False, i.e., if \n",
        "                        loss is being calculated. If True, then cross entropy loss\n",
        "                        will also be added to the output loss.\n",
        "    \n",
        "    Returns:\n",
        "        Either the predicted sequences with their scores for each element in the batch\n",
        "        (if preds is True), or the loss value summed over all elements of the batch\n",
        "        (if preds is False).\n",
        "    \"\"\"\n",
        "    tokenized_threads, token_type_ids, comp_type_labels, global_attention_mask = batch\n",
        "    \n",
        "    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n",
        "    \n",
        "    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n",
        "                                            attention_mask=pad_mask,\n",
        "                                            global_attention_mask=global_attention_mask).last_hidden_state)\n",
        "    \n",
        "    if preds:\n",
        "        return crf_layer.viterbi_tags(logits, pad_mask)\n",
        "    \n",
        "    log_likelihood = crf_layer(logits, comp_type_labels, pad_mask)\n",
        "    \n",
        "    if cross_entropy:\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        \n",
        "        pad_mask, comp_type_labels = pad_mask.reshape(-1), comp_type_labels.reshape(-1)\n",
        "        \n",
        "        ce_loss = torch.sum(pad_mask*cross_entropy_layer(logits, comp_type_labels))\n",
        "        \n",
        "        return ce_loss - log_likelihood\n",
        "\n",
        "    return -log_likelihood"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lkPCsgEY4"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yfpzEMBGra"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n",
        "                                      linear_layer.parameters(),\n",
        "                                      crf_layer.parameters()),\n",
        "                       lr = 2e-5,)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSWnkO8gLD6"
      },
      "source": [
        "### Training And Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTNaP3kLaN2X"
      },
      "source": [
        "def train(dataset):\n",
        "    accumulate_over = 4\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (tokenized_threads, masked_threads, comp_type_labels, _ ) in enumerate(dataset):\n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads),\n",
        "                                             device=device, dtype=torch.int32)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0), \n",
        "                                         device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0), \n",
        "                                      device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0), \n",
        "                                        device=device, dtype=torch.long)\n",
        "        \n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        loss = compute((tokenized_threads,\n",
        "                        torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                        comp_type_labels, \n",
        "                        global_attention_mask))/data_config[\"batch_size\"]\n",
        "        \n",
        "        print(\"Loss: \", loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        if i%accumulate_over==accumulate_over-1:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BpSI83cU24"
      },
      "source": [
        "def evaluate(dataset, metric):\n",
        "    \n",
        "    int_to_labels = {v:k for k, v in ac_dict.items()}\n",
        "    \n",
        "    for tokenized_threads, masked_threads, comp_type_labels, _ in dataset:\n",
        "        \n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads), \n",
        "                                             device=device)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0),\n",
        "                                        device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0),\n",
        "                                     device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0),\n",
        "                                        device=device)\n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        preds = compute((tokenized_threads,\n",
        "                         torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                         comp_type_labels,\n",
        "                         global_attention_mask),\n",
        "                        preds=True)\n",
        "        \n",
        "        lengths = torch.sum(torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0), \n",
        "                            axis=-1)\n",
        "        \n",
        "        preds = [ [int_to_labels[pred] for pred in pred[0][:lengths[i]]]\n",
        "                  for i, pred in enumerate(preds)\n",
        "                ]\n",
        "        \n",
        "        refs = [ [int_to_labels[ref] for ref in labels[:lengths[i]]]\n",
        "                 for i, labels in enumerate(comp_type_labels.cpu().tolist())\n",
        "               ]\n",
        "        \n",
        "        metric.add_batch(predictions=preds, \n",
        "                         references=refs,)\n",
        "                         #tokenized_threads=tokenized_threads.cpu().tolist())\n",
        "    \n",
        "    print(metric.compute())"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZV6rnIQgOYA"
      },
      "source": [
        "### Final Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2O5qCufGwA"
      },
      "source": [
        "n_epochs = 35"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xcK9sOgX44",
        "outputId": "71660684-9311-4eae-aace-f5ea640fa3dd"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    print(f\"------------EPOCH {epoch+1}---------------\")\n",
        "    train_dataset, _, test_dataset = get_datasets()\n",
        "    train(train_dataset)\n",
        "    evaluate(test_dataset, metric)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------EPOCH 1---------------\n",
            "Loss:  tensor(2662.7983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1993.1859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1798.4768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2255.1416, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3383.7368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2618.2593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3064.5889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3202.7256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2894.2588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2760.7642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2424.2334, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2671.1470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3606.2119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2277.6997, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2838.8088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(875.9063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1066.8961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1790.6411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1033.6230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1803.8922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2109.0906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1739.7537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2691.6064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6230.4795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3721.0093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1319.9841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1274.9365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1913.2214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2896.0920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.02418207681365576, 'recall': 0.022727272727272728, 'f1': 0.023432115782219164, 'number': 748}, 'P': {'precision': 0.02702702702702703, 'recall': 0.0009652509652509653, 'f1': 0.001863932898415657, 'number': 1036}, 'overall_precision': 0.024324324324324326, 'overall_recall': 0.010089686098654708, 'overall_f1': 0.014263074484944531, 'overall_accuracy': 0.36054421768707484}\n",
            "------------EPOCH 2---------------\n",
            "Loss:  tensor(2107.8308, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1452.8328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1292.0394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1673.3383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2539.4036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1913.4812, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2399.4453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2319.2163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2250.5732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2153.4026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1971.2872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2225.7637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2984.9895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1844.5874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2268.4863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(730.8472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(921.5944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1553.8220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(936.3783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1477.8357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1865.8311, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1561.9392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2329.5562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5298.6279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3141.3623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1129.4827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1062.9561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1699.6251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2466.4832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.06802721088435375, 'recall': 0.040106951871657755, 'f1': 0.05046257359125315, 'number': 748}, 'P': {'precision': 0.09118852459016394, 'recall': 0.0859073359073359, 'f1': 0.08846918489065606, 'number': 1036}, 'overall_precision': 0.0839802399435427, 'overall_recall': 0.06670403587443946, 'overall_f1': 0.07435176507341457, 'overall_accuracy': 0.5388567367245406}\n",
            "------------EPOCH 3---------------\n",
            "Loss:  tensor(1689.4562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1398.7480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1224.3005, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1588.9708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2085.6484, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1634.0432, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2161.3064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2096.8042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1935.0911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1907.1357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1678.4983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1948.8143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2675.3149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1715.0945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2016.4832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(680.5791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(794.8375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1422.9240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(761.4747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1252.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1669.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1218.8618, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1974.2161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4561.6250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2547.2302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1029.3281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(970.8912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1469.9686, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2170.0830, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.11564625850340136, 'recall': 0.06818181818181818, 'f1': 0.08578637510513036, 'number': 748}, 'P': {'precision': 0.15553977272727273, 'recall': 0.21138996138996138, 'f1': 0.17921440261865793, 'number': 1036}, 'overall_precision': 0.1460248783126014, 'overall_recall': 0.15134529147982062, 'overall_f1': 0.1486374896779521, 'overall_accuracy': 0.5956340745253326}\n",
            "------------EPOCH 4---------------\n",
            "Loss:  tensor(1282.2720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1135.3379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(924.8440, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1255.1470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1704.7107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1362.9868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1893.9406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1949.2368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1757.3489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1654.4963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1398.3218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1653.9924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2305.7278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1444.6373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1719.5348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(603.0465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(712.3755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1275.4683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(676.0226, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1047.7029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1430.5675, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1031.6448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1741.3630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3957.0947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2055.5933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(916.3017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(844.5355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1263.2844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1890.3965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.173015873015873, 'recall': 0.14572192513368984, 'f1': 0.158200290275762, 'number': 748}, 'P': {'precision': 0.23175487465181058, 'recall': 0.4015444015444015, 'f1': 0.2938890851289297, 'number': 1036}, 'overall_precision': 0.21649484536082475, 'overall_recall': 0.2942825112107623, 'overall_f1': 0.2494654312188168, 'overall_accuracy': 0.6285308153111991}\n",
            "------------EPOCH 5---------------\n",
            "Loss:  tensor(986.4026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(843.1193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(623.2952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(931.2916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1430.4320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1097.5006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1680.6804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1782.5776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1544.1127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1376.6133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1182.8711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1416.4641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1954.6648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1211.7573, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1461.9473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(514.9373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(590.7867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1136.6074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(604.1929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(863.7642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1234.5570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(866.8584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1492.9845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3286.9368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1547.7853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(721.6740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(628.8608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1031.6021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1577.3020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2161654135338346, 'recall': 0.1537433155080214, 'f1': 0.1796875, 'number': 748}, 'P': {'precision': 0.3932225063938619, 'recall': 0.5936293436293436, 'f1': 0.47307692307692306, 'number': 1036}, 'overall_precision': 0.3482824427480916, 'overall_recall': 0.40919282511210764, 'overall_f1': 0.3762886597938145, 'overall_accuracy': 0.649487257589603}\n",
            "------------EPOCH 6---------------\n",
            "Loss:  tensor(723.2817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(672.9234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(453.3738, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(739.3848, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1251.2999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(826.0515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1388.1272, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1567.8257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1312.6241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1150.3103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(960.6674, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1126.2325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1598.1392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(974.6933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1231.1901, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(432.5097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(505.9556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1007.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(562.8180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(678.7406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1019.6661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(650.1343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1304.9475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2534.4998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1034.9875, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(533.1038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(439.5099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(800.9584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1293.6824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.21813031161473087, 'recall': 0.10294117647058823, 'f1': 0.13987284287011809, 'number': 748}, 'P': {'precision': 0.42857142857142855, 'recall': 0.5675675675675675, 'f1': 0.4883720930232558, 'number': 1036}, 'overall_precision': 0.3855072463768116, 'overall_recall': 0.3727578475336323, 'overall_f1': 0.3790253633513822, 'overall_accuracy': 0.659132906894101}\n",
            "------------EPOCH 7---------------\n",
            "Loss:  tensor(576.5136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(549.4788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(389.8592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(645.9165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1091.3889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(651.8883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1168.0493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1514.2603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1130.4805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(915.6354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(801.7258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(891.2305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1366.2970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(831.2217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1078.0273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(325.9716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(405.3303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(830.8598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(479.6470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(525.6293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(949.5234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(557.6392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1326.9399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2390.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(750.7894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(431.2013, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(395.2048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(719.4754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1114.6355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.24122137404580152, 'recall': 0.21122994652406418, 'f1': 0.22523164647184607, 'number': 748}, 'P': {'precision': 0.35443037974683544, 'recall': 0.40540540540540543, 'f1': 0.3782080144079244, 'number': 1036}, 'overall_precision': 0.3141304347826087, 'overall_recall': 0.3239910313901345, 'overall_f1': 0.3189845474613687, 'overall_accuracy': 0.661326022946492}\n",
            "------------EPOCH 8---------------\n",
            "Loss:  tensor(524.3041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(334.5742, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(270.0814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(430.5532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1039.7197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(572.7543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1040.3508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1339.8662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1049.8912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(844.2064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(776.7166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(788.0290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1653.3879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(989.0626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1189.6687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.1664, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(549.8599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(951.2137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(643.6266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(693.4657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(710.3819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(452.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(874.3898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1766.6553, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(560.5511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(327.5807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(283.7765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(612.3531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1811.9894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2489561586638831, 'recall': 0.6377005347593583, 'f1': 0.3581081081081081, 'number': 748}, 'P': {'precision': 0.3484848484848485, 'recall': 0.0222007722007722, 'f1': 0.041742286751361164, 'number': 1036}, 'overall_precision': 0.2522704339051463, 'overall_recall': 0.2802690582959641, 'overall_f1': 0.2655337227827934, 'overall_accuracy': 0.44613666362067217}\n",
            "------------EPOCH 9---------------\n",
            "Loss:  tensor(2099.0750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(802.5337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(619.3558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1056.0510, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2375.5571, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1370.9301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2226.3179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1516.9219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1077.1638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1071.8749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(928.8184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1108.4144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1294.6960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(821.1245, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(956.1349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(317.5290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(423.3283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(722.5152, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(380.3285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(583.8151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(965.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(751.9657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1219.3560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2836.0337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1697.0125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(614.3298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(471.2983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1082.3575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1427.6622, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3401015228426396, 'recall': 0.08957219251336898, 'f1': 0.14179894179894179, 'number': 748}, 'P': {'precision': 0.4143763213530655, 'recall': 0.7567567567567568, 'f1': 0.53551912568306, 'number': 1036}, 'overall_precision': 0.4073719483006223, 'overall_recall': 0.47701793721973096, 'overall_f1': 0.43945262070746194, 'overall_accuracy': 0.6606559041527058}\n",
            "------------EPOCH 10---------------\n",
            "Loss:  tensor(555.5422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(650.8454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(503.5833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(782.9257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1060.6843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(811.3176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1139.9832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1440.3264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1146.9702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(924.4003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(707.6649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(780.5483, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1436.2491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(833.3049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(979.1019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(322.8559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(402.1266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(672.6936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(358.1403, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(610.4526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(730.2481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(477.4688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(793.9854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1773.0808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(658.1248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(329.3422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(248.4341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(530.3414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(917.4600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.34782608695652173, 'recall': 0.5882352941176471, 'f1': 0.4371584699453552, 'number': 748}, 'P': {'precision': 0.503448275862069, 'recall': 0.49324324324324326, 'f1': 0.49829351535836175, 'number': 1036}, 'overall_precision': 0.41710526315789476, 'overall_recall': 0.5330717488789237, 'overall_f1': 0.468011811023622, 'overall_accuracy': 0.651761600162453}\n",
            "------------EPOCH 11---------------\n",
            "Loss:  tensor(467.8063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(268.4223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(224.9379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(385.7637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(737.5655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(400.1166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1023.6070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(866.3402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(690.2559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(651.0507, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(482.2695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(658.1323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(859.2541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(622.2330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(749.4249, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(215.4237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(221.2517, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(374.7067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(220.9285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(236.5118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(438.6188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.3184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(405.9391, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1113.3430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(526.3911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(300.0741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.2599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(463.3158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(653.8758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.34988713318284426, 'recall': 0.4144385026737968, 'f1': 0.379436964504284, 'number': 748}, 'P': {'precision': 0.5293040293040293, 'recall': 0.5579150579150579, 'f1': 0.543233082706767, 'number': 1036}, 'overall_precision': 0.448938321536906, 'overall_recall': 0.4977578475336323, 'overall_f1': 0.47208931419457745, 'overall_accuracy': 0.7054929434460351}\n",
            "------------EPOCH 12---------------\n",
            "Loss:  tensor(223.1167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(177.9892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(162.6980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.1433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(599.8938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(164.3940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(580.7226, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(620.7233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(449.4464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(420.7542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(273.9802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(449.6654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(744.6968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(469.9710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(541.9282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.6750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(175.8303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(245.6707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(158.5868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(165.8485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(345.3286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(113.7187, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.6821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(641.8566, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(197.9585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(131.4128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(132.7199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(239.3196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(402.3410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3824940047961631, 'recall': 0.4264705882352941, 'f1': 0.40328697850821743, 'number': 748}, 'P': {'precision': 0.4772727272727273, 'recall': 0.6486486486486487, 'f1': 0.5499181669394435, 'number': 1036}, 'overall_precision': 0.44201605709188224, 'overall_recall': 0.5554932735426009, 'overall_f1': 0.4923000496770989, 'overall_accuracy': 0.6846177276880901}\n",
            "------------EPOCH 13---------------\n",
            "Loss:  tensor(156.8717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(163.4940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.9515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(278.6347, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(638.3876, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(116.1388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(618.8456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(740.0273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(343.7933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(280.8716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(233.1773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(310.8533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(338.1255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(222.7918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(367.9393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.6171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(135.7264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.6089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.2071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.0021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(379.5334, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.1120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(277.9611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(690.8399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(172.7249, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(112.1717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.6723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(297.8474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(388.9496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3628593628593629, 'recall': 0.6243315508021391, 'f1': 0.458968058968059, 'number': 748}, 'P': {'precision': 0.5379975874547648, 'recall': 0.4305019305019305, 'f1': 0.47828418230563, 'number': 1036}, 'overall_precision': 0.43147448015122875, 'overall_recall': 0.5117713004484304, 'overall_f1': 0.46820512820512816, 'overall_accuracy': 0.6620976748908519}\n",
            "------------EPOCH 14---------------\n",
            "Loss:  tensor(155.8654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.0224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.5225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.5343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(346.5887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.4343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(311.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(302.9137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(209.2284, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(148.0569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.7703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(195.2774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.9787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.4331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(305.4278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.7370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.8016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.3456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.5298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(176.2489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(165.3655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.2824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(167.2289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(439.1661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.6626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.6578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.6435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(157.7770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(163.8095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.41163793103448276, 'recall': 0.5106951871657754, 'f1': 0.4558472553699284, 'number': 748}, 'P': {'precision': 0.5482787573467675, 'recall': 0.6303088803088803, 'f1': 0.5864391558149978, 'number': 1036}, 'overall_precision': 0.4884379424256725, 'overall_recall': 0.5801569506726457, 'overall_f1': 0.5303612605687933, 'overall_accuracy': 0.7036856533658239}\n",
            "------------EPOCH 15---------------\n",
            "Loss:  tensor(61.8315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.5631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.8522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.8215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(167.2123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.7544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(146.0797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(215.2425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(146.8585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.6759, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.3080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.9184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(137.9373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.4183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(199.8409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.2194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.2705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.6090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.9711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.3889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.5677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.1460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(213.8796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.4677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.1649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.5441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.4978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(106.1879, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39194139194139194, 'recall': 0.5721925133689839, 'f1': 0.4652173913043478, 'number': 748}, 'P': {'precision': 0.5667001003009027, 'recall': 0.5453667953667953, 'f1': 0.5558288243974422, 'number': 1036}, 'overall_precision': 0.47534705600765914, 'overall_recall': 0.5566143497757847, 'overall_f1': 0.5127807900852052, 'overall_accuracy': 0.696923545537618}\n",
            "------------EPOCH 16---------------\n",
            "Loss:  tensor(45.5065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.0685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.4230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.7797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.4165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(110.5464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(105.2726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.1525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.3595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.8003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(112.9362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.2934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.5986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.2062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.6411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.9202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.2406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.2664, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.3845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.2248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(154.4221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.1396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.3926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.8033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.8419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.1381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.41007194244604317, 'recall': 0.5334224598930482, 'f1': 0.46368390470656595, 'number': 748}, 'P': {'precision': 0.5577586206896552, 'recall': 0.6245173745173745, 'f1': 0.5892531876138433, 'number': 1036}, 'overall_precision': 0.4903891233005157, 'overall_recall': 0.5863228699551569, 'overall_f1': 0.5340822057697218, 'overall_accuracy': 0.707320540156361}\n",
            "------------EPOCH 17---------------\n",
            "Loss:  tensor(29.6944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.0596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.5373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.2650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.9766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.4807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.7132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.9040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.9171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.7176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.3956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.5092, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.7220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.3290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.4403, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.9480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(119.9534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.6762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.3503, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.3615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39285714285714285, 'recall': 0.5588235294117647, 'f1': 0.4613686534216336, 'number': 748}, 'P': {'precision': 0.5533333333333333, 'recall': 0.5608108108108109, 'f1': 0.5570469798657719, 'number': 1036}, 'overall_precision': 0.4725638599810785, 'overall_recall': 0.5599775784753364, 'overall_f1': 0.5125705489994868, 'overall_accuracy': 0.7013097776424002}\n",
            "------------EPOCH 18---------------\n",
            "Loss:  tensor(24.9214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.2365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.3731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.5248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.3867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.2975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.8240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.7559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.5588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.3734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.0881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(83.2679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8397, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.6950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.1361, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.1220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.5106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.42023346303501946, 'recall': 0.5775401069518716, 'f1': 0.4864864864864865, 'number': 748}, 'P': {'precision': 0.5765682656826568, 'recall': 0.6032818532818532, 'f1': 0.589622641509434, 'number': 1036}, 'overall_precision': 0.5004734848484849, 'overall_recall': 0.5924887892376681, 'overall_f1': 0.5426078028747434, 'overall_accuracy': 0.7098182556604732}\n",
            "------------EPOCH 19---------------\n",
            "Loss:  tensor(18.6508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.9712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.5808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.3818, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.9571, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.1796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.0958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.9960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.9938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.4773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.5039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.5405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8810, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.8662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.8199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.8197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.4712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4200196270853778, 'recall': 0.5721925133689839, 'f1': 0.4844368986983587, 'number': 748}, 'P': {'precision': 0.5702554744525548, 'recall': 0.6032818532818532, 'f1': 0.5863039399624764, 'number': 1036}, 'overall_precision': 0.4978723404255319, 'overall_recall': 0.5902466367713004, 'overall_f1': 0.5401384970505259, 'overall_accuracy': 0.7116864656310286}\n",
            "------------EPOCH 20---------------\n",
            "Loss:  tensor(15.1587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.6117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.6631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.0904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.8672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.4031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.1286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.2629, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.8916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.4588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.8441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2681, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.8539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.9904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.0315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.7657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.7626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.0899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.40613382899628253, 'recall': 0.5842245989304813, 'f1': 0.47916666666666663, 'number': 748}, 'P': {'precision': 0.5683520599250936, 'recall': 0.5859073359073359, 'f1': 0.5769961977186311, 'number': 1036}, 'overall_precision': 0.4869402985074627, 'overall_recall': 0.5852017937219731, 'overall_f1': 0.5315682281059063, 'overall_accuracy': 0.7071987003756727}\n",
            "------------EPOCH 21---------------\n",
            "Loss:  tensor(11.8656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.8656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1665, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.1985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.0705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.8128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.0865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.4680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5735, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.8993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.1719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3974475843208751, 'recall': 0.5828877005347594, 'f1': 0.4726287262872629, 'number': 748}, 'P': {'precision': 0.5664136622390892, 'recall': 0.5762548262548263, 'f1': 0.5712918660287081, 'number': 1036}, 'overall_precision': 0.4802417480241748, 'overall_recall': 0.5790358744394619, 'overall_f1': 0.5250317662007624, 'overall_accuracy': 0.7021017362168748}\n",
            "------------EPOCH 22---------------\n",
            "Loss:  tensor(9.3791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8360, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.1048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.3959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.5830, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.0825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.8134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.5274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.4708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.0367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.6710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.8965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3974475843208751, 'recall': 0.5828877005347594, 'f1': 0.4726287262872629, 'number': 748}, 'P': {'precision': 0.5696564885496184, 'recall': 0.5762548262548263, 'f1': 0.572936660268714, 'number': 1036}, 'overall_precision': 0.48158508158508156, 'overall_recall': 0.5790358744394619, 'overall_f1': 0.5258335454314075, 'overall_accuracy': 0.7016143770941212}\n",
            "------------EPOCH 23---------------\n",
            "Loss:  tensor(8.2795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.9048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.3586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.7267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.7741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5135, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.1406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.1937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6487, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.7142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38678414096916297, 'recall': 0.5868983957219251, 'f1': 0.4662772172065852, 'number': 748}, 'P': {'precision': 0.5766990291262136, 'recall': 0.5733590733590733, 'f1': 0.5750242013552759, 'number': 1036}, 'overall_precision': 0.4771362586605081, 'overall_recall': 0.5790358744394619, 'overall_f1': 0.5231704228918713, 'overall_accuracy': 0.6945476698141944}\n",
            "------------EPOCH 24---------------\n",
            "Loss:  tensor(6.5408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.0742, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.0193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9923, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.7881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.3260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.6956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.2217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3879539415411869, 'recall': 0.5855614973262032, 'f1': 0.4667021843367075, 'number': 748}, 'P': {'precision': 0.5704697986577181, 'recall': 0.5743243243243243, 'f1': 0.5723905723905723, 'number': 1036}, 'overall_precision': 0.47559852670349906, 'overall_recall': 0.5790358744394619, 'overall_f1': 0.5222446916076846, 'overall_accuracy': 0.6913392222560666}\n",
            "------------EPOCH 25---------------\n",
            "Loss:  tensor(30.3150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.5366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.2890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2445, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38219424460431656, 'recall': 0.5681818181818182, 'f1': 0.45698924731182794, 'number': 748}, 'P': {'precision': 0.5667627281460135, 'recall': 0.5694980694980695, 'f1': 0.5681271064034666, 'number': 1036}, 'overall_precision': 0.4714352066883418, 'overall_recall': 0.5689461883408071, 'overall_f1': 0.5156210312420625, 'overall_accuracy': 0.6914204487765255}\n",
            "------------EPOCH 26---------------\n",
            "Loss:  tensor(8.2256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.7811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0018, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.8477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.5243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.385981308411215, 'recall': 0.5521390374331551, 'f1': 0.45434543454345433, 'number': 748}, 'P': {'precision': 0.5663311985361391, 'recall': 0.5974903474903475, 'f1': 0.5814936589948332, 'number': 1036}, 'overall_precision': 0.47711511789181693, 'overall_recall': 0.57847533632287, 'overall_f1': 0.5229288066886243, 'overall_accuracy': 0.6947507361153417}\n",
            "------------EPOCH 27---------------\n",
            "Loss:  tensor(6.0700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.2764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1092, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.6803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3939393939393939, 'recall': 0.5735294117647058, 'f1': 0.46706586826347307, 'number': 748}, 'P': {'precision': 0.5636025998142989, 'recall': 0.5859073359073359, 'f1': 0.5745385707524845, 'number': 1036}, 'overall_precision': 0.47830101569713757, 'overall_recall': 0.5807174887892377, 'overall_f1': 0.5245569620253163, 'overall_accuracy': 0.6976748908518632}\n",
            "------------EPOCH 28---------------\n",
            "Loss:  tensor(5.6183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1597, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8976, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5552, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.1570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.7298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.6643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.9621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9165, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.2762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3837312113174182, 'recall': 0.5802139037433155, 'f1': 0.46194784459819055, 'number': 748}, 'P': {'precision': 0.5610217596972564, 'recall': 0.5723938223938224, 'f1': 0.5666507405637841, 'number': 1036}, 'overall_precision': 0.4693784277879342, 'overall_recall': 0.5756726457399103, 'overall_f1': 0.5171198388721048, 'overall_accuracy': 0.6952177886079806}\n",
            "------------EPOCH 29---------------\n",
            "Loss:  tensor(5.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2848, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.2793, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8045, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3877005347593583, 'recall': 0.5815508021390374, 'f1': 0.4652406417112299, 'number': 748}, 'P': {'precision': 0.556710775047259, 'recall': 0.5685328185328186, 'f1': 0.562559694364852, 'number': 1036}, 'overall_precision': 0.46972477064220186, 'overall_recall': 0.5739910313901345, 'overall_f1': 0.5166498486377397, 'overall_accuracy': 0.6942227637323586}\n",
            "------------EPOCH 30---------------\n",
            "Loss:  tensor(4.6532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1238, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.9970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7735, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38640429338103754, 'recall': 0.5775401069518716, 'f1': 0.4630225080385852, 'number': 748}, 'P': {'precision': 0.5559736594543744, 'recall': 0.5704633204633205, 'f1': 0.5631252977608384, 'number': 1036}, 'overall_precision': 0.469050894085282, 'overall_recall': 0.5734304932735426, 'overall_f1': 0.5160151324085751, 'overall_accuracy': 0.6938369377601787}\n",
            "------------EPOCH 31---------------\n",
            "Loss:  tensor(4.3090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3850, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.2601, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3088, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.8009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0145, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.384, 'recall': 0.5775401069518716, 'f1': 0.4612920448478377, 'number': 748}, 'P': {'precision': 0.5551366635249765, 'recall': 0.5685328185328186, 'f1': 0.5617548879351454, 'number': 1036}, 'overall_precision': 0.4670631290027447, 'overall_recall': 0.5723094170403588, 'overall_f1': 0.5143576826196474, 'overall_accuracy': 0.693430805157884}\n",
            "------------EPOCH 32---------------\n",
            "Loss:  tensor(4.0207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.6410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.4963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2836, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3573, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3823008849557522, 'recall': 0.5775401069518716, 'f1': 0.46006389776357826, 'number': 748}, 'P': {'precision': 0.5547169811320755, 'recall': 0.5675675675675675, 'f1': 0.5610687022900763, 'number': 1036}, 'overall_precision': 0.4657534246575342, 'overall_recall': 0.5717488789237668, 'overall_f1': 0.5133366884750882, 'overall_accuracy': 0.6936744847192609}\n",
            "------------EPOCH 33---------------\n",
            "Loss:  tensor(3.7887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.5504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.4345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.5149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3814977973568282, 'recall': 0.5788770053475936, 'f1': 0.4599044078597982, 'number': 748}, 'P': {'precision': 0.5558712121212122, 'recall': 0.5666023166023166, 'f1': 0.5611854684512428, 'number': 1036}, 'overall_precision': 0.4655408489274304, 'overall_recall': 0.5717488789237668, 'overall_f1': 0.5132075471698113, 'overall_accuracy': 0.6933089653771957}\n",
            "------------EPOCH 34---------------\n",
            "Loss:  tensor(3.5447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.3583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.7011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.5978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8990, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2067, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.1596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.4007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1391, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6667, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38204225352112675, 'recall': 0.5802139037433155, 'f1': 0.4607218683651805, 'number': 748}, 'P': {'precision': 0.5558712121212122, 'recall': 0.5666023166023166, 'f1': 0.5611854684512428, 'number': 1036}, 'overall_precision': 0.4657846715328467, 'overall_recall': 0.5723094170403588, 'overall_f1': 0.5135814889336016, 'overall_accuracy': 0.6940603106914408}\n",
            "------------EPOCH 35---------------\n",
            "Loss:  tensor(3.3399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.6210, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.2006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.2524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.4123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.9344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.2999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3665, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9756, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3829225352112676, 'recall': 0.5815508021390374, 'f1': 0.46178343949044587, 'number': 748}, 'P': {'precision': 0.5563981042654028, 'recall': 0.5666023166023166, 'f1': 0.5614538498326159, 'number': 1036}, 'overall_precision': 0.46645367412140576, 'overall_recall': 0.5728699551569507, 'overall_f1': 0.5142138364779875, 'overall_accuracy': 0.6929434460351305}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mi7LRmfIYbR"
      },
      "source": [
        "### Rough -- Checking dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9kGw_svbXM",
        "outputId": "b2838b14-a51e-4129-bb8c-20b274bb7a1c"
      },
      "source": [
        "\" \".join(\" mY name is \".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mY name is'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-oVkplUCJh"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qMGQ5GOzbn"
      },
      "source": [
        "train_dataset, _, test_dataset = get_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdpyzvKIfM8",
        "outputId": "8531f009-4bed-4ea9-daa4-d76d023d52e9"
      },
      "source": [
        "for tokenized_threads, masked_threads, comp_type_labels, _ in test_dataset:\n",
        "    tokenized_threads, masked_threads, comp_type_labels = tokenized_threads[0], masked_threads[0], comp_type_labels[0]\n",
        "    for tokenized_thread, masked_thread, comp_type_label in zip(tokenized_threads, masked_threads, comp_type_labels):\n",
        "        print(comp_type_label[:100])\n",
        "        print(tokenized_thread[:100])\n",
        "        print(tokenizer.decode(tokenized_thread[:500]))\n",
        "        start, end = 0, 0\n",
        "        prev_type = \"other\"\n",
        "        i = 0\n",
        "        while i<tokenized_thread.shape[0]:\n",
        "            if comp_type_label[i]==ac_dict[\"O\"]:\n",
        "                if prev_type==\"other\":\n",
        "                    end += 1\n",
        "                else:\n",
        "                    print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                    print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                    start = i\n",
        "                    end = i\n",
        "                    prev_type=\"other\"\n",
        "                \n",
        "            if comp_type_label[i] in [ac_dict[\"B-C\"], ac_dict[\"B-P\"]]:\n",
        "                print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                start = i\n",
        "                end = i\n",
        "                prev_type = \"Claim\" if comp_type_label[i]==ac_dict[\"B-C\"] else \"Premise\"\n",
        "            \n",
        "            if comp_type_label[i] in [ac_dict[\"I-C\"], ac_dict[\"I-P\"]]:\n",
        "                end += 1\n",
        "            \n",
        "            i+=1\n",
        "        break\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
            " 2 2 2 2 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4]\n",
            "[    0 18814   846    35  7978     9  1901    16   145   551   350   444\n",
            " 50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268\n",
            "   100  1819   923    84   481  1901    53     7   162  1437  8585    16\n",
            "    10   699   516   227 20203   110    78  8322   235    36  1437    22\n",
            "   270  1284 29384   328]\n",
            "<s>CMV: Freedom of speech is being taken too far [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE] I certainly value our free speech but to me there is a clear line between exercising your first amendment right (  \" President Obama sucks! \" etc ) and doing things that are known to be offensive to other cultures (  Satirical cartoons of prophets, assassinating leaders, etc ). [NEWLINE] [NEWLINE]  Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE] Sure, but that doesn't mean we condone the bully's actions and don't punish the bullies for acting. [NEWLINE] We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments. [NEWLINE]  Complete freedom in the expression of any idea, offensive or not, is a major element of that world. [NEWLINE] [NEWLINE]  If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us. [NEWLINE] [NEWLINE] [USER0] [NEWLINE] I certainly agree with your points - I didn't mean to imply that I was only for * * some * * freedom of speech. [NEWLINE] I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset. [NEWLINE] [NEWLINE]  Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"  [NEWLINE] Is that so hard? [NEWLINE] [NEWLINE] </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Component:  <s>CM  of type:  other [    0 18814]\n",
            "Masked Component:  <s>CM  of type:  other [    0 18814]\n",
            "Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Masked Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Masked Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Masked Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Component:   but to me   of type:  other [  53    7  162 1437]\n",
            "Masked Component:  <mask> to me   of type:  other [50264     7   162  1437]\n",
            "Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Masked Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Masked Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Masked Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Component:   (   of type:  other [  36 1437]\n",
            "Masked Component:   (   of type:  other [  36 1437]\n",
            "Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Masked Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Masked Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Component:   Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111    53   114    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Masked Component:   Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111 50264 50264    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            "    53   114    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Masked Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            " 50264 50264    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Component:  Sure  of type:  Claim [32541]\n",
            "Masked Component:  Sure  of type:  Claim [32541]\n",
            "Component:  , but   of type:  other [   6   53 1437]\n",
            "Masked Component:  ,<mask>   of type:  other [    6 50264  1437]\n",
            "Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Masked Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Masked Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Masked Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [ 318   47  236    7 1744    5 3519    7 1994   13  143    9  201    6\n",
            "   47   33    7 1744    5 3519    7 1994   13   70    9  201]\n",
            "Masked Component:  <mask> you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [50264    47   236     7  1744     5  3519     7  1994    13   143     9\n",
            "   201     6    47    33     7  1744     5  3519     7  1994    13    70\n",
            "     9   201]\n",
            "Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Masked Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Component:   -   of type:  other [ 111 1437]\n",
            "Masked Component:   -   of type:  other [ 111 1437]\n",
            "Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Masked Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901    77  1402  1134\n",
            "   120  4904]\n",
            "Masked Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech<mask> certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901 50264  1402  1134\n",
            "   120  4904]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"   of type:  Premise [ 2612    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45   142\n",
            "    52   214  6023     9    47    53   142    52  2098   110  2728     4\n",
            "    22  1437]\n",
            "Masked Component:  <mask> can't America be the bigger person and say \" Ok, we won't publish certain types of material, not<mask> we're afraid of you<mask><mask> we respect your views. \"   of type:  Premise [50264    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45 50264\n",
            "    52   214  6023     9    47 50264 50264    52  2098   110  2728     4\n",
            "    22  1437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZnb1Nz-MX4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2601bebd-3989-40a5-af7d-91d578651823"
      },
      "source": [
        "import re\n",
        "re.sub(r\"\\s*</claim>([^\\s])\", r\"</claim> \\1\", \"<claim>my name is </claim>jeevesh.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<claim>my name is</claim> jeevesh.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UVJb5jy178"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}