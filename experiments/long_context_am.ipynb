{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "long_context_am.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNk49+Juj7VlrEFkAecjhzT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/arg_mining/blob/main/experiments/long_context_am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGdooHmfgd-"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GPY8HXWtkyE"
      },
      "source": [
        "%%capture\n",
        "#if running on colab, install below 4\n",
        "#!git clone https://github.com/Jeevesh8/arg_mining\n",
        "#!pip install transformers\n",
        "#!pip install seqeval datasets allennlp\n",
        "#!pip install flax\n",
        "\n",
        "#if connected to local runtime, run the next command too\n",
        "#pip install bs4 tensorflow torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8QzJCbx9lO"
      },
      "source": [
        "\n",
        "\n",
        "*   Update ``arg_mining/datasets/cmv_modes/configs.py`` as per your requirements, all experiments considered till now, set ``batch_size`` to 2, and all other variables with their default value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glbajGinKG4"
      },
      "source": [
        "#Run to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pCBwYZjfkHw"
      },
      "source": [
        "### Load Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKUqJo4e_Rc"
      },
      "source": [
        "%%capture\n",
        "from datasets import load_metric\n",
        "metric = load_metric('seqeval')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kJb8EE0Fm-"
      },
      "source": [
        "### Krippendorff's Alpha Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1XOXUu0FOw"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class krip_alpha():\n",
        "    \"\"\"A module for computing sentence level Krippendorff's Alpha,\n",
        "    for argumentative components  annotated at the token level. Must use\n",
        "    labels [\"B-C\", \"B-P\"].\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"See self.compute_metric() for what each of these data actually mean.\n",
        "        \"\"\"\n",
        "        self.pred_has_claim = 0\n",
        "        self.ref_has_claim = 0\n",
        "        self.pred_has_premise = 0\n",
        "        self.ref_has_premise = 0\n",
        "        \n",
        "        self.claim_wise_agreement = 0\n",
        "        self.premise_wise_agreement = 0\n",
        "        \n",
        "        self.claim_wise_disagreement = 0\n",
        "        self.premise_wise_disagreement = 0\n",
        "    \n",
        "        self.total_sentences = 0\n",
        "        \n",
        "        self.has_both_ref = 0\n",
        "        self.has_both_pred = 0\n",
        "        self.has_none_ref = 0\n",
        "        self.has_none_pred = 0\n",
        "\n",
        "    def preprocess(self, threads: List[List[int]]) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            threads:    A list of all threads in a batch. A thread is a list of \n",
        "                        integers corresponding to token_ids of the tokens in the \n",
        "                        thread.\n",
        "        Returns:\n",
        "            A List with all the threads, where each thread now consists of \n",
        "            sentence lists. Where, a sentence list in a thread list is the list \n",
        "            of token_ids corresponding to a sentence in a thread. \n",
        "        \"\"\"\n",
        "        threads_lis = []\n",
        "\n",
        "        for i, thread in enumerate(threads):\n",
        "            sentence = []\n",
        "            threads_lis.append([])\n",
        "            for j, token_id in enumerate(thread):\n",
        "                if token_id==tokenizer.pad_token_id:\n",
        "                    break\n",
        "                \n",
        "                sentence.append(token_id)\n",
        "                token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "                #print(\"appended token:\", token)\n",
        "\n",
        "                next_token = 'None' if j==len(thread) else tokenizer.convert_ids_to_tokens(thread[j+1])\n",
        "\n",
        "                if (token.count('.')+token.count('?')+token.count('!')>=1 and \n",
        "                    next_token.count('.')+next_token.count('?')+next_token.count('!')==0):\n",
        "\n",
        "                    threads_lis[i].append(sentence)\n",
        "                    #print(\"Sample sentence: \", tokenizer.decode(sentence))\n",
        "                    sentence = []\n",
        "                \n",
        "                elif re.findall(r\"\\[USER\\d+\\]|\\[UNU\\]\", token)!=[]:\n",
        "                    prev_part = tokenizer.decode(sentence[:-1])[1:-1]\n",
        "                    if re.search(r'[a-zA-Z]', prev_part) is not None:\n",
        "                        threads_lis[i].append(sentence[:-1])\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(sentence[:-1]))\n",
        "                        sentence = [sentence[-1]]\n",
        "                    else:\n",
        "                        k=len(sentence)-2\n",
        "                        while k>=0 and sentence[k]==tokenizer.convert_tokens_to_ids('Ġ'):\n",
        "                            k-=1\n",
        "                        sentence = sentence[k+1:]\n",
        "                        threads_lis[i][-1] += sentence[:k]\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(threads_lis[i][-1]))\n",
        "                \n",
        "            has_rem_token = False\n",
        "            for elem in sentence:\n",
        "                if (elem!=tokenizer.convert_tokens_to_ids('Ġ') and\n",
        "                    elem!=tokenizer.eos_token_id):\n",
        "                    has_rem_token = True\n",
        "                    break\n",
        "            \n",
        "            if has_rem_token:\n",
        "                threads_lis[i].append(sentence)\n",
        "                #print(\"Sample sentence at end of thread: \", tokenizer.decode(sentence))\n",
        "                sentence = []\n",
        "\n",
        "        return threads_lis\n",
        "\n",
        "    def get_sentence_wise_preds(self, threads: List[List[List[int]]], \n",
        "                                      predictions: List[List[str]]) -> List[List[List[str]]]:\n",
        "        \"\"\"Splits the prediction corresponding to each thread, into predictions\n",
        "        for each sentence in the corresponding thread in \"threads\" list.\n",
        "        Args:\n",
        "            threads:      A list of threads, where each thread consists of further \n",
        "                          lists corresponding to the various sentences in the\n",
        "                          thread. [As output by self.preprocess()]\n",
        "            predictions:  A list of predictions for each thread, in the threads\n",
        "                          list. Each prediciton consists of a list of componenet \n",
        "                          types corresponding to each token in a thread.\n",
        "        Returns:\n",
        "            The predictions list, with each prediction split into predictions \n",
        "            corresponding to the sentences in the corresponding thread specified\n",
        "            in the threads list. \n",
        "        \"\"\"\n",
        "        sentence_wise_preds = []\n",
        "        for i, thread in enumerate(threads):\n",
        "            next_sentence_beg = 0\n",
        "            sentence_wise_preds.append([])\n",
        "            for sentence in thread:\n",
        "                sentence_wise_preds[i].append(\n",
        "                    predictions[i][next_sentence_beg:next_sentence_beg+len(sentence)])\n",
        "                next_sentence_beg += len(sentence)\n",
        "        return sentence_wise_preds\n",
        "    \n",
        "    def update_state(self, pred_sentence: List[str], ref_sentence: List[str]) -> None:\n",
        "        \"\"\"Updates the various information maintained for the computation of\n",
        "        Krippendorff's alpha, based on the predictions(pred_sentence) and \n",
        "        references(ref_sentence) provided for a particular sentence, in some \n",
        "        thread.\n",
        "        \"\"\"\n",
        "        self.total_sentences += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence:\n",
        "            self.pred_has_claim += 1\n",
        "            if 'B-C' in ref_sentence:\n",
        "                self.ref_has_claim += 1\n",
        "                self.claim_wise_agreement += 1\n",
        "            else:\n",
        "                self.claim_wise_disagreement += 1\n",
        "            \n",
        "        elif 'B-C' in ref_sentence:\n",
        "            self.ref_has_claim += 1\n",
        "            self.claim_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.claim_wise_agreement += 1\n",
        "        \n",
        "        if 'B-P' in pred_sentence:\n",
        "            self.pred_has_premise += 1\n",
        "            if 'B-P' in ref_sentence:\n",
        "                self.ref_has_premise += 1\n",
        "                self.premise_wise_agreement += 1\n",
        "            else:\n",
        "                self.premise_wise_disagreement += 1\n",
        "\n",
        "        elif 'B-P' in ref_sentence:\n",
        "            self.ref_has_premise += 1\n",
        "            self.premise_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.premise_wise_agreement += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence and 'B-P' in pred_sentence:\n",
        "            self.has_both_pred += 1\n",
        "        \n",
        "        if 'B-C' in ref_sentence and 'B-P' in ref_sentence:\n",
        "            self.has_both_ref += 1\n",
        "        \n",
        "        if 'B-C' not in pred_sentence and 'B-P' not in pred_sentence:\n",
        "            self.has_none_pred += 1\n",
        "        \n",
        "        if 'B-C' not in ref_sentence and 'B-P' not in ref_sentence:\n",
        "            self.has_none_ref += 1\n",
        "        return\n",
        "\n",
        "    def add_batch(self, predictions: List[List[str]], \n",
        "                  references: List[List[str]], \n",
        "                  tokenized_threads: List[List[int]]) -> None:\n",
        "        \"\"\"Add a batch of predictions and references for the computation of \n",
        "        Krippendorff's alpha.\n",
        "        Args:\n",
        "            predictions:      A list of predictions for each thread, in the \n",
        "                              threads list. Each prediciton consists of a list \n",
        "                              of component types corresponding to each token in \n",
        "                              a thread.\n",
        "            references:       Same structure as predictions, but consisting of \n",
        "                              acutal gold labels, instead of predicted ones.\n",
        "            tokenized_thread: A list of all threads in a batch. A thread is a \n",
        "                              list of integers corresponding to token_ids of the\n",
        "                              tokens in the thread.\n",
        "        \"\"\"\n",
        "        threads = self.preprocess(tokenized_threads)\n",
        "        \n",
        "        sentence_wise_preds = self.get_sentence_wise_preds(threads, predictions)\n",
        "        sentence_wise_refs = self.get_sentence_wise_preds(threads, references)\n",
        "\n",
        "        for pred_thread, ref_thread in zip(sentence_wise_preds, sentence_wise_refs):\n",
        "            for pred_sentence, ref_sentence in zip(pred_thread, ref_thread):\n",
        "                self.update_state(pred_sentence, ref_sentence)\n",
        "\n",
        "    def compute(self, print_additional: bool=True) -> None:\n",
        "        \"\"\"Prints out the metric, for the batched added till now. And then \n",
        "        resets all data being maintained by the metric. \n",
        "        Args:\n",
        "            print_additional:   If True, will print all the data being \n",
        "                                maintained instead of just the Krippendorff's \n",
        "                                alphas for claims and premises.\n",
        "        \"\"\"\n",
        "        print(\"Sentence level Krippendorff's alpha for Claims: \", 1-(self.claim_wise_disagreement/(self.claim_wise_agreement+self.claim_wise_disagreement))/0.5)\n",
        "        print(\"Sentence level Krippendorff's alpha for Premises: \", 1-(self.premise_wise_disagreement/(self.premise_wise_agreement+self.premise_wise_disagreement))/0.5)\n",
        "        \n",
        "        if print_additional:\n",
        "            print(\"Additional attributes: \")\n",
        "            print(\"\\tTotal Sentences:\", self.total_sentences)\n",
        "            print(\"\\tPrediction setences having claims:\", self.pred_has_claim)\n",
        "            print(\"\\tPrediction sentences having premises:\", self.pred_has_premise)\n",
        "            print(\"\\tReference setences having claims:\", self.ref_has_claim)\n",
        "            print(\"\\tReference sentences having premises:\", self.ref_has_premise)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tPrediction Sentence having both claim and premise:\", self.has_both_pred)\n",
        "            print(\"\\tPrediction Sentence having neither claim nor premise:\", self.has_none_pred)\n",
        "            print(\"\\tReference Sentence having both claim and premise:\", self.has_both_ref)\n",
        "            print(\"\\tReference Sentence having neither claim nor premise:\", self.has_none_ref)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tSentences having claim in both reference and prediction:\", self.claim_wise_agreement)\n",
        "            print(\"\\tSentences having claim in only one of reference or prediction:\", self.claim_wise_disagreement)\n",
        "            print(\"\\tSentences having premise in both reference and prediction:\", self.premise_wise_agreement)\n",
        "            print(\"\\tSentences having premise in only one of reference or prediction:\", self.premise_wise_disagreement)\n",
        "        self.__init__()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRNv0wBWtFd"
      },
      "source": [
        "metric = krip_alpha()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6BxMkfm3R"
      },
      "source": [
        "### Define & Load Tokenizer, Model, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wcsqmllnfRB"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DeXxCDS_id",
        "outputId": "3c67d0ae-33ef-4d4e-c8e6-9998f405d9bf"
      },
      "source": [
        "device"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v938tu5rydoT"
      },
      "source": [
        "#### Load Model/Tokenizer from HF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRkCQOZu6HS"
      },
      "source": [
        "model_version = 'allenai/longformer-base-4096'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cB-7M9t0HP"
      },
      "source": [
        "%%capture\n",
        "from transformers import LongformerTokenizer, AutoModel\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_version)\n",
        "transformer_model = AutoModel.from_pretrained(model_version).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4IWXMOymZk"
      },
      "source": [
        "#### Or load them from pretrained files..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNf-8LQVyliT",
        "outputId": "013f8f0e-fb6d-41d8-d8a5-568190f38fcf"
      },
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "\n",
        "tokenizer = LongformerTokenizer.from_pretrained('./4epoch_complete/tokenizer/')\n",
        "transformer_model = LongformerModel.from_pretrained('./4epoch_complete/model/').to(device)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./4epoch_complete/model/ were not used when initializing LongformerModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at ./4epoch_complete/model/ and are newly initialized: ['longformer.pooler.dense.bias', 'longformer.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEV4yTy11zUm"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4PwtV9zai9"
      },
      "source": [
        "#### To add extra token type embeddings..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajTzrbzkwbT"
      },
      "source": [
        "def resize_token_type_embeddings(transformer_model, new_size):\n",
        "    old_embeddings = transformer_model.embeddings.token_type_embeddings.weight\n",
        "    old_size, hidden_dim = old_embeddings.shape\n",
        "    transformer_model.embeddings.token_type_embeddings = nn.Embedding(new_size, hidden_dim, device=transformer_model.device)\n",
        "    with torch.no_grad():\n",
        "        transformer_model.embeddings.token_type_embeddings.weight[:old_size] = old_embeddings\n",
        "\n",
        "resize_token_type_embeddings(transformer_model, 2)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHJiT0sjXid"
      },
      "source": [
        "transformer_model.config.type_vocab_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZ89akNzoMo"
      },
      "source": [
        "#### Load in discourse markers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p6dGA83cGVS"
      },
      "source": [
        "with open('./Discourse_Markers.txt') as f:\n",
        "    discourse_markers = [dm.strip() for dm in f.readlines()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bQWcCd1p1G"
      },
      "source": [
        "%%capture\n",
        "from arg_mining.datasets.cmv_modes import load_dataset, data_config"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5O1Wm7Dzqqe"
      },
      "source": [
        "#### Add special tokens to tokenizer and model vocab, if not already there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkOZsiCzl1k"
      },
      "source": [
        "tokenizer.add_tokens(data_config[\"special_tokens\"])\n",
        "\n",
        "transformer_model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh32PtIEz1mF"
      },
      "source": [
        "#### Function to get train, test data (80/20 split currently)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCeCgbLxVyQ"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_8gVbufzoX"
      },
      "source": [
        "### Define layers for a Linear-Chain-CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seBwkZdByesM"
      },
      "source": [
        "from allennlp.modules.conditional_random_field import ConditionalRandomField as crf\n",
        "\n",
        "ac_dict = data_config[\"arg_components\"]\n",
        "\n",
        "allowed_transitions =([(ac_dict[\"B-C\"], ac_dict[\"I-C\"]), \n",
        "                       (ac_dict[\"B-P\"], ac_dict[\"I-P\"])] + \n",
        "                      [(ac_dict[\"I-C\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-C\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"I-P\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-P\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"O\"], ac_dict[ct]) \n",
        "                        for ct in [\"O\", \"B-C\", \"B-P\"]])\n",
        "                    \n",
        "linear_layer = nn.Linear(transformer_model.config.hidden_size,\n",
        "                         len(ac_dict)).to(device)\n",
        "\n",
        "crf_layer = crf(num_tags=len(ac_dict),\n",
        "                constraints=allowed_transitions,\n",
        "                include_start_end_transitions=False).to(device)\n",
        "\n",
        "cross_entropy_layer = nn.CrossEntropyLoss(weight=torch.log(torch.tensor([3.3102, 61.4809, 3.6832, 49.6827, 2.5639], \n",
        "                                                                        device=device)), reduction='none')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUicsK33f9d7"
      },
      "source": [
        "### Global Attention Mask Utility for Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSo7p2I5mOi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n",
        "    0 elsewhere.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(tokenized_threads)\n",
        "    for user_token in [\"UNU\"]+[f\"[USER{i}]\" for i in range(data_config[\"max_users\"])]:\n",
        "        user_token_id = tokenizer.encode(user_token)[1:-1]\n",
        "        mask = np.where(tokenized_threads==user_token_id, 1, mask)\n",
        "    return np.array(mask, dtype=bool)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4PLQihf5CT"
      },
      "source": [
        "### Loss and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3u8eH1rjZe"
      },
      "source": [
        "from typing import Tuple"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuJ5aryW9tUC"
      },
      "source": [
        "def compute(batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            preds: bool=False, cross_entropy: bool=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n",
        "                component type labels of shape [batch_size, seq_len], and a global\n",
        "                attention mask for Longformer, of the same shape.\n",
        "        \n",
        "        preds:  If True, returns a List(of batch_size size) of Tuples of form \n",
        "                (tag_sequence, viterbi_score) where the tag_sequence is the \n",
        "                viterbi-decoded sequence, for the corresponding sample in the batch.\n",
        "        \n",
        "        cross_entropy:  This argument will only be used if preds=False, i.e., if \n",
        "                        loss is being calculated. If True, then cross entropy loss\n",
        "                        will also be added to the output loss.\n",
        "    \n",
        "    Returns:\n",
        "        Either the predicted sequences with their scores for each element in the batch\n",
        "        (if preds is True), or the loss value summed over all elements of the batch\n",
        "        (if preds is False).\n",
        "    \"\"\"\n",
        "    tokenized_threads, token_type_ids, comp_type_labels, global_attention_mask = batch\n",
        "    \n",
        "    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n",
        "    \n",
        "    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n",
        "                                            attention_mask=pad_mask,\n",
        "                                            global_attention_mask=global_attention_mask).last_hidden_state)\n",
        "    \n",
        "    if preds:\n",
        "        return crf_layer.viterbi_tags(logits, pad_mask)\n",
        "    \n",
        "    log_likelihood = crf_layer(logits, comp_type_labels, pad_mask)\n",
        "    \n",
        "    if cross_entropy:\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        \n",
        "        pad_mask, comp_type_labels = pad_mask.reshape(-1), comp_type_labels.reshape(-1)\n",
        "        \n",
        "        ce_loss = torch.sum(pad_mask*cross_entropy_layer(logits, comp_type_labels))\n",
        "        \n",
        "        return ce_loss - log_likelihood\n",
        "\n",
        "    return -log_likelihood"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lkPCsgEY4"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yfpzEMBGra"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n",
        "                                      linear_layer.parameters(),\n",
        "                                      crf_layer.parameters()),\n",
        "                       lr = 2e-5,)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSWnkO8gLD6"
      },
      "source": [
        "### Training And Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTNaP3kLaN2X"
      },
      "source": [
        "def train(dataset):\n",
        "    accumulate_over = 4\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (tokenized_threads, masked_threads, comp_type_labels, _ ) in enumerate(dataset):\n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads),\n",
        "                                             device=device, dtype=torch.int32)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0), \n",
        "                                         device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0), \n",
        "                                      device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0), \n",
        "                                        device=device, dtype=torch.long)\n",
        "        \n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        loss = compute((tokenized_threads,\n",
        "                        torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                        comp_type_labels, \n",
        "                        global_attention_mask))/data_config[\"batch_size\"]\n",
        "        \n",
        "        print(\"Loss: \", loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        if i%accumulate_over==accumulate_over-1:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BpSI83cU24"
      },
      "source": [
        "def evaluate(dataset, metric):\n",
        "    \n",
        "    int_to_labels = {v:k for k, v in ac_dict.items()}\n",
        "    \n",
        "    for tokenized_threads, masked_threads, comp_type_labels, _ in dataset:\n",
        "        \n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads), \n",
        "                                             device=device)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0),\n",
        "                                        device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0),\n",
        "                                     device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0),\n",
        "                                        device=device)\n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        preds = compute((tokenized_threads,\n",
        "                         torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                         comp_type_labels,\n",
        "                         global_attention_mask),\n",
        "                        preds=True)\n",
        "        \n",
        "        lengths = torch.sum(torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0), \n",
        "                            axis=-1)\n",
        "        \n",
        "        preds = [ [int_to_labels[pred] for pred in pred[0][:lengths[i]]]\n",
        "                  for i, pred in enumerate(preds)\n",
        "                ]\n",
        "        \n",
        "        refs = [ [int_to_labels[ref] for ref in labels[:lengths[i]]]\n",
        "                 for i, labels in enumerate(comp_type_labels.cpu().tolist())\n",
        "               ]\n",
        "        \n",
        "        metric.add_batch(predictions=preds, \n",
        "                         references=refs,)\n",
        "                         #tokenized_threads=tokenized_threads.cpu().tolist())\n",
        "    \n",
        "    print(metric.compute())"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZV6rnIQgOYA"
      },
      "source": [
        "### Final Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2O5qCufGwA"
      },
      "source": [
        "n_epochs = 35"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xcK9sOgX44",
        "outputId": "a3cddc3a-8de4-4149-d24b-792028c07e77"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    print(f\"------------EPOCH {epoch+1}---------------\")\n",
        "    train_dataset, _, test_dataset = get_datasets()\n",
        "    train(train_dataset)\n",
        "    evaluate(test_dataset, metric)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------EPOCH 1---------------\n",
            "Loss:  tensor(2706.0752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1972.6887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1780.1833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2266.5498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3299.8987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2557.9482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3099.8110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3049.1584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2745.9858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2699.9558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2384.4854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2665.6172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3515.8525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2189.4951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2725.4453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(889.7032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1072.4631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1830.1111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1062.8634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1765.9854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2108.3914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1741.7368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2686.0229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6105.8232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3561.9700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1277.7716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1221.9949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1852.5186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2819.1240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3167.6624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1283.7229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4216.1426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3671.0505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1280.3983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1801.3459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2630.3042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1809.4618, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3701.6077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1001.8506, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1836.4031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1848.5352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1586.8237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2668.7498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1752.7620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2671.8188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1756.4006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.0, 'recall': 0.0, 'f1': 0.0, 'number': 289}, 'P': {'precision': 0.18825561312607944, 'recall': 0.331306990881459, 'f1': 0.24008810572687228, 'number': 329}, 'overall_precision': 0.18825561312607944, 'overall_recall': 0.17637540453074432, 'overall_f1': 0.18212197159565582, 'overall_accuracy': 0.5040340654415061}\n",
            "------------EPOCH 2---------------\n",
            "Loss:  tensor(1417.9803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1897.2144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1669.9792, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2128.6934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2130.1399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1757.9226, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2307.4336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2547.5415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2136.2354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1995.7446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1671.2229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1970.5660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2705.3428, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1581.2091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1918.5305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(656.4415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(816.3527, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1467.8021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(836.5888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1328.4297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1776.1991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1306.9880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2083.5376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4672.4443, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2494.9702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1129.1211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1094.3214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1550.7266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2366.2339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2582.6709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1102.6055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3540.8621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3260.7246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1033.7522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1596.6985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2397.8804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1576.6311, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3037.5312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(870.9160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1569.6461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1486.4644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1329.3943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2105.6025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1357.9551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2356.7856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1484.4287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.1917808219178082, 'recall': 0.04844290657439446, 'f1': 0.07734806629834254, 'number': 289}, 'P': {'precision': 0.18934081346423562, 'recall': 0.41033434650455924, 'f1': 0.2591170825335892, 'number': 329}, 'overall_precision': 0.1895674300254453, 'overall_recall': 0.24110032362459546, 'overall_f1': 0.21225071225071226, 'overall_accuracy': 0.5444867772299418}\n",
            "------------EPOCH 3---------------\n",
            "Loss:  tensor(1116.2391, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1634.2539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1418.7268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1807.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1606.0049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1380.4778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1884.8110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2111.3508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1865.7244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1758.3687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1412.1265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1649.7358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2514.4966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1368.9758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1721.0789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(572.4596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(754.9750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1346.3033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(837.8915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1293.7460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1530.2795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1201.3328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1833.4509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4103.0352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2091.3193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(922.3894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(868.9833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1313.1711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1956.8643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1963.7747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(906.6162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2899.0022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2874.2007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(974.4634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1424.1296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2093.5767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1376.9668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2815.6179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(841.2456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1521.8140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1265.7180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1133.9259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1945.8955, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1175.6641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2115.4453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1370.0831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2, 'recall': 0.1695501730103806, 'f1': 0.18352059925093633, 'number': 289}, 'P': {'precision': 0.30847457627118646, 'recall': 0.5531914893617021, 'f1': 0.39608269858541895, 'number': 329}, 'overall_precision': 0.27664670658682633, 'overall_recall': 0.3737864077669903, 'overall_f1': 0.31796283551273224, 'overall_accuracy': 0.5768713581353653}\n",
            "------------EPOCH 4---------------\n",
            "Loss:  tensor(1044.7988, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1266.9105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1075.4014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1444.2983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1386.3291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1118.9963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1702.3556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1809.4301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1642.2642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1516.2388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1234.5510, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1481.8716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2250.0647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1216.8062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1573.8792, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(490.9757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(690.0358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1222.3948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(718.8499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1062.7946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1350.6304, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1034.2505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1605.8809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3780.4656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1807.0117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(825.3691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(754.6545, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1211.1523, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1700.0649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1639.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(792.7314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2601.0967, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2515.1274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(790.0902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1242.4379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1744.0740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1179.9683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2464.7954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(759.3339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1452.2927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1175.6523, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(907.3204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1584.6831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1086.2925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1906.0219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1364.9844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3325581395348837, 'recall': 0.49480968858131485, 'f1': 0.39777468706536856, 'number': 289}, 'P': {'precision': 0.45918367346938777, 'recall': 0.41033434650455924, 'f1': 0.43338683788121996, 'number': 329}, 'overall_precision': 0.3839779005524862, 'overall_recall': 0.44983818770226536, 'overall_f1': 0.4143070044709388, 'overall_accuracy': 0.6249439713133124}\n",
            "------------EPOCH 5---------------\n",
            "Loss:  tensor(936.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(818.1459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(685.8641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(973.7051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1184.3372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(954.3075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1575.2314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1575.0454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1360.7224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1424.9946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1070.8770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1243.7660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1860.3054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1049.5659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1450.4745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(414.7901, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(605.3976, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1049.9026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(516.9769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(809.0728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1093.2820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(752.8884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1405.3821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3454.2251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1476.0225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(717.3179, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(623.9657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1083.6289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1548.3433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1361.7489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(653.6622, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2382.4585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2049.7739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(628.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(986.1337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1469.9702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(908.6354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1974.6136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(601.3746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1057.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1012.2104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(566.4523, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1194.2966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(861.3150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1461.5498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1201.0767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3181818181818182, 'recall': 0.4117647058823529, 'f1': 0.358974358974359, 'number': 289}, 'P': {'precision': 0.5, 'recall': 0.45592705167173253, 'f1': 0.4769475357710652, 'number': 329}, 'overall_precision': 0.3991097922848665, 'overall_recall': 0.43527508090614886, 'overall_f1': 0.4164086687306502, 'overall_accuracy': 0.6585611833258629}\n",
            "------------EPOCH 6---------------\n",
            "Loss:  tensor(792.3679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(475.7279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(434.0652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(604.6985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1049.9680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(763.1366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1212.2517, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1264.3142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1200.7603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1133.4346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(841.4607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(941.4101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1553.9095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(849.6863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1194.2441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(324.2989, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(463.2208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(748.7657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(419.1795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(590.4861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(922.2537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(639.0502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1140.4768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2723.3982, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1057.4561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(559.0853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(424.9991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(912.1395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1410.0468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1243.7640, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(484.0817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2200.4614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2111.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(515.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(883.8747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1563.2528, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(612.7677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1550.5071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(513.6021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(776.6043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(895.5380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(476.1841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(698.8907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1113.2288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(913.4727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.2535211267605634, 'recall': 0.31141868512110726, 'f1': 0.27950310559006214, 'number': 289}, 'P': {'precision': 0.46634615384615385, 'recall': 0.2948328267477204, 'f1': 0.3612662942271881, 'number': 329}, 'overall_precision': 0.3321492007104796, 'overall_recall': 0.30258899676375406, 'overall_f1': 0.31668077900084673, 'overall_accuracy': 0.617996414164052}\n",
            "------------EPOCH 7---------------\n",
            "Loss:  tensor(691.2054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(314.0257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(312.0371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(466.0714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1438.4929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(975.9694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1446.1702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1795.3812, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1066.3444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1007.9884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(798.1788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(901.6624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1195.8838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(835.2010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(958.4069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(320.9065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(367.8627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(566.2163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(332.2861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(461.7206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(641.0768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(460.5878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(932.2537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2063.5942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(902.2993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(473.4786, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(375.5524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(727.0852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1330.9556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1163.2344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(405.8702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2027.9762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2051.6714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(515.8033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(781.6968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1680.4114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(852.2134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1603.3124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(526.6954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(844.5212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(940.8406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(559.4336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(809.3370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(430.9585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1079.1973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(873.6776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3978260869565217, 'recall': 0.6332179930795848, 'f1': 0.4886515353805074, 'number': 289}, 'P': {'precision': 0.4897119341563786, 'recall': 0.3617021276595745, 'f1': 0.41608391608391604, 'number': 329}, 'overall_precision': 0.4295874822190612, 'overall_recall': 0.4886731391585761, 'overall_f1': 0.4572293716881151, 'overall_accuracy': 0.653014343343792}\n",
            "------------EPOCH 8---------------\n",
            "Loss:  tensor(808.6832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(294.6285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(217.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(382.6032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1081.9110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(635.9819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1109.4541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1012.9970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1193.5734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(940.9869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1237.3042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1429.9539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1293.9902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(856.2126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(987.6645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(291.6883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(406.5110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(525.8799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(356.9206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(477.8546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(560.2124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.2836, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(881.2334, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1797.6619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(558.0872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(315.4224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.1807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(606.4890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(892.8932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(769.3069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(310.2541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1419.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1332.4519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(396.3654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.3251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1101.4199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(502.6196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1133.0166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(399.6837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(596.8433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(732.8945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(399.7164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(507.8826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(427.1673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1018.9412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(660.6138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.40535372848948376, 'recall': 0.7335640138408305, 'f1': 0.522167487684729, 'number': 289}, 'P': {'precision': 0.5381526104417671, 'recall': 0.4072948328267477, 'f1': 0.46366782006920415, 'number': 329}, 'overall_precision': 0.4481865284974093, 'overall_recall': 0.5598705501618123, 'overall_f1': 0.497841726618705, 'overall_accuracy': 0.6508852532496638}\n",
            "------------EPOCH 9---------------\n",
            "Loss:  tensor(498.9019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(378.6758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(258.1219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(518.2975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(694.8859, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(575.0463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1199.1899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(929.4973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(820.6498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(719.9661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(519.9172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(719.2279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(751.7973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(522.0835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(648.0247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(203.5426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(321.5855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(431.7524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(266.3726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(338.8077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(705.4111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(542.2153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(866.1050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2431.0781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(565.0573, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(293.3594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(303.8956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(523.1850, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(731.4474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(576.8011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(241.8886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(825.2892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(680.6022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(259.6340, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(429.7620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(629.3244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(299.2634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(626.5374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(227.6378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(355.9345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(535.4347, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(269.6204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(271.7489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(283.7065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(801.6912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(620.5256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4444444444444444, 'recall': 0.5259515570934256, 'f1': 0.48177496038034867, 'number': 289}, 'P': {'precision': 0.4802784222737819, 'recall': 0.6291793313069909, 'f1': 0.5447368421052632, 'number': 329}, 'overall_precision': 0.4644243208279431, 'overall_recall': 0.580906148867314, 'overall_f1': 0.5161754133716752, 'overall_accuracy': 0.6550313760645451}\n",
            "------------EPOCH 10---------------\n",
            "Loss:  tensor(379.0873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.2180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(205.0775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(542.5560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(578.6212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(571.8892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1084.2441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1060.2244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(665.6788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(619.1393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(393.9038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(658.7300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(956.4757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(566.8273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(659.1136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(194.9549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(235.6903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(391.1839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(300.8947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.0186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(307.8722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(248.4498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(452.3012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1079.9337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(285.1139, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.0708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(132.5536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.8344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.4478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(548.4094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(232.8281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(584.9337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(837.1860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(316.9754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(397.7676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(711.6467, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(476.9536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(964.8798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(293.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(441.1083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(491.3585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(152.4907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(255.7825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(232.9952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(465.0996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(293.4127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.46439628482972134, 'recall': 0.5190311418685121, 'f1': 0.4901960784313725, 'number': 289}, 'P': {'precision': 0.544529262086514, 'recall': 0.6504559270516718, 'f1': 0.592797783933518, 'number': 329}, 'overall_precision': 0.5083798882681564, 'overall_recall': 0.5889967637540453, 'overall_f1': 0.545727136431784, 'overall_accuracy': 0.703664276109368}\n",
            "------------EPOCH 11---------------\n",
            "Loss:  tensor(133.8656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.2902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.7063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(189.6302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(344.3460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.4975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(451.1750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(471.6203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(311.8091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(283.2047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(247.9117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(365.7332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(355.9156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.9785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(400.2927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.9418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(166.0055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(242.6480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(217.9733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.1672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(324.1992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(231.4243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(455.7310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(817.9174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(262.4768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(160.2341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.7262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(340.3219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(613.7551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(558.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(155.9425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(616.7878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(417.1728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.0885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(223.8532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(400.6355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(182.4180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(322.5682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.5945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(187.8699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(314.2504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.0726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.2555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(145.9816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(589.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(471.2245, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4583333333333333, 'recall': 0.22837370242214533, 'f1': 0.30484988452655887, 'number': 289}, 'P': {'precision': 0.4732965009208103, 'recall': 0.7811550151975684, 'f1': 0.5894495412844036, 'number': 329}, 'overall_precision': 0.47016011644832606, 'overall_recall': 0.5226537216828478, 'overall_f1': 0.49501915708812255, 'overall_accuracy': 0.6664612281488121}\n",
            "------------EPOCH 12---------------\n",
            "Loss:  tensor(182.8026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(266.6132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.1365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(363.6057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(534.4814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(199.2225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(581.7479, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1151.0612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(449.6266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(469.7432, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(261.5463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(343.4930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(357.0985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(173.4647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(290.5817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.7589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(144.5526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(194.1797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(138.4556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(334.5482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.7658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(336.2796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(737.2876, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(403.7308, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(152.2138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(223.6431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(454.3349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(838.2283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1114.7644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(324.8070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1296.6345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.0209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(265.6857, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(386.2453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.5612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(196.2412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(451.4186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.8043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(222.9574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.3353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(75.6739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.9366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.3145, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(341.1852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(209.5278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4676258992805755, 'recall': 0.22491349480968859, 'f1': 0.3037383177570094, 'number': 289}, 'P': {'precision': 0.448566610455312, 'recall': 0.8085106382978723, 'f1': 0.577006507592191, 'number': 329}, 'overall_precision': 0.4521857923497268, 'overall_recall': 0.5355987055016181, 'overall_f1': 0.49037037037037035, 'overall_accuracy': 0.6456745853877185}\n",
            "------------EPOCH 13---------------\n",
            "Loss:  tensor(165.7654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(157.9563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.9463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(193.6862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(670.1191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(408.6291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(632.6085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(966.2183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1093.1829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(639.7872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(488.9271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(632.6749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1361.9840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(872.9709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1045.8715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(210.5916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(541.2794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(615.7280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(287.2661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(493.0678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(303.4655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(168.0121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.4690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(969.5375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(172.2482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.5902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.7707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.1261, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(307.2083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(375.3137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.9230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(328.7871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(587.6450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(222.2021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(325.4474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(423.6841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(563.9505, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1319.0709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.3632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(628.8341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(585.5002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(388.5028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1016.9855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(430.7925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(573.0327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(607.3380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.43897216274089934, 'recall': 0.7093425605536332, 'f1': 0.5423280423280422, 'number': 289}, 'P': {'precision': 0.6218487394957983, 'recall': 0.44984802431610943, 'f1': 0.5220458553791888, 'number': 329}, 'overall_precision': 0.500709219858156, 'overall_recall': 0.5711974110032363, 'overall_f1': 0.5336356764928193, 'overall_accuracy': 0.6881443298969072}\n",
            "------------EPOCH 14---------------\n",
            "Loss:  tensor(340.8658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.3394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(143.8250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(123.2564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(317.1130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(201.9613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(357.1890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.6093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(213.2634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(246.6126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(163.2801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.8652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(232.8024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.7171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(253.4180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.7666, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.6065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(170.7625, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.5851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(170.2927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.8710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.1962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(292.6287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(693.7015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(239.8942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(169.3747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.9783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(498.3250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(434.0117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(444.7830, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.4569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(695.5860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(545.1965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(157.8606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(243.2244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(393.6929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.4004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(319.4981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(110.5459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.5314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.9847, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.3407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.7866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.8753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(239.9732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.7349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4607594936708861, 'recall': 0.629757785467128, 'f1': 0.5321637426900584, 'number': 289}, 'P': {'precision': 0.5792682926829268, 'recall': 0.5775075987841946, 'f1': 0.578386605783866, 'number': 329}, 'overall_precision': 0.5145228215767634, 'overall_recall': 0.6019417475728155, 'overall_f1': 0.5548098434004474, 'overall_accuracy': 0.691674137158225}\n",
            "------------EPOCH 15---------------\n",
            "Loss:  tensor(97.0969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.7441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.3603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.9850, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(247.2570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.9633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(287.8438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(208.6891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(169.5522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(207.7006, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.8936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(224.6926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.7547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(159.4266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(235.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.5733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.7793, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.1920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.6137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.2840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(173.6227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.5119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(154.7560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(346.1232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.7649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.7466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.8281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.9096, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.4893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.8745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(162.4823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(192.8616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.6904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.1190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(186.7903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.7478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(182.8966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.4300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.6799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.7190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.2993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.1799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.2188, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.1855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.46335078534031415, 'recall': 0.6124567474048442, 'f1': 0.5275707898658718, 'number': 289}, 'P': {'precision': 0.577639751552795, 'recall': 0.5653495440729484, 'f1': 0.5714285714285714, 'number': 329}, 'overall_precision': 0.515625, 'overall_recall': 0.587378640776699, 'overall_f1': 0.5491679273827533, 'overall_accuracy': 0.70663379650381}\n",
            "------------EPOCH 16---------------\n",
            "Loss:  tensor(58.4027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.6553, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.1736, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.7046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.9191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(139.2255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(128.0931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.7669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.1121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.7606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.3157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(105.6155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.4397, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.5536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.2689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.6211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.0312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.5856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.6931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.2729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(100.5733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(210.8842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.1164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.6648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.0555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.2869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.0892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.9661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.1924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.0264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.6201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.1337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.4217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(112.8540, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(135.7745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.3507, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.9641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0935, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.4851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(119.3644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.9198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4690265486725664, 'recall': 0.5501730103806228, 'f1': 0.5063694267515924, 'number': 289}, 'P': {'precision': 0.5753424657534246, 'recall': 0.6382978723404256, 'f1': 0.6051873198847263, 'number': 329}, 'overall_precision': 0.5241477272727273, 'overall_recall': 0.5970873786407767, 'overall_f1': 0.5582450832072617, 'overall_accuracy': 0.7082586284177499}\n",
            "------------EPOCH 17---------------\n",
            "Loss:  tensor(31.9700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.3730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.2640, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.4855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.0751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.7944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.3367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.0102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.5203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.8970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.3004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.6117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.0182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.2896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.1511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.1740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.3033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.8108, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.6946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.8016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.4252, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.8449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.4833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.0776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(167.4965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.1176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.4822, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.1232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.8627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.6352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.4059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.6536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.1531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.7488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.4286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.6053, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.8596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.6392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4598337950138504, 'recall': 0.5743944636678201, 'f1': 0.5107692307692309, 'number': 289}, 'P': {'precision': 0.5786350148367952, 'recall': 0.5927051671732523, 'f1': 0.5855855855855856, 'number': 329}, 'overall_precision': 0.5171919770773639, 'overall_recall': 0.5841423948220065, 'overall_f1': 0.5486322188449847, 'overall_accuracy': 0.7047848498431197}\n",
            "------------EPOCH 18---------------\n",
            "Loss:  tensor(25.3469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.7058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.2345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.8674, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.3637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.2114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.5893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.9699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.2594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.7768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.2695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.5953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2791, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.7808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2988, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2092, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.2993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6308, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.0587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(106.2582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.8010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7959, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.3798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.3193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.8290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.4013, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.3564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.9781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.4627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(75.2392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.7700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.6368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.4529, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.5320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.9602, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.5723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.1996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.8451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4558011049723757, 'recall': 0.5709342560553633, 'f1': 0.5069124423963134, 'number': 289}, 'P': {'precision': 0.5739130434782609, 'recall': 0.601823708206687, 'f1': 0.5875370919881306, 'number': 329}, 'overall_precision': 0.5134370579915134, 'overall_recall': 0.587378640776699, 'overall_f1': 0.5479245283018866, 'overall_accuracy': 0.7008628417749888}\n",
            "------------EPOCH 19---------------\n",
            "Loss:  tensor(21.7981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.5866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8735, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.8790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.6405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.5627, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.1893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.9408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.9976, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.3109, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.5656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.6279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.8747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.4496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.1897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.5938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.7132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.9746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.1090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.0004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.3400, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(131.0063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.3063, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.4010, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.1881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.6266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.4759, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.8825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.0066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.9200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.6430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.7243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.7027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.8053, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.3913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4642857142857143, 'recall': 0.5847750865051903, 'f1': 0.5176110260336906, 'number': 289}, 'P': {'precision': 0.5623188405797102, 'recall': 0.5896656534954408, 'f1': 0.5756676557863502, 'number': 329}, 'overall_precision': 0.5119887165021156, 'overall_recall': 0.587378640776699, 'overall_f1': 0.5470987189148455, 'overall_accuracy': 0.695988346033169}\n",
            "------------EPOCH 20---------------\n",
            "Loss:  tensor(20.0339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.6401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.7061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.7654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.3218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.0599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.5678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.4729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.7526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4576, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.7715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.9264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.0924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.5957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.9265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.6177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.1387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.9846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.3539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.0931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.6243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.5813, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8412, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.0909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.6064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.6137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45604395604395603, 'recall': 0.5743944636678201, 'f1': 0.5084226646248086, 'number': 289}, 'P': {'precision': 0.5635838150289018, 'recall': 0.5927051671732523, 'f1': 0.5777777777777778, 'number': 329}, 'overall_precision': 0.5084507042253521, 'overall_recall': 0.5841423948220065, 'overall_f1': 0.5436746987951807, 'overall_accuracy': 0.6956521739130435}\n",
            "------------EPOCH 21---------------\n",
            "Loss:  tensor(17.7623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.0514, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.7566, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.8419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.6980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.3254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.6565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.1795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.7730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.9286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.7564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.6613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.0995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.9456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.5296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.8653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.5700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9249, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.7449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45308310991957107, 'recall': 0.5847750865051903, 'f1': 0.5105740181268883, 'number': 289}, 'P': {'precision': 0.565597667638484, 'recall': 0.5896656534954408, 'f1': 0.5773809523809524, 'number': 329}, 'overall_precision': 0.5069832402234636, 'overall_recall': 0.587378640776699, 'overall_f1': 0.5442278860569714, 'overall_accuracy': 0.6954280591662931}\n",
            "------------EPOCH 22---------------\n",
            "Loss:  tensor(16.2044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.6946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.8620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7613, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.2214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8311, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.2770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.0773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0496, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.8097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.8468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.8707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.1676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.0061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.9044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.6793, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.8563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.8204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.0547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.0399, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.44324324324324327, 'recall': 0.5674740484429066, 'f1': 0.4977238239757208, 'number': 289}, 'P': {'precision': 0.5677233429394812, 'recall': 0.5987841945288754, 'f1': 0.5828402366863905, 'number': 329}, 'overall_precision': 0.5034867503486751, 'overall_recall': 0.5841423948220065, 'overall_f1': 0.5408239700374533, 'overall_accuracy': 0.6961564320932317}\n",
            "------------EPOCH 23---------------\n",
            "Loss:  tensor(15.2257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6131, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.5924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.5040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1306, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.6985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.5225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.7024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3360, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.7125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.8809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0528, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.2877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.5378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.9852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8518, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.1080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.5615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.2694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.3885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4366576819407008, 'recall': 0.5605536332179931, 'f1': 0.49090909090909096, 'number': 289}, 'P': {'precision': 0.5660919540229885, 'recall': 0.5987841945288754, 'f1': 0.5819793205317578, 'number': 329}, 'overall_precision': 0.4993045897079277, 'overall_recall': 0.580906148867314, 'overall_f1': 0.537023186237846, 'overall_accuracy': 0.6954280591662931}\n",
            "------------EPOCH 24---------------\n",
            "Loss:  tensor(14.0355, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.6855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.9074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.2206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.4951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3704, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9540, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.4343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.8323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.4254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6306, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.7691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.4408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.5647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.43799472295514513, 'recall': 0.5743944636678201, 'f1': 0.497005988023952, 'number': 289}, 'P': {'precision': 0.5677966101694916, 'recall': 0.6109422492401215, 'f1': 0.5885797950219619, 'number': 329}, 'overall_precision': 0.5006821282401092, 'overall_recall': 0.5938511326860841, 'overall_f1': 0.543301258327165, 'overall_accuracy': 0.7040564769161811}\n",
            "------------EPOCH 25---------------\n",
            "Loss:  tensor(15.9011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1701, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.4499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3682, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.6473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.8177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.1364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.0703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.8802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.4347, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.0687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.4543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.8685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.2860, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.5495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5597, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.3554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.2100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.2357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.44266666666666665, 'recall': 0.5743944636678201, 'f1': 0.5, 'number': 289}, 'P': {'precision': 0.5845272206303725, 'recall': 0.6200607902735562, 'f1': 0.6017699115044248, 'number': 329}, 'overall_precision': 0.511049723756906, 'overall_recall': 0.598705501618123, 'overall_f1': 0.5514157973174366, 'overall_accuracy': 0.7028798744957419}\n",
            "------------EPOCH 26---------------\n",
            "Loss:  tensor(6.4956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.6780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.1135, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0216, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.9300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7071, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4453, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.6451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.0723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4514, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5239, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6564, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8679, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4430051813471503, 'recall': 0.5916955017301038, 'f1': 0.5066666666666667, 'number': 289}, 'P': {'precision': 0.5685714285714286, 'recall': 0.6048632218844985, 'f1': 0.5861561119293077, 'number': 329}, 'overall_precision': 0.5027173913043478, 'overall_recall': 0.598705501618123, 'overall_f1': 0.5465288035450517, 'overall_accuracy': 0.7004706409681757}\n",
            "------------EPOCH 27---------------\n",
            "Loss:  tensor(5.2903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6997, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.9000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.8741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.5021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.6310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5169, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1122, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.6544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1013, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.3379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.2984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7024, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.8883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7684, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.7520, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.4608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45478723404255317, 'recall': 0.5916955017301038, 'f1': 0.5142857142857143, 'number': 289}, 'P': {'precision': 0.5649717514124294, 'recall': 0.60790273556231, 'f1': 0.5856515373352854, 'number': 329}, 'overall_precision': 0.5082191780821917, 'overall_recall': 0.6003236245954693, 'overall_f1': 0.5504451038575667, 'overall_accuracy': 0.7032160466158673}\n",
            "------------EPOCH 28---------------\n",
            "Loss:  tensor(4.3152, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1890, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.5296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.0450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6618, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.4841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.4950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1231, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4405, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3180, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0973, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4479166666666667, 'recall': 0.5951557093425606, 'f1': 0.5111441307578009, 'number': 289}, 'P': {'precision': 0.5677966101694916, 'recall': 0.6109422492401215, 'f1': 0.5885797950219619, 'number': 329}, 'overall_precision': 0.505420054200542, 'overall_recall': 0.6035598705501618, 'overall_f1': 0.5501474926253688, 'overall_accuracy': 0.70338413267593}\n",
            "------------EPOCH 29---------------\n",
            "Loss:  tensor(3.6327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7413, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5347, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.6000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2899, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2818, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1919, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.7554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.2275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45263157894736844, 'recall': 0.5951557093425606, 'f1': 0.5142002989536621, 'number': 289}, 'P': {'precision': 0.56657223796034, 'recall': 0.60790273556231, 'f1': 0.5865102639296187, 'number': 329}, 'overall_precision': 0.5075034106412005, 'overall_recall': 0.6019417475728155, 'overall_f1': 0.550703182827535, 'overall_accuracy': 0.7033281039892425}\n",
            "------------EPOCH 30---------------\n",
            "Loss:  tensor(3.1557, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.4117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3742, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.3594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.3960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0602, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3667, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2211, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7141, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45144356955380577, 'recall': 0.5951557093425606, 'f1': 0.5134328358208956, 'number': 289}, 'P': {'precision': 0.56657223796034, 'recall': 0.60790273556231, 'f1': 0.5865102639296187, 'number': 329}, 'overall_precision': 0.5068119891008175, 'overall_recall': 0.6019417475728155, 'overall_f1': 0.5502958579881658, 'overall_accuracy': 0.7050089645898701}\n",
            "------------EPOCH 31---------------\n",
            "Loss:  tensor(2.8277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.1729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9936, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9156, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8213, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.0369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.9291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.5831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8504, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.5038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.45144356955380577, 'recall': 0.5951557093425606, 'f1': 0.5134328358208956, 'number': 289}, 'P': {'precision': 0.5685714285714286, 'recall': 0.6048632218844985, 'f1': 0.5861561119293077, 'number': 329}, 'overall_precision': 0.5075239398084815, 'overall_recall': 0.6003236245954693, 'overall_f1': 0.5500370644922166, 'overall_accuracy': 0.7028238458090542}\n",
            "------------EPOCH 32---------------\n",
            "Loss:  tensor(2.5555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.9680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.3856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.8101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7202, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7078, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0348, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.5222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2457, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.2452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8117, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4516971279373368, 'recall': 0.5986159169550173, 'f1': 0.5148809523809523, 'number': 289}, 'P': {'precision': 0.5669515669515669, 'recall': 0.6048632218844985, 'f1': 0.5852941176470589, 'number': 329}, 'overall_precision': 0.5068119891008175, 'overall_recall': 0.6019417475728155, 'overall_f1': 0.5502958579881658, 'overall_accuracy': 0.7027678171223667}\n",
            "------------EPOCH 33---------------\n",
            "Loss:  tensor(2.3335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.7964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.6597, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5590, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5675, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1.6612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8713, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3983, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.1314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.3341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0346, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.2731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.4484536082474227, 'recall': 0.6020761245674741, 'f1': 0.5140324963072378, 'number': 289}, 'P': {'precision': 0.5754985754985755, 'recall': 0.6139817629179332, 'f1': 0.5941176470588235, 'number': 329}, 'overall_precision': 0.5087956698240866, 'overall_recall': 0.6084142394822006, 'overall_f1': 0.5541635961680177, 'overall_accuracy': 0.7033281039892425}\n",
            "------------EPOCH 34---------------\n",
            "Loss:  tensor(2.2817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.1288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2303, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.1800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.0771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0362, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6187, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.4933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8225, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3600, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.1503, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6220, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.3526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.4595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.42424242424242425, 'recall': 0.5813148788927336, 'f1': 0.4905109489051095, 'number': 289}, 'P': {'precision': 0.5765765765765766, 'recall': 0.5835866261398176, 'f1': 0.580060422960725, 'number': 329}, 'overall_precision': 0.49382716049382713, 'overall_recall': 0.5825242718446602, 'overall_f1': 0.5345211581291759, 'overall_accuracy': 0.7005266696548633}\n",
            "------------EPOCH 35---------------\n",
            "Loss:  tensor(3.6951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.1182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.8163, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.6501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.1340, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.5591, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4013, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4102, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.2524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.2925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.3431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.7079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.5079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.2574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.1910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.6826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.7377, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.8918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.44274809160305345, 'recall': 0.6020761245674741, 'f1': 0.5102639296187684, 'number': 289}, 'P': {'precision': 0.5885885885885885, 'recall': 0.5957446808510638, 'f1': 0.5921450151057401, 'number': 329}, 'overall_precision': 0.509641873278237, 'overall_recall': 0.598705501618123, 'overall_f1': 0.5505952380952381, 'overall_accuracy': 0.698005378753922}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mi7LRmfIYbR"
      },
      "source": [
        "### Rough -- Checking dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9kGw_svbXM",
        "outputId": "b2838b14-a51e-4129-bb8c-20b274bb7a1c"
      },
      "source": [
        "\" \".join(\" mY name is \".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mY name is'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-oVkplUCJh"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qMGQ5GOzbn"
      },
      "source": [
        "train_dataset, _, test_dataset = get_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdpyzvKIfM8",
        "outputId": "8531f009-4bed-4ea9-daa4-d76d023d52e9"
      },
      "source": [
        "for tokenized_threads, masked_threads, comp_type_labels, _ in test_dataset:\n",
        "    tokenized_threads, masked_threads, comp_type_labels = tokenized_threads[0], masked_threads[0], comp_type_labels[0]\n",
        "    for tokenized_thread, masked_thread, comp_type_label in zip(tokenized_threads, masked_threads, comp_type_labels):\n",
        "        print(comp_type_label[:100])\n",
        "        print(tokenized_thread[:100])\n",
        "        print(tokenizer.decode(tokenized_thread[:500]))\n",
        "        start, end = 0, 0\n",
        "        prev_type = \"other\"\n",
        "        i = 0\n",
        "        while i<tokenized_thread.shape[0]:\n",
        "            if comp_type_label[i]==ac_dict[\"O\"]:\n",
        "                if prev_type==\"other\":\n",
        "                    end += 1\n",
        "                else:\n",
        "                    print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                    print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                    start = i\n",
        "                    end = i\n",
        "                    prev_type=\"other\"\n",
        "                \n",
        "            if comp_type_label[i] in [ac_dict[\"B-C\"], ac_dict[\"B-P\"]]:\n",
        "                print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                start = i\n",
        "                end = i\n",
        "                prev_type = \"Claim\" if comp_type_label[i]==ac_dict[\"B-C\"] else \"Premise\"\n",
        "            \n",
        "            if comp_type_label[i] in [ac_dict[\"I-C\"], ac_dict[\"I-P\"]]:\n",
        "                end += 1\n",
        "            \n",
        "            i+=1\n",
        "        break\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
            " 2 2 2 2 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4]\n",
            "[    0 18814   846    35  7978     9  1901    16   145   551   350   444\n",
            " 50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268\n",
            "   100  1819   923    84   481  1901    53     7   162  1437  8585    16\n",
            "    10   699   516   227 20203   110    78  8322   235    36  1437    22\n",
            "   270  1284 29384   328]\n",
            "<s>CMV: Freedom of speech is being taken too far [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE] I certainly value our free speech but to me there is a clear line between exercising your first amendment right (  \" President Obama sucks! \" etc ) and doing things that are known to be offensive to other cultures (  Satirical cartoons of prophets, assassinating leaders, etc ). [NEWLINE] [NEWLINE]  Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE] Sure, but that doesn't mean we condone the bully's actions and don't punish the bullies for acting. [NEWLINE] We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments. [NEWLINE]  Complete freedom in the expression of any idea, offensive or not, is a major element of that world. [NEWLINE] [NEWLINE]  If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us. [NEWLINE] [NEWLINE] [USER0] [NEWLINE] I certainly agree with your points - I didn't mean to imply that I was only for * * some * * freedom of speech. [NEWLINE] I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset. [NEWLINE] [NEWLINE]  Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"  [NEWLINE] Is that so hard? [NEWLINE] [NEWLINE] </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Component:  <s>CM  of type:  other [    0 18814]\n",
            "Masked Component:  <s>CM  of type:  other [    0 18814]\n",
            "Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Masked Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Masked Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Masked Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Component:   but to me   of type:  other [  53    7  162 1437]\n",
            "Masked Component:  <mask> to me   of type:  other [50264     7   162  1437]\n",
            "Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Masked Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Masked Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Masked Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Component:   (   of type:  other [  36 1437]\n",
            "Masked Component:   (   of type:  other [  36 1437]\n",
            "Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Masked Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Masked Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Component:   Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111    53   114    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Masked Component:   Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111 50264 50264    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            "    53   114    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Masked Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            " 50264 50264    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Component:  Sure  of type:  Claim [32541]\n",
            "Masked Component:  Sure  of type:  Claim [32541]\n",
            "Component:  , but   of type:  other [   6   53 1437]\n",
            "Masked Component:  ,<mask>   of type:  other [    6 50264  1437]\n",
            "Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Masked Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Masked Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Masked Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [ 318   47  236    7 1744    5 3519    7 1994   13  143    9  201    6\n",
            "   47   33    7 1744    5 3519    7 1994   13   70    9  201]\n",
            "Masked Component:  <mask> you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [50264    47   236     7  1744     5  3519     7  1994    13   143     9\n",
            "   201     6    47    33     7  1744     5  3519     7  1994    13    70\n",
            "     9   201]\n",
            "Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Masked Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Component:   -   of type:  other [ 111 1437]\n",
            "Masked Component:   -   of type:  other [ 111 1437]\n",
            "Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Masked Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901    77  1402  1134\n",
            "   120  4904]\n",
            "Masked Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech<mask> certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901 50264  1402  1134\n",
            "   120  4904]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"   of type:  Premise [ 2612    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45   142\n",
            "    52   214  6023     9    47    53   142    52  2098   110  2728     4\n",
            "    22  1437]\n",
            "Masked Component:  <mask> can't America be the bigger person and say \" Ok, we won't publish certain types of material, not<mask> we're afraid of you<mask><mask> we respect your views. \"   of type:  Premise [50264    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45 50264\n",
            "    52   214  6023     9    47 50264 50264    52  2098   110  2728     4\n",
            "    22  1437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZnb1Nz-MX4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2601bebd-3989-40a5-af7d-91d578651823"
      },
      "source": [
        "import re\n",
        "re.sub(r\"\\s*</claim>([^\\s])\", r\"</claim> \\1\", \"<claim>my name is </claim>jeevesh.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<claim>my name is</claim> jeevesh.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UVJb5jy178"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}