{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "long_context_am.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN47Ong8Rg7ObiYkbp9xI4o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/arg_mining/blob/main/experiments/long_context_am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGdooHmfgd-"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GPY8HXWtkyE"
      },
      "source": [
        "%%capture\n",
        "#if running on colab, install below 4\n",
        "#!git clone https://github.com/Jeevesh8/arg_mining\n",
        "#!pip install transformers\n",
        "#!pip install seqeval datasets allennlp\n",
        "#!pip install flax\n",
        "\n",
        "#if connected to local runtime, run the next command too\n",
        "#pip install bs4 tensorflow torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8QzJCbx9lO"
      },
      "source": [
        "\n",
        "\n",
        "*   Update ``arg_mining/datasets/cmv_modes/configs.py`` as per your requirements, all experiments considered till now, set ``batch_size`` to 2, and all other variables with their default value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glbajGinKG4"
      },
      "source": [
        "#Run to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pCBwYZjfkHw"
      },
      "source": [
        "### Load Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKUqJo4e_Rc"
      },
      "source": [
        "#from datasets import load_metric\n",
        "#metric = load_metric('seqeval')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kJb8EE0Fm-"
      },
      "source": [
        "### Krippendorff's Alpha Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1XOXUu0FOw"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class krip_alpha():\n",
        "    \"\"\"A module for computing sentence level Krippendorff's Alpha,\n",
        "    for argumentative components  annotated at the token level. Must use\n",
        "    labels [\"B-C\", \"B-P\"].\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"See self.compute_metric() for what each of these data actually mean.\n",
        "        \"\"\"\n",
        "        self.pred_has_claim = 0\n",
        "        self.ref_has_claim = 0\n",
        "        self.pred_has_premise = 0\n",
        "        self.ref_has_premise = 0\n",
        "        \n",
        "        self.claim_wise_agreement = 0\n",
        "        self.premise_wise_agreement = 0\n",
        "        \n",
        "        self.claim_wise_disagreement = 0\n",
        "        self.premise_wise_disagreement = 0\n",
        "    \n",
        "        self.total_sentences = 0\n",
        "        \n",
        "        self.has_both_ref = 0\n",
        "        self.has_both_pred = 0\n",
        "        self.has_none_ref = 0\n",
        "        self.has_none_pred = 0\n",
        "\n",
        "    def preprocess(self, threads: List[List[int]]) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            threads:    A list of all threads in a batch. A thread is a list of \n",
        "                        integers corresponding to token_ids of the tokens in the \n",
        "                        thread.\n",
        "        Returns:\n",
        "            A List with all the threads, where each thread now consists of \n",
        "            sentence lists. Where, a sentence list in a thread list is the list \n",
        "            of token_ids corresponding to a sentence in a thread. \n",
        "        \"\"\"\n",
        "        threads_lis = []\n",
        "\n",
        "        for i, thread in enumerate(threads):\n",
        "            sentence = []\n",
        "            threads_lis.append([])\n",
        "            for j, token_id in enumerate(thread):\n",
        "                if token_id==tokenizer.pad_token_id:\n",
        "                    break\n",
        "                \n",
        "                sentence.append(token_id)\n",
        "                token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "                #print(\"appended token:\", token)\n",
        "\n",
        "                next_token = 'None' if j==len(thread) else tokenizer.convert_ids_to_tokens(thread[j+1])\n",
        "\n",
        "                if (token.count('.')+token.count('?')+token.count('!')>=1 and \n",
        "                    next_token.count('.')+next_token.count('?')+next_token.count('!')==0):\n",
        "\n",
        "                    threads_lis[i].append(sentence)\n",
        "                    #print(\"Sample sentence: \", tokenizer.decode(sentence))\n",
        "                    sentence = []\n",
        "                \n",
        "                elif re.findall(r\"\\[USER\\d+\\]|\\[UNU\\]\", token)!=[]:\n",
        "                    prev_part = tokenizer.decode(sentence[:-1])[1:-1]\n",
        "                    if re.search(r'[a-zA-Z]', prev_part) is not None:\n",
        "                        threads_lis[i].append(sentence[:-1])\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(sentence[:-1]))\n",
        "                        sentence = [sentence[-1]]\n",
        "                    else:\n",
        "                        k=len(sentence)-2\n",
        "                        while k>=0 and sentence[k]==tokenizer.convert_tokens_to_ids('Ġ'):\n",
        "                            k-=1\n",
        "                        sentence = sentence[k+1:]\n",
        "                        threads_lis[i][-1] += sentence[:k]\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(threads_lis[i][-1]))\n",
        "                \n",
        "            has_rem_token = False\n",
        "            for elem in sentence:\n",
        "                if (elem!=tokenizer.convert_tokens_to_ids('Ġ') and\n",
        "                    elem!=tokenizer.eos_token_id):\n",
        "                    has_rem_token = True\n",
        "                    break\n",
        "            \n",
        "            if has_rem_token:\n",
        "                threads_lis[i].append(sentence)\n",
        "                #print(\"Sample sentence at end of thread: \", tokenizer.decode(sentence))\n",
        "                sentence = []\n",
        "\n",
        "        return threads_lis\n",
        "\n",
        "    def get_sentence_wise_preds(self, threads: List[List[List[int]]], \n",
        "                                      predictions: List[List[str]]) -> List[List[List[str]]]:\n",
        "        \"\"\"Splits the prediction corresponding to each thread, into predictions\n",
        "        for each sentence in the corresponding thread in \"threads\" list.\n",
        "        Args:\n",
        "            threads:      A list of threads, where each thread consists of further \n",
        "                          lists corresponding to the various sentences in the\n",
        "                          thread. [As output by self.preprocess()]\n",
        "            predictions:  A list of predictions for each thread, in the threads\n",
        "                          list. Each prediciton consists of a list of componenet \n",
        "                          types corresponding to each token in a thread.\n",
        "        Returns:\n",
        "            The predictions list, with each prediction split into predictions \n",
        "            corresponding to the sentences in the corresponding thread specified\n",
        "            in the threads list. \n",
        "        \"\"\"\n",
        "        sentence_wise_preds = []\n",
        "        for i, thread in enumerate(threads):\n",
        "            next_sentence_beg = 0\n",
        "            sentence_wise_preds.append([])\n",
        "            for sentence in thread:\n",
        "                sentence_wise_preds[i].append(\n",
        "                    predictions[i][next_sentence_beg:next_sentence_beg+len(sentence)])\n",
        "                next_sentence_beg += len(sentence)\n",
        "        return sentence_wise_preds\n",
        "    \n",
        "    def update_state(self, pred_sentence: List[str], ref_sentence: List[str]) -> None:\n",
        "        \"\"\"Updates the various information maintained for the computation of\n",
        "        Krippendorff's alpha, based on the predictions(pred_sentence) and \n",
        "        references(ref_sentence) provided for a particular sentence, in some \n",
        "        thread.\n",
        "        \"\"\"\n",
        "        self.total_sentences += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence:\n",
        "            self.pred_has_claim += 1\n",
        "            if 'B-C' in ref_sentence:\n",
        "                self.ref_has_claim += 1\n",
        "                self.claim_wise_agreement += 1\n",
        "            else:\n",
        "                self.claim_wise_disagreement += 1\n",
        "            \n",
        "        elif 'B-C' in ref_sentence:\n",
        "            self.ref_has_claim += 1\n",
        "            self.claim_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.claim_wise_agreement += 1\n",
        "        \n",
        "        if 'B-P' in pred_sentence:\n",
        "            self.pred_has_premise += 1\n",
        "            if 'B-P' in ref_sentence:\n",
        "                self.ref_has_premise += 1\n",
        "                self.premise_wise_agreement += 1\n",
        "            else:\n",
        "                self.premise_wise_disagreement += 1\n",
        "\n",
        "        elif 'B-P' in ref_sentence:\n",
        "            self.ref_has_premise += 1\n",
        "            self.premise_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.premise_wise_agreement += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence and 'B-P' in pred_sentence:\n",
        "            self.has_both_pred += 1\n",
        "        \n",
        "        if 'B-C' in ref_sentence and 'B-P' in ref_sentence:\n",
        "            self.has_both_ref += 1\n",
        "        \n",
        "        if 'B-C' not in pred_sentence and 'B-P' not in pred_sentence:\n",
        "            self.has_none_pred += 1\n",
        "        \n",
        "        if 'B-C' not in ref_sentence and 'B-P' not in ref_sentence:\n",
        "            self.has_none_ref += 1\n",
        "        return\n",
        "\n",
        "    def add_batch(self, predictions: List[List[str]], \n",
        "                  references: List[List[str]], \n",
        "                  tokenized_threads: List[List[int]]) -> None:\n",
        "        \"\"\"Add a batch of predictions and references for the computation of \n",
        "        Krippendorff's alpha.\n",
        "        Args:\n",
        "            predictions:      A list of predictions for each thread, in the \n",
        "                              threads list. Each prediciton consists of a list \n",
        "                              of component types corresponding to each token in \n",
        "                              a thread.\n",
        "            references:       Same structure as predictions, but consisting of \n",
        "                              acutal gold labels, instead of predicted ones.\n",
        "            tokenized_thread: A list of all threads in a batch. A thread is a \n",
        "                              list of integers corresponding to token_ids of the\n",
        "                              tokens in the thread.\n",
        "        \"\"\"\n",
        "        threads = self.preprocess(tokenized_threads)\n",
        "        \n",
        "        sentence_wise_preds = self.get_sentence_wise_preds(threads, predictions)\n",
        "        sentence_wise_refs = self.get_sentence_wise_preds(threads, references)\n",
        "\n",
        "        for pred_thread, ref_thread in zip(sentence_wise_preds, sentence_wise_refs):\n",
        "            for pred_sentence, ref_sentence in zip(pred_thread, ref_thread):\n",
        "                self.update_state(pred_sentence, ref_sentence)\n",
        "\n",
        "    def compute(self, print_additional: bool=True) -> None:\n",
        "        \"\"\"Prints out the metric, for the batched added till now. And then \n",
        "        resets all data being maintained by the metric. \n",
        "        Args:\n",
        "            print_additional:   If True, will print all the data being \n",
        "                                maintained instead of just the Krippendorff's \n",
        "                                alphas for claims and premises.\n",
        "        \"\"\"\n",
        "        print(\"Sentence level Krippendorff's alpha for Claims: \", 1-(self.claim_wise_disagreement/(self.claim_wise_agreement+self.claim_wise_disagreement))/0.5)\n",
        "        print(\"Sentence level Krippendorff's alpha for Premises: \", 1-(self.premise_wise_disagreement/(self.premise_wise_agreement+self.premise_wise_disagreement))/0.5)\n",
        "        \n",
        "        if print_additional:\n",
        "            print(\"Additional attributes: \")\n",
        "            print(\"\\tTotal Sentences:\", self.total_sentences)\n",
        "            print(\"\\tPrediction setences having claims:\", self.pred_has_claim)\n",
        "            print(\"\\tPrediction sentences having premises:\", self.pred_has_premise)\n",
        "            print(\"\\tReference setences having claims:\", self.ref_has_claim)\n",
        "            print(\"\\tReference sentences having premises:\", self.ref_has_premise)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tPrediction Sentence having both claim and premise:\", self.has_both_pred)\n",
        "            print(\"\\tPrediction Sentence having neither claim nor premise:\", self.has_none_pred)\n",
        "            print(\"\\tReference Sentence having both claim and premise:\", self.has_both_ref)\n",
        "            print(\"\\tReference Sentence having neither claim nor premise:\", self.has_none_ref)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tSentences having claim in both reference and prediction:\", self.claim_wise_agreement)\n",
        "            print(\"\\tSentences having claim in only one of reference or prediction:\", self.claim_wise_disagreement)\n",
        "            print(\"\\tSentences having premise in both reference and prediction:\", self.premise_wise_agreement)\n",
        "            print(\"\\tSentences having premise in only one of reference or prediction:\", self.premise_wise_disagreement)\n",
        "        self.__init__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRNv0wBWtFd"
      },
      "source": [
        "metric = krip_alpha()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6BxMkfm3R"
      },
      "source": [
        "### Define & Load Tokenizer, Model, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wcsqmllnfRB"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DeXxCDS_id",
        "outputId": "e82f23f7-8380-4b45-fd96-73943a990807"
      },
      "source": [
        "device"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRkCQOZu6HS"
      },
      "source": [
        "model_version = 'allenai/longformer-base-4096'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cB-7M9t0HP"
      },
      "source": [
        "%%capture\n",
        "from transformers import LongformerTokenizer, AutoModel\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_version)\n",
        "transformer_model = AutoModel.from_pretrained(model_version).to(device)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajTzrbzkwbT"
      },
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "#Extend Token Type Embeddings\n",
        "def resize_token_type_embeddings(transformer_model, new_size):\n",
        "    old_embeddings = transformer_model.embeddings.token_type_embeddings.weight\n",
        "    old_size, hidden_dim = old_embeddings.shape\n",
        "    transformer_model.embeddings.token_type_embeddings = nn.Embedding(new_size, hidden_dim, device=transformer_model.device)\n",
        "    with torch.no_grad():\n",
        "        transformer_model.embeddings.token_type_embeddings.weight[:old_size] = old_embeddings\n",
        "\n",
        "#resize_token_type_embeddings(transformer_model, 2)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p6dGA83cGVS"
      },
      "source": [
        "with open('./Discourse_Markers.txt') as f:\n",
        "    discourse_markers = [dm.strip() for dm in f.readlines()]"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCeCgbLxVyQ"
      },
      "source": [
        "%%capture\n",
        "from arg_mining.datasets.cmv_modes import load_dataset, data_config\n",
        "\n",
        "tokenizer.add_tokens(data_config[\"special_tokens\"])\n",
        "\n",
        "transformer_model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_8gVbufzoX"
      },
      "source": [
        "### Define layers for a Linear-Chain-CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seBwkZdByesM"
      },
      "source": [
        "from allennlp.modules.conditional_random_field import ConditionalRandomField as crf\n",
        "\n",
        "ac_dict = data_config[\"arg_components\"]\n",
        "\n",
        "allowed_transitions =([(ac_dict[\"B-C\"], ac_dict[\"I-C\"]), \n",
        "                       (ac_dict[\"B-P\"], ac_dict[\"I-P\"])] + \n",
        "                      [(ac_dict[\"I-C\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-C\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"I-P\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-P\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"O\"], ac_dict[ct]) \n",
        "                        for ct in [\"O\", \"B-C\", \"B-P\"]])\n",
        "                    \n",
        "linear_layer = nn.Linear(transformer_model.config.hidden_size,\n",
        "                         len(ac_dict)).to(device)\n",
        "\n",
        "crf_layer = crf(num_tags=len(ac_dict),\n",
        "                constraints=allowed_transitions,\n",
        "                include_start_end_transitions=False).to(device)\n",
        "\n",
        "cross_entropy_layer = nn.CrossEntropyLoss(weight=torch.log(torch.tensor([3.3102, 61.4809, 3.6832, 49.6827, 2.5639], \n",
        "                                                                        device=device)), reduction='none')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUicsK33f9d7"
      },
      "source": [
        "### Global Attention Mask Utility for Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSo7p2I5mOi"
      },
      "source": [
        "import numpy as np\n",
        "from threading import Lock\n",
        "\n",
        "def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n",
        "    0 elsewhere.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(tokenized_threads)\n",
        "    for user_token in [\"UNU\"]+[f\"[USER{i}]\" for i in range(data_config[\"max_users\"])]:\n",
        "        user_token_id = tokenizer.encode(user_token)[1:-1]\n",
        "        mask = np.where(tokenized_threads==user_token_id, 1, mask)\n",
        "    return np.array(mask, dtype=bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4PLQihf5CT"
      },
      "source": [
        "### Loss and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3u8eH1rjZe"
      },
      "source": [
        "from typing import Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuJ5aryW9tUC"
      },
      "source": [
        "def compute(batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            preds: bool=False, cross_entropy: bool=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n",
        "                component type labels of shape [batch_size, seq_len], and a global\n",
        "                attention mask for Longformer, of the same shape.\n",
        "        \n",
        "        preds:  If True, returns a List(of batch_size size) of Tuples of form \n",
        "                (tag_sequence, viterbi_score) where the tag_sequence is the \n",
        "                viterbi-decoded sequence, for the corresponding sample in the batch.\n",
        "        \n",
        "        cross_entropy:  This argument will only be used if preds=False, i.e., if \n",
        "                        loss is being calculated. If True, then cross entropy loss\n",
        "                        will also be added to the output loss.\n",
        "    \n",
        "    Returns:\n",
        "        Either the predicted sequences with their scores for each element in the batch\n",
        "        (if preds is True), or the loss value summed over all elements of the batch\n",
        "        (if preds is False).\n",
        "    \"\"\"\n",
        "    tokenized_threads, token_type_ids, comp_type_labels, global_attention_mask = batch\n",
        "    \n",
        "    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n",
        "    \n",
        "    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n",
        "                               attention_mask=pad_mask,\n",
        "                               global_attention_mask=global_attention_mask).last_hidden_state)\n",
        "    \n",
        "    if preds:\n",
        "        return crf_layer.viterbi_tags(logits, pad_mask)\n",
        "    \n",
        "    log_likelihood = crf_layer(logits, comp_type_labels, pad_mask)\n",
        "    \n",
        "    if cross_entropy:\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        \n",
        "        pad_mask, comp_type_labels = pad_mask.reshape(-1), comp_type_labels.reshape(-1)\n",
        "        \n",
        "        ce_loss = torch.sum(pad_mask*cross_entropy_layer(logits, comp_type_labels))\n",
        "        \n",
        "        return ce_loss - log_likelihood\n",
        "\n",
        "    return -log_likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lkPCsgEY4"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yfpzEMBGra"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n",
        "                                      linear_layer.parameters(),\n",
        "                                      crf_layer.parameters()),\n",
        "                       lr = 2e-5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSWnkO8gLD6"
      },
      "source": [
        "### Training And Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTNaP3kLaN2X"
      },
      "source": [
        "def train(dataset):\n",
        "    accumulate_over = 4\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (tokenized_threads, masked_threads, comp_type_labels, _ ) in enumerate(dataset):\n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads),\n",
        "                                             device=device, dtype=torch.int32)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0), \n",
        "                                         device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0), \n",
        "                                      device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0), \n",
        "                                        device=device, dtype=torch.long)\n",
        "        \n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        loss = compute((tokenized_threads,\n",
        "                        torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                        comp_type_labels, \n",
        "                        global_attention_mask))/data_config[\"batch_size\"]\n",
        "        \n",
        "        print(\"Loss: \", loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        if i%accumulate_over==accumulate_over-1:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHJiT0sjXid"
      },
      "source": [
        "#transformer_model.config.type_vocab_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BpSI83cU24"
      },
      "source": [
        "def evaluate(dataset, metric):\n",
        "    \n",
        "    int_to_labels = {v:k for k, v in ac_dict.items()}\n",
        "    \n",
        "    for tokenized_threads, masked_threads, comp_type_labels, _ in dataset:\n",
        "        \n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads), \n",
        "                                             device=device)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0),\n",
        "                                        device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0),\n",
        "                                     device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0),\n",
        "                                        device=device)\n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        preds = compute((tokenized_threads,\n",
        "                         torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                         comp_type_labels,\n",
        "                         global_attention_mask),\n",
        "                        preds=True)\n",
        "        \n",
        "        lengths = torch.sum(torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0), \n",
        "                            axis=-1)\n",
        "        \n",
        "        preds = [ [int_to_labels[pred] for pred in pred[0][:lengths[i]]]\n",
        "                  for i, pred in enumerate(preds)\n",
        "                ]\n",
        "        \n",
        "        refs = [ [int_to_labels[ref] for ref in labels[:lengths[i]]]\n",
        "                 for i, labels in enumerate(comp_type_labels.cpu().tolist())\n",
        "               ]\n",
        "        \n",
        "        metric.add_batch(predictions=preds, \n",
        "                         references=refs,\n",
        "                         tokenized_threads=tokenized_threads.cpu().tolist())\n",
        "    \n",
        "    metric.compute()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZV6rnIQgOYA"
      },
      "source": [
        "### Final Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2O5qCufGwA"
      },
      "source": [
        "n_epochs = 35"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xcK9sOgX44",
        "outputId": "a06e29ea-0bac-4236-b0f1-3d39ed2d8df8"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    print(f\"------------EPOCH {epoch+1}---------------\")\n",
        "    train_dataset, _, test_dataset = get_datasets()\n",
        "    train(train_dataset)\n",
        "    evaluate(test_dataset, metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------EPOCH 1---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n",
            "2021-08-28 08:30:02.365331: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2021-08-28 08:30:02.366103: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(3407.9351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2641.9219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3555.2568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3094.2864, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1984.3126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1975.3589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2497.9238, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2742.0479, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1779.1321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1940.6316, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2118.5872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3209.8467, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2486.1079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3834.9028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3660.1646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2477.6904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2312.0889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(933.2599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1696.5168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1109.8612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2127.4541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1383.7949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2146.1470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1551.5142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3540.4775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6521.0093, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3584.9785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3775.4924, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1514.2583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1422.9335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1336.5867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3034.4746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1954.7271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2151.1462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3066.0212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(983.8304, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3174.8647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5803.3486, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1556.4009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3533.8567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2241.7998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3637.8838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3273.3064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4881.7598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1891.0533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.4418103448275862\n",
            "Sentence level Krippendorff's alpha for Premises:  0.2349137931034483\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 22\n",
            "\tPrediction sentences having premises: 732\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 10\n",
            "\tPrediction Sentence having neither claim nor premise: 184\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 669\n",
            "\tSentences having claim in only one of reference or prediction: 259\n",
            "\tSentences having premise in both reference and prediction: 573\n",
            "\tSentences having premise in only one of reference or prediction: 355\n",
            "------------EPOCH 2---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(2403.8184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1937.0144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2681.0688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2442.2280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1732.7751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1704.4634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2023.1184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1900.6138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1321.7638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1452.8081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1624.4686, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2554.2480, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1944.0974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3223.6528, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3024.0740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1938.8633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1937.9811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(820.0444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1484.4537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(971.4950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1782.3247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1179.1562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1757.1870, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1297.8234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2958.2827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5657.3818, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3017.5103, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3270.8105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1273.7816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1256.5002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1185.0110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2611.1484, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1649.8694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1814.7488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2669.6631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(867.9749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2634.0940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4974.5811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1341.9359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3054.8032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2057.5017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3171.6548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2719.3115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4166.1416, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1653.9777, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.4181034482758621\n",
            "Sentence level Krippendorff's alpha for Premises:  0.4288793103448276\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 53\n",
            "\tPrediction sentences having premises: 630\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 13\n",
            "\tPrediction Sentence having neither claim nor premise: 258\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 658\n",
            "\tSentences having claim in only one of reference or prediction: 270\n",
            "\tSentences having premise in both reference and prediction: 663\n",
            "\tSentences having premise in only one of reference or prediction: 265\n",
            "------------EPOCH 3---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(2148.3301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1760.2097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2416.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2226.6196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1553.4331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1529.8361, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1745.9009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1597.6711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1061.7511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1207.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1363.6497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2149.8330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1436.8474, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2749.0005, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2605.0840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1666.2042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1712.8042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(706.7232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1299.0896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(879.2318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1532.4513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1041.3826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1516.9733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1070.8298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2465.3752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5031.9644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2429.3398, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2840.6501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1031.1199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1117.9290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1041.6536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2292.1816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1344.0400, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1402.6895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2285.9502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(744.3423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2198.9041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4173.4429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1116.1646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2345.9993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1803.4161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2796.4209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2250.7510, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3625.3452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1482.2089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.49353448275862066\n",
            "Sentence level Krippendorff's alpha for Premises:  0.5258620689655172\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 152\n",
            "\tPrediction sentences having premises: 523\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 56\n",
            "\tPrediction Sentence having neither claim nor premise: 309\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 693\n",
            "\tSentences having claim in only one of reference or prediction: 235\n",
            "\tSentences having premise in both reference and prediction: 708\n",
            "\tSentences having premise in only one of reference or prediction: 220\n",
            "------------EPOCH 4---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(1862.4155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1587.3352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2130.1296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1956.3630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1239.4824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1216.9395, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1429.4263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1367.8455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(895.9198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(961.7404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1125.2979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1805.4146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1011.0215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2152.5083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2182.7537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1381.7607, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1379.5760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(614.3942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1090.6050, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(784.6835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1243.6116, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(885.6274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1232.3569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(779.9200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1981.2433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4288.0381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1882.5278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2274.1792, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(829.1843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(911.2262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(881.9457, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2010.4429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1151.3696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1228.9087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2059.3489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(692.3046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1592.0012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3414.8193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(881.5380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1875.4324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1617.0641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2246.8567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1640.1028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2856.9783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1253.4813, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5021551724137931\n",
            "Sentence level Krippendorff's alpha for Premises:  0.5775862068965517\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 94\n",
            "\tPrediction sentences having premises: 475\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 34\n",
            "\tPrediction Sentence having neither claim nor premise: 393\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 697\n",
            "\tSentences having claim in only one of reference or prediction: 231\n",
            "\tSentences having premise in both reference and prediction: 732\n",
            "\tSentences having premise in only one of reference or prediction: 196\n",
            "------------EPOCH 5---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(1632.0532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1433.8461, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1886.7905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1692.1345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(807.6995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(906.8228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1122.5161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1057.6001, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(715.6586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(807.3764, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(860.1114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1448.7419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(654.5310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1664.6887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1743.6841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1209.7344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1150.5952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(495.5968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(940.7030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(694.5305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(961.2538, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(785.4436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(969.6191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(575.0115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1545.6133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3601.4568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1246.7263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1806.9674, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(608.4897, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(708.8060, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(688.4548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1586.5061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(821.3429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(871.7587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1753.4255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.6364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1161.7672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2590.3882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(734.2488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1535.5958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1245.6533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1682.5813, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1178.2401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2301.1929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(835.0415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5431034482758621\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6142241379310345\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 109\n",
            "\tPrediction sentences having premises: 522\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 35\n",
            "\tPrediction Sentence having neither claim nor premise: 332\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 716\n",
            "\tSentences having claim in only one of reference or prediction: 212\n",
            "\tSentences having premise in both reference and prediction: 749\n",
            "\tSentences having premise in only one of reference or prediction: 179\n",
            "------------EPOCH 6---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(1389.7288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1150.0918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1671.3004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1138.6744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(628.3945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(700.5011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(971.7308, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(843.5500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(934.7783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(860.9689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1211.5212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1510.9691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(602.2466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1837.9841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1651.8816, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1041.9370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(917.8258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(427.6942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1000.3201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(706.4080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1153.4575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(917.0580, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1276.2280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(556.3182, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1399.8379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3285.8716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1162.7539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1583.7012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(481.0373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(478.1342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(506.2750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1050.2784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1037.3887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1161.1550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1832.4956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(575.1934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1546.2389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3968.8975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(980.4643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2769.6233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1697.2208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2169.0928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1534.7468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2660.2168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(930.6867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5107758620689655\n",
            "Sentence level Krippendorff's alpha for Premises:  0.5495689655172413\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 398\n",
            "\tPrediction sentences having premises: 304\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 67\n",
            "\tPrediction Sentence having neither claim nor premise: 293\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 701\n",
            "\tSentences having claim in only one of reference or prediction: 227\n",
            "\tSentences having premise in both reference and prediction: 719\n",
            "\tSentences having premise in only one of reference or prediction: 209\n",
            "------------EPOCH 7---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(1070.9681, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1047.5367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1137.2739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1134.3325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(517.2913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(496.2416, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(820.8821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(682.0155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(548.6782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(643.3915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(716.4697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1238.2324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(611.5044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1508.7593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1486.4990, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(966.0567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(918.3090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(502.6023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1137.5732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(749.0668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1234.4885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1045.1304, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1290.3147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(938.8383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1803.0012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3479.8352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1637.1958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1767.9214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(665.1652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(713.6699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(598.0500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1817.8925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(864.1880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(600.4270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1627.7151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.9197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1343.4121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2842.3872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(803.5209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1411.9929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1174.6042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1565.6732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1099.6787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2109.2690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(674.5928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.4461206896551724\n",
            "Sentence level Krippendorff's alpha for Premises:  0.4849137931034483\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 464\n",
            "\tPrediction sentences having premises: 232\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 71\n",
            "\tPrediction Sentence having neither claim nor premise: 303\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 671\n",
            "\tSentences having claim in only one of reference or prediction: 257\n",
            "\tSentences having premise in both reference and prediction: 689\n",
            "\tSentences having premise in only one of reference or prediction: 239\n",
            "------------EPOCH 8---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(946.2626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(902.3070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(921.7173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1002.2727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(398.1904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(368.6743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(626.2266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(702.9415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(667.5663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(757.1695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(693.6782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1190.9304, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(890.1402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2150.8047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1898.5295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1299.6204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1451.9640, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(452.6414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1122.0271, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(530.4971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(770.0875, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(676.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(764.7322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(390.9140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1135.6125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2280.5488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(804.6203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(995.2766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(325.4977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(338.2090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(333.5555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(952.1689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(591.2698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(493.4028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1145.7512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(382.5729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(869.1754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1995.9498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(586.6121, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1142.3430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1147.7251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1515.7241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(897.8889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1743.8525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(530.4335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5668103448275862\n",
            "Sentence level Krippendorff's alpha for Premises:  0.665948275862069\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 242\n",
            "\tPrediction sentences having premises: 458\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 292\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 727\n",
            "\tSentences having claim in only one of reference or prediction: 201\n",
            "\tSentences having premise in both reference and prediction: 773\n",
            "\tSentences having premise in only one of reference or prediction: 155\n",
            "------------EPOCH 9---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(918.6301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(811.4392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(965.5130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(710.0371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(266.2195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(277.3233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(430.3559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(380.5029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(297.8382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(420.6367, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(365.2918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(705.2374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(295.6415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(911.2621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(816.9630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(596.2197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(700.1212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(318.2674, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(979.0478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(623.8042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(826.3422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(645.8975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(649.6356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(272.2327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(904.5508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1743.3252, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(439.1147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(762.6649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.5937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(215.9770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(315.9650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(926.0883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(343.8296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(370.0319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(845.8038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(190.4233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(419.2150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1442.0854, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(359.6700, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(834.2418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(586.7754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(895.3290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(623.5957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1109.8369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(394.0279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5107758620689655\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6185344827586207\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 308\n",
            "\tPrediction sentences having premises: 474\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 83\n",
            "\tPrediction Sentence having neither claim nor premise: 229\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 701\n",
            "\tSentences having claim in only one of reference or prediction: 227\n",
            "\tSentences having premise in both reference and prediction: 751\n",
            "\tSentences having premise in only one of reference or prediction: 177\n",
            "------------EPOCH 10---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(1008.6840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(871.3998, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1034.2743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(899.8966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(440.2283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(299.2635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(747.2942, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(426.5047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.0706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(462.6805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(516.3536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1319.1243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(324.2821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1244.8494, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(846.4351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(595.7838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(481.7957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(263.0379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(600.4692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(344.6133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(448.4633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(371.0795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(415.6142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.7845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(726.8648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1331.4045, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(268.2639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(520.3035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(157.8266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(159.3183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(350.6767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(587.5286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(279.4357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(378.9041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(893.3275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(185.4895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(470.5524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1633.3738, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(513.8217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(834.6595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(691.3212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(723.5598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(573.6380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1214.8569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(256.8118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5797413793103448\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6573275862068966\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 266\n",
            "\tPrediction sentences having premises: 406\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 51\n",
            "\tPrediction Sentence having neither claim nor premise: 307\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 733\n",
            "\tSentences having claim in only one of reference or prediction: 195\n",
            "\tSentences having premise in both reference and prediction: 769\n",
            "\tSentences having premise in only one of reference or prediction: 159\n",
            "------------EPOCH 11---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(544.1752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(415.0838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(592.6981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(417.2905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(145.8073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.7218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(229.7694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(198.5183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(144.7285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(261.1337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(185.2332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(444.5233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(171.8065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(977.7795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(542.8106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(491.6646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(634.5112, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(183.4717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(515.8776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(300.7729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(461.7341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(418.1342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(320.2806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.6947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(722.8966, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1248.1660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(264.1954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(794.2716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.4554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.6489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(168.8588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(413.4320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(226.0421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(229.3406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(646.2477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(115.3186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(185.3874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(868.5376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(261.6336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(637.4863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(400.9730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(543.6195, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(459.8961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(817.8342, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(243.8746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.4849137931034483\n",
            "Sentence level Krippendorff's alpha for Premises:  0.5172413793103448\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 416\n",
            "\tPrediction sentences having premises: 247\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 56\n",
            "\tPrediction Sentence having neither claim nor premise: 321\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 689\n",
            "\tSentences having claim in only one of reference or prediction: 239\n",
            "\tSentences having premise in both reference and prediction: 704\n",
            "\tSentences having premise in only one of reference or prediction: 224\n",
            "------------EPOCH 12---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(460.5989, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(451.0874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(663.3718, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(590.3903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(139.7658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(106.4458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(215.5584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(225.0263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(152.8079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(145.2464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(236.1622, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(361.8846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.4140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(446.8363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(342.2214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(276.6680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(256.4746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(111.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(358.5479, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(180.6690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(373.1921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(331.4869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(348.2433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.2199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(722.8142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(957.6914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(334.9150, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(489.2174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(207.6849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(131.3568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(193.3512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(691.3444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(185.4831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.8820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(487.4744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(126.3026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(164.9111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(710.5601, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(137.7380, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(459.1038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(231.0374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(305.6353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.2204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(432.3260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(162.0411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.4137931034482759\n",
            "Sentence level Krippendorff's alpha for Premises:  0.42025862068965514\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 473\n",
            "\tPrediction sentences having premises: 176\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 46\n",
            "\tPrediction Sentence having neither claim nor premise: 325\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 656\n",
            "\tSentences having claim in only one of reference or prediction: 272\n",
            "\tSentences having premise in both reference and prediction: 659\n",
            "\tSentences having premise in only one of reference or prediction: 269\n",
            "------------EPOCH 13---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(398.5171, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(377.7353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(540.4568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(374.3389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(167.0393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.6612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(253.6351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(358.1801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(311.3766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(389.3780, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(327.4110, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(522.0153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(241.0518, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(676.4017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(742.1868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(598.6212, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(234.9883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(160.7369, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(337.8309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(182.2133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(207.0215, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(177.6631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(200.6120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.1263, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(330.1157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(466.7309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.2511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(160.5232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.7084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.6629, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(114.1774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(182.2840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(178.7616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.8541, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(445.0012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.7241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(401.9563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1037.1016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(276.7094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(601.7751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(382.0429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(781.4056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(368.1447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(812.2264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.2396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5754310344827587\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6379310344827587\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 270\n",
            "\tPrediction sentences having premises: 425\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 297\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 731\n",
            "\tSentences having claim in only one of reference or prediction: 197\n",
            "\tSentences having premise in both reference and prediction: 760\n",
            "\tSentences having premise in only one of reference or prediction: 168\n",
            "------------EPOCH 14---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(365.1920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(327.4026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(517.7158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(189.4404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.6775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.8747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.5744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(88.7049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.2350, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.1166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.2642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(185.4003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.6105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(332.0842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(217.3721, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(202.1298, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(154.5442, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.7546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(272.6238, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(144.8106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(253.5079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(186.7194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(282.8965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.2196, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(581.2214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1051.6670, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(190.5497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(235.1115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(115.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.0734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(156.4828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(428.2336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(194.1798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(279.4288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(461.2327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.2914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.0058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(656.4744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(140.8513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(443.8821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(184.1312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(234.6531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.8349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(384.6831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.3257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5775862068965517\n",
            "Sentence level Krippendorff's alpha for Premises:  0.646551724137931\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 271\n",
            "\tPrediction sentences having premises: 417\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 304\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 732\n",
            "\tSentences having claim in only one of reference or prediction: 196\n",
            "\tSentences having premise in both reference and prediction: 764\n",
            "\tSentences having premise in only one of reference or prediction: 164\n",
            "------------EPOCH 15---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(244.0293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(223.4388, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(366.8988, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.1657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(123.0663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(105.5287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(146.7771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(101.7850, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.1771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.7345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(145.9584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(358.0140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.9915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(416.2232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(349.8435, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(196.9111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(181.7145, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.3908, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(350.7387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(176.7343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(183.7900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(234.7543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(168.6862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.5146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(300.2022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(406.6164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.6409, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(171.7206, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.4821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.8842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(92.7765, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(150.8197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.1705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.5153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(217.1611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.7532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.2339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(273.8065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.2205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(332.9882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(170.3979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(179.8197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.5328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(247.8870, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.4387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5775862068965517\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6206896551724138\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 341\n",
            "\tPrediction sentences having premises: 323\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 55\n",
            "\tPrediction Sentence having neither claim nor premise: 319\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 732\n",
            "\tSentences having claim in only one of reference or prediction: 196\n",
            "\tSentences having premise in both reference and prediction: 752\n",
            "\tSentences having premise in only one of reference or prediction: 176\n",
            "------------EPOCH 16---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(138.6021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(152.2540, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(200.5356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(126.6508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.2208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.7970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.5458, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.9219, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.9072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4720, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.9407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(199.5865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.9570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(239.1035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.9090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.5642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.4295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.8003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(265.8384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.0766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.8335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.5527, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.4252, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.9523, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(162.3464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(300.4437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.2251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.5770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.4999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.8318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.4309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.5378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.9235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.2233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(196.4656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.3394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(75.1985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(224.9534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.2987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(297.1925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.9305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(138.6274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.4961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.3886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.2604, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5689655172413793\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6400862068965517\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 307\n",
            "\tPrediction sentences having premises: 372\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 59\n",
            "\tPrediction Sentence having neither claim nor premise: 308\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 728\n",
            "\tSentences having claim in only one of reference or prediction: 200\n",
            "\tSentences having premise in both reference and prediction: 761\n",
            "\tSentences having premise in only one of reference or prediction: 167\n",
            "------------EPOCH 17---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(103.3017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(132.5073, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.9214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.8259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.9146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.9087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.1493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.8083, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.9826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(131.8044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.0568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(189.2531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.4638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.2761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.8484, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.5309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(180.8817, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.9137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.5581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.9971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.2345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(128.4795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(169.3946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.3183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.8081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.0994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.9222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.2463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.8677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3981, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(148.5118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.1652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.1725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.4989, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(253.4559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.8442, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.5266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.0954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(151.3154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.0601, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5646551724137931\n",
            "Sentence level Krippendorff's alpha for Premises:  0.625\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 299\n",
            "\tPrediction sentences having premises: 375\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 59\n",
            "\tPrediction Sentence having neither claim nor premise: 313\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 726\n",
            "\tSentences having claim in only one of reference or prediction: 202\n",
            "\tSentences having premise in both reference and prediction: 754\n",
            "\tSentences having premise in only one of reference or prediction: 174\n",
            "------------EPOCH 18---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(76.0253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.9201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.2495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.2652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.0029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.9125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.4851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.7432, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.7451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.9130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(103.6038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.6477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.5488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.3330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.8357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.8352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.8269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.5616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.6546, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(83.0274, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.4044, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.4726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.2621, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.8185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(142.9012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.4066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.3016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.6339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.2003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.2477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.1227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.3645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.4475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(130.7802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.1638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(163.6413, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.8663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(228.6809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.4490, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.9205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.3858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.9192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.5160, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5538793103448276\n",
            "Sentence level Krippendorff's alpha for Premises:  0.625\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 322\n",
            "\tPrediction sentences having premises: 359\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 60\n",
            "\tPrediction Sentence having neither claim nor premise: 307\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 721\n",
            "\tSentences having claim in only one of reference or prediction: 207\n",
            "\tSentences having premise in both reference and prediction: 754\n",
            "\tSentences having premise in only one of reference or prediction: 174\n",
            "------------EPOCH 19---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(61.1872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.9567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.6592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.0166, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.6282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9317, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.6203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.8078, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.6283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.0037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.6205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.0833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.7076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.7485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.1848, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.9003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.0595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3226, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.9584, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.3128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.0427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.0200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.7253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.2843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.3840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.1443, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.9772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.6837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.7827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(151.0127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(183.1501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.3285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.7660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.6142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1092, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5538793103448276\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6120689655172413\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 320\n",
            "\tPrediction sentences having premises: 369\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 67\n",
            "\tPrediction Sentence having neither claim nor premise: 306\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 721\n",
            "\tSentences having claim in only one of reference or prediction: 207\n",
            "\tSentences having premise in both reference and prediction: 748\n",
            "\tSentences having premise in only one of reference or prediction: 180\n",
            "------------EPOCH 20---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(50.5917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.0658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.4098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.6691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3148, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.6126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.9194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.3680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.9756, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.3710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2411, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.9448, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.9000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.7017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.1863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.3737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.9714, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.0108, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.3162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.3547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.8726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3107, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.7919, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.5669, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.1790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.5887, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.1397, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.1941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(133.6766, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(148.0344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.7036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.0776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.4858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(77.1261, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.2969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5431034482758621\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6099137931034483\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 325\n",
            "\tPrediction sentences having premises: 366\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 65\n",
            "\tPrediction Sentence having neither claim nor premise: 302\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 716\n",
            "\tSentences having claim in only one of reference or prediction: 212\n",
            "\tSentences having premise in both reference and prediction: 747\n",
            "\tSentences having premise in only one of reference or prediction: 181\n",
            "------------EPOCH 21---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(42.8525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.5465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.7119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.8763, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7901, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.6488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.8070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.0282, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.2872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.8988, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.2654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.1247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.0794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.0162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.8192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.3385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.9382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.5364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.5814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.2390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.3944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.0723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.5199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.4168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.8813, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.6246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(129.0469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.0408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.0661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.8476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.5953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9567, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5474137931034483\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6185344827586207\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 331\n",
            "\tPrediction sentences having premises: 362\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 299\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 718\n",
            "\tSentences having claim in only one of reference or prediction: 210\n",
            "\tSentences having premise in both reference and prediction: 751\n",
            "\tSentences having premise in only one of reference or prediction: 177\n",
            "------------EPOCH 22---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(37.4864, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4337, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.1042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5958, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7300, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8651, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.7807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.1961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2269, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.0495, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.5671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.3801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.0753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.0244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.0270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.4255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0703, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.8654, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.1246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.7742, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7719, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.2230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.7184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.6497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.1436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.9379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.3222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.5729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(116.7238, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.2147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.2774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.3434, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5919, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5344827586206897\n",
            "Sentence level Krippendorff's alpha for Premises:  0.625\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 335\n",
            "\tPrediction sentences having premises: 357\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 63\n",
            "\tPrediction Sentence having neither claim nor premise: 299\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 712\n",
            "\tSentences having claim in only one of reference or prediction: 216\n",
            "\tSentences having premise in both reference and prediction: 754\n",
            "\tSentences having premise in only one of reference or prediction: 174\n",
            "------------EPOCH 23---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(29.4840, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.4525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.2752, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6149, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.7741, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.8114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.6174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.3209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.1767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.7562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.3277, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.9042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1553, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.7834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.6651, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5874, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.0820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6710, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.7761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.6723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7444, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.5191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2555, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(104.1647, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7144, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.4243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.1354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.8191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.0589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5431034482758621\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6163793103448276\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 331\n",
            "\tPrediction sentences having premises: 363\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 65\n",
            "\tPrediction Sentence having neither claim nor premise: 299\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 716\n",
            "\tSentences having claim in only one of reference or prediction: 212\n",
            "\tSentences having premise in both reference and prediction: 750\n",
            "\tSentences having premise in only one of reference or prediction: 178\n",
            "------------EPOCH 24---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(24.6266, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.8580, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.8877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6788, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.7148, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.9969, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8013, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8781, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.2733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.6224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7920, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.4228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.2563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0836, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.1035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9161, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.0154, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6260, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.8415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.0232, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.7074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.7665, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(96.7258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.1327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.1704, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.540948275862069\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6185344827586207\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 332\n",
            "\tPrediction sentences having premises: 362\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 66\n",
            "\tPrediction Sentence having neither claim nor premise: 300\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 715\n",
            "\tSentences having claim in only one of reference or prediction: 213\n",
            "\tSentences having premise in both reference and prediction: 751\n",
            "\tSentences having premise in only one of reference or prediction: 177\n",
            "------------EPOCH 25---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(18.6882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.8530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.9364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.5630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.2534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7418, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6772, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.7695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.5039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.7025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9651, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.3017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1459, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.8047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8284, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0677, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.9087, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5058, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0930, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3421, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.8805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(117.6130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.1153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.7268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.6615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.1812, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5517241379310345\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6120689655172413\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 323\n",
            "\tPrediction sentences having premises: 359\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 61\n",
            "\tPrediction Sentence having neither claim nor premise: 307\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 720\n",
            "\tSentences having claim in only one of reference or prediction: 208\n",
            "\tSentences having premise in both reference and prediction: 748\n",
            "\tSentences having premise in only one of reference or prediction: 180\n",
            "------------EPOCH 26---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(25.9610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.1836, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.8065, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9864, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4885, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2605, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5846, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.5152, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2709, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5046, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.7630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.1620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.0802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1414, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.0588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.3237, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.2549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.6689, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3151, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.4258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5452586206896552\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6120689655172413\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 328\n",
            "\tPrediction sentences having premises: 367\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 67\n",
            "\tPrediction Sentence having neither claim nor premise: 300\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 717\n",
            "\tSentences having claim in only one of reference or prediction: 211\n",
            "\tSentences having premise in both reference and prediction: 748\n",
            "\tSentences having premise in only one of reference or prediction: 180\n",
            "------------EPOCH 27---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(13.9750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.8922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0336, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3492, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.4702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.6927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9596, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0134, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6997, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.1021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.3614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.5739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.5452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.7011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.5519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.9645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0417, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.6934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.2392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.5183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6603, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.5833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.3562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.6072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.540948275862069\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6120689655172413\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 330\n",
            "\tPrediction sentences having premises: 367\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 66\n",
            "\tPrediction Sentence having neither claim nor premise: 297\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 715\n",
            "\tSentences having claim in only one of reference or prediction: 213\n",
            "\tSentences having premise in both reference and prediction: 748\n",
            "\tSentences having premise in only one of reference or prediction: 180\n",
            "------------EPOCH 28---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(13.3053, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0722, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3543, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0605, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.0724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9685, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7447, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5503, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.7706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2862, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4168, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.3532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.4538, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.4236, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.1233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4309, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.3329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.4288, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.3532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5875, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.8302, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7727, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.3028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.6014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.8118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.5280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5431034482758621\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6142241379310345\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 331\n",
            "\tPrediction sentences having premises: 362\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 299\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 716\n",
            "\tSentences having claim in only one of reference or prediction: 212\n",
            "\tSentences having premise in both reference and prediction: 749\n",
            "\tSentences having premise in only one of reference or prediction: 179\n",
            "------------EPOCH 29---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(12.6372, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.6550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.3737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3344, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.8338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.4610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9599, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4109, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.8861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.0934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9755, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.0749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6758, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3591, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3259, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.1771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.4258, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4608, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.0296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.1265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.5762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1373, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.1753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(83.7946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.7568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.9281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3020, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.6974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.3396, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5517241379310345\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6142241379310345\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 327\n",
            "\tPrediction sentences having premises: 360\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 60\n",
            "\tPrediction Sentence having neither claim nor premise: 301\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 720\n",
            "\tSentences having claim in only one of reference or prediction: 208\n",
            "\tSentences having premise in both reference and prediction: 749\n",
            "\tSentences having premise in only one of reference or prediction: 179\n",
            "------------EPOCH 30---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(12.2119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3393, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.6694, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.8066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2466, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.8660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8473, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.7012, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2591, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.0867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.5761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4735, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0159, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.1631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3630, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1475, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.6449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2374, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5495689655172413\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6163793103448276\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 326\n",
            "\tPrediction sentences having premises: 359\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 61\n",
            "\tPrediction Sentence having neither claim nor premise: 304\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 719\n",
            "\tSentences having claim in only one of reference or prediction: 209\n",
            "\tSentences having premise in both reference and prediction: 750\n",
            "\tSentences having premise in only one of reference or prediction: 178\n",
            "------------EPOCH 31---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(15.7824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.6119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.9209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.3565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6358, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4696, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6514, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.4635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1162, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7970, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.9611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3273, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.6850, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.4995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5289, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6085, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.2323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7467, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.9319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.7718, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.2952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.7234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.1389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9470, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.4808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.2283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8075, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.3476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5560344827586207\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6077586206896552\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 327\n",
            "\tPrediction sentences having premises: 365\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 300\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 722\n",
            "\tSentences having claim in only one of reference or prediction: 206\n",
            "\tSentences having premise in both reference and prediction: 746\n",
            "\tSentences having premise in only one of reference or prediction: 182\n",
            "------------EPOCH 32---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(7.3436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9003, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8419, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3133, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.1863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0784, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.3329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3717, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3865, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.0952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.6170, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2170, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4333, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0549, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4331, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.1851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.3191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.6204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.2500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4893, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6686, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.8254, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5101, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(68.0485, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.1204, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8025, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5847, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.4111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5495689655172413\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6056034482758621\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 324\n",
            "\tPrediction sentences having premises: 366\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 60\n",
            "\tPrediction Sentence having neither claim nor premise: 298\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 719\n",
            "\tSentences having claim in only one of reference or prediction: 209\n",
            "\tSentences having premise in both reference and prediction: 745\n",
            "\tSentences having premise in only one of reference or prediction: 183\n",
            "------------EPOCH 33---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(8.7606, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.0077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.2244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.3984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.7691, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.7382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.0530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.2964, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2038, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1335, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.1472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2837, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6035, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.6365, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6072, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.5828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.4635, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.8233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6069, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2517, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.9894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.9153, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.7796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.1946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6375, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.1218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.6318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.4019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.5120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8708, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1148, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.6892, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.5913, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.9256, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5474137931034483\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6163793103448276\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 329\n",
            "\tPrediction sentences having premises: 363\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 65\n",
            "\tPrediction Sentence having neither claim nor premise: 301\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 718\n",
            "\tSentences having claim in only one of reference or prediction: 210\n",
            "\tSentences having premise in both reference and prediction: 750\n",
            "\tSentences having premise in only one of reference or prediction: 178\n",
            "------------EPOCH 34---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(6.0190, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.1663, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1577, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7922, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.7612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.6207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4.2147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2.9609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.5371, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.5468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.1361, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.3881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.4489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.3938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.0177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.0243, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.5660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.3687, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9826, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.4551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.2945, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.3422, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5833, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.7343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.2454, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.0174, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.7146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3251, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.6456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.2434, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.9508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.9802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9676, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.5558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.8408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5581896551724138\n",
            "Sentence level Krippendorff's alpha for Premises:  0.625\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 328\n",
            "\tPrediction sentences having premises: 363\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 64\n",
            "\tPrediction Sentence having neither claim nor premise: 301\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 723\n",
            "\tSentences having claim in only one of reference or prediction: 205\n",
            "\tSentences having premise in both reference and prediction: 754\n",
            "\tSentences having premise in only one of reference or prediction: 174\n",
            "------------EPOCH 35---------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Loss:  tensor(14.5241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.3200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.5737, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9918, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.4581, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3.0869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.5962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.7264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.8878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.6379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2364, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1704, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.8135, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.3114, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.5481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2148, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.3778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.6379, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.6968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.1082, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2697, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.0943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5.4205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.9587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8139, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.2489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.0660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.9566, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6.2578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.4715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.5914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4286, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.7246, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(7.9489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Sentence level Krippendorff's alpha for Claims:  0.5689655172413793\n",
            "Sentence level Krippendorff's alpha for Premises:  0.6228448275862069\n",
            "Additional attributes: \n",
            "\tTotal Sentences: 928\n",
            "\tPrediction setences having claims: 333\n",
            "\tPrediction sentences having premises: 368\n",
            "\tReference setences having claims: 255\n",
            "\tReference sentences having premises: 403\n",
            "\n",
            "\n",
            "\tPrediction Sentence having both claim and premise: 72\n",
            "\tPrediction Sentence having neither claim nor premise: 299\n",
            "\tReference Sentence having both claim and premise: 65\n",
            "\tReference Sentence having neither claim nor premise: 335\n",
            "\n",
            "\n",
            "\tSentences having claim in both reference and prediction: 728\n",
            "\tSentences having claim in only one of reference or prediction: 200\n",
            "\tSentences having premise in both reference and prediction: 753\n",
            "\tSentences having premise in only one of reference or prediction: 175\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mi7LRmfIYbR"
      },
      "source": [
        "### Rough -- Checking dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9kGw_svbXM",
        "outputId": "b2838b14-a51e-4129-bb8c-20b274bb7a1c"
      },
      "source": [
        "\" \".join(\" mY name is \".split())"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mY name is'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-oVkplUCJh"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qMGQ5GOzbn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57323397-ef83-477b-a59d-7405173bb62c"
      },
      "source": [
        "train_dataset, _, test_dataset = get_datasets()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'change-my-view-modes' already exists and is not an empty directory.\n",
            "2021-08-29 07:42:38.410841: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcusolver.so.11'; dlerror: libcusolver.so.11: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda-11.0/lib64:/usr/local/cuda-11.0/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
            "2021-08-29 07:42:38.411592: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1835] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
            "Skipping registering GPU devices...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdpyzvKIfM8",
        "outputId": "8531f009-4bed-4ea9-daa4-d76d023d52e9"
      },
      "source": [
        "for tokenized_threads, masked_threads, comp_type_labels, _ in test_dataset:\n",
        "    tokenized_threads, masked_threads, comp_type_labels = tokenized_threads[0], masked_threads[0], comp_type_labels[0]\n",
        "    for tokenized_thread, masked_thread, comp_type_label in zip(tokenized_threads, masked_threads, comp_type_labels):\n",
        "        print(comp_type_label[:100])\n",
        "        print(tokenized_thread[:100])\n",
        "        print(tokenizer.decode(tokenized_thread[:500]))\n",
        "        start, end = 0, 0\n",
        "        prev_type = \"other\"\n",
        "        i = 0\n",
        "        while i<tokenized_thread.shape[0]:\n",
        "            if comp_type_label[i]==ac_dict[\"O\"]:\n",
        "                if prev_type==\"other\":\n",
        "                    end += 1\n",
        "                else:\n",
        "                    print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                    print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                    start = i\n",
        "                    end = i\n",
        "                    prev_type=\"other\"\n",
        "                \n",
        "            if comp_type_label[i] in [ac_dict[\"B-C\"], ac_dict[\"B-P\"]]:\n",
        "                print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                start = i\n",
        "                end = i\n",
        "                prev_type = \"Claim\" if comp_type_label[i]==ac_dict[\"B-C\"] else \"Premise\"\n",
        "            \n",
        "            if comp_type_label[i] in [ac_dict[\"I-C\"], ac_dict[\"I-P\"]]:\n",
        "                end += 1\n",
        "            \n",
        "            i+=1\n",
        "        break\n",
        "    break"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
            " 2 2 2 2 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4]\n",
            "[    0 18814   846    35  7978     9  1901    16   145   551   350   444\n",
            " 50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268\n",
            "   100  1819   923    84   481  1901    53     7   162  1437  8585    16\n",
            "    10   699   516   227 20203   110    78  8322   235    36  1437    22\n",
            "   270  1284 29384   328]\n",
            "<s>CMV: Freedom of speech is being taken too far [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE] I certainly value our free speech but to me there is a clear line between exercising your first amendment right (  \" President Obama sucks! \" etc ) and doing things that are known to be offensive to other cultures (  Satirical cartoons of prophets, assassinating leaders, etc ). [NEWLINE] [NEWLINE]  Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE] Sure, but that doesn't mean we condone the bully's actions and don't punish the bullies for acting. [NEWLINE] We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments. [NEWLINE]  Complete freedom in the expression of any idea, offensive or not, is a major element of that world. [NEWLINE] [NEWLINE]  If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us. [NEWLINE] [NEWLINE] [USER0] [NEWLINE] I certainly agree with your points - I didn't mean to imply that I was only for * * some * * freedom of speech. [NEWLINE] I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset. [NEWLINE] [NEWLINE]  Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"  [NEWLINE] Is that so hard? [NEWLINE] [NEWLINE] </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Component:  <s>CM  of type:  other [    0 18814]\n",
            "Masked Component:  <s>CM  of type:  other [    0 18814]\n",
            "Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Masked Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Masked Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Masked Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Component:   but to me   of type:  other [  53    7  162 1437]\n",
            "Masked Component:  <mask> to me   of type:  other [50264     7   162  1437]\n",
            "Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Masked Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Masked Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Masked Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Component:   (   of type:  other [  36 1437]\n",
            "Masked Component:   (   of type:  other [  36 1437]\n",
            "Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Masked Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Masked Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Component:   Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111    53   114    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Masked Component:   Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111 50264 50264    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            "    53   114    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Masked Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            " 50264 50264    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Component:  Sure  of type:  Claim [32541]\n",
            "Masked Component:  Sure  of type:  Claim [32541]\n",
            "Component:  , but   of type:  other [   6   53 1437]\n",
            "Masked Component:  ,<mask>   of type:  other [    6 50264  1437]\n",
            "Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Masked Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Masked Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Masked Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [ 318   47  236    7 1744    5 3519    7 1994   13  143    9  201    6\n",
            "   47   33    7 1744    5 3519    7 1994   13   70    9  201]\n",
            "Masked Component:  <mask> you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [50264    47   236     7  1744     5  3519     7  1994    13   143     9\n",
            "   201     6    47    33     7  1744     5  3519     7  1994    13    70\n",
            "     9   201]\n",
            "Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Masked Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Component:   -   of type:  other [ 111 1437]\n",
            "Masked Component:   -   of type:  other [ 111 1437]\n",
            "Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Masked Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901    77  1402  1134\n",
            "   120  4904]\n",
            "Masked Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech<mask> certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901 50264  1402  1134\n",
            "   120  4904]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"   of type:  Premise [ 2612    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45   142\n",
            "    52   214  6023     9    47    53   142    52  2098   110  2728     4\n",
            "    22  1437]\n",
            "Masked Component:  <mask> can't America be the bigger person and say \" Ok, we won't publish certain types of material, not<mask> we're afraid of you<mask><mask> we respect your views. \"   of type:  Premise [50264    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45 50264\n",
            "    52   214  6023     9    47 50264 50264    52  2098   110  2728     4\n",
            "    22  1437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZnb1Nz-MX4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2601bebd-3989-40a5-af7d-91d578651823"
      },
      "source": [
        "import re\n",
        "re.sub(r\"\\s*</claim>([^\\s])\", r\"</claim> \\1\", \"<claim>my name is </claim>jeevesh.\")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<claim>my name is</claim> jeevesh.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UVJb5jy178"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}