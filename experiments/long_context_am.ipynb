{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "long_context_am.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "35kJb8EE0Fm-",
        "-Mi7LRmfIYbR"
      ],
      "authorship_tag": "ABX9TyPHQWg9CsLjHnC8T2VrNbcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jeevesh8/arg_mining/blob/main/experiments/long_context_am.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOGdooHmfgd-"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GPY8HXWtkyE"
      },
      "source": [
        "%%capture\n",
        "#if running on colab, install below 4\n",
        "#!git clone https://github.com/Jeevesh8/arg_mining\n",
        "#!pip install transformers\n",
        "#!pip install seqeval datasets allennlp\n",
        "#!pip install flax\n",
        "\n",
        "#if connected to local runtime, run the next command too\n",
        "#pip install bs4 tensorflow torch "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs8QzJCbx9lO"
      },
      "source": [
        "\n",
        "\n",
        "*   Update ``arg_mining/datasets/cmv_modes/configs.py`` as per your requirements, all experiments considered till now, set ``batch_size`` to 2, and all other variables with their default value.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_glbajGinKG4"
      },
      "source": [
        "#Run to ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8pCBwYZjfkHw"
      },
      "source": [
        "### Load Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkKUqJo4e_Rc"
      },
      "source": [
        "%%capture\n",
        "from datasets import load_metric\n",
        "metric = load_metric('seqeval')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35kJb8EE0Fm-"
      },
      "source": [
        "### Krippendorff's Alpha Metric"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ik1XOXUu0FOw"
      },
      "source": [
        "from typing import List, Tuple\n",
        "import re\n",
        "\n",
        "class krip_alpha():\n",
        "    \"\"\"A module for computing sentence level Krippendorff's Alpha,\n",
        "    for argumentative components  annotated at the token level. Must use\n",
        "    labels [\"B-C\", \"B-P\"].\n",
        "    \"\"\"\n",
        "    def __init__(self) -> None:\n",
        "        \"\"\"See self.compute_metric() for what each of these data actually mean.\n",
        "        \"\"\"\n",
        "        self.pred_has_claim = 0\n",
        "        self.ref_has_claim = 0\n",
        "        self.pred_has_premise = 0\n",
        "        self.ref_has_premise = 0\n",
        "        \n",
        "        self.claim_wise_agreement = 0\n",
        "        self.premise_wise_agreement = 0\n",
        "        \n",
        "        self.claim_wise_disagreement = 0\n",
        "        self.premise_wise_disagreement = 0\n",
        "    \n",
        "        self.total_sentences = 0\n",
        "        \n",
        "        self.has_both_ref = 0\n",
        "        self.has_both_pred = 0\n",
        "        self.has_none_ref = 0\n",
        "        self.has_none_pred = 0\n",
        "\n",
        "    def preprocess(self, threads: List[List[int]]) -> List[List[List[int]]]:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            threads:    A list of all threads in a batch. A thread is a list of \n",
        "                        integers corresponding to token_ids of the tokens in the \n",
        "                        thread.\n",
        "        Returns:\n",
        "            A List with all the threads, where each thread now consists of \n",
        "            sentence lists. Where, a sentence list in a thread list is the list \n",
        "            of token_ids corresponding to a sentence in a thread. \n",
        "        \"\"\"\n",
        "        threads_lis = []\n",
        "\n",
        "        for i, thread in enumerate(threads):\n",
        "            sentence = []\n",
        "            threads_lis.append([])\n",
        "            for j, token_id in enumerate(thread):\n",
        "                if token_id==tokenizer.pad_token_id:\n",
        "                    break\n",
        "                \n",
        "                sentence.append(token_id)\n",
        "                token = tokenizer.convert_ids_to_tokens(token_id)\n",
        "                #print(\"appended token:\", token)\n",
        "\n",
        "                next_token = 'None' if j==len(thread) else tokenizer.convert_ids_to_tokens(thread[j+1])\n",
        "\n",
        "                if (token.count('.')+token.count('?')+token.count('!')>=1 and \n",
        "                    next_token.count('.')+next_token.count('?')+next_token.count('!')==0):\n",
        "\n",
        "                    threads_lis[i].append(sentence)\n",
        "                    #print(\"Sample sentence: \", tokenizer.decode(sentence))\n",
        "                    sentence = []\n",
        "                \n",
        "                elif re.findall(r\"\\[USER\\d+\\]|\\[UNU\\]\", token)!=[]:\n",
        "                    prev_part = tokenizer.decode(sentence[:-1])[1:-1]\n",
        "                    if re.search(r'[a-zA-Z]', prev_part) is not None:\n",
        "                        threads_lis[i].append(sentence[:-1])\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(sentence[:-1]))\n",
        "                        sentence = [sentence[-1]]\n",
        "                    else:\n",
        "                        k=len(sentence)-2\n",
        "                        while k>=0 and sentence[k]==tokenizer.convert_tokens_to_ids('Ġ'):\n",
        "                            k-=1\n",
        "                        sentence = sentence[k+1:]\n",
        "                        threads_lis[i][-1] += sentence[:k]\n",
        "                        #print(\"Sample sentence just befor user token:\", tokenizer.decode(threads_lis[i][-1]))\n",
        "                \n",
        "            has_rem_token = False\n",
        "            for elem in sentence:\n",
        "                if (elem!=tokenizer.convert_tokens_to_ids('Ġ') and\n",
        "                    elem!=tokenizer.eos_token_id):\n",
        "                    has_rem_token = True\n",
        "                    break\n",
        "            \n",
        "            if has_rem_token:\n",
        "                threads_lis[i].append(sentence)\n",
        "                #print(\"Sample sentence at end of thread: \", tokenizer.decode(sentence))\n",
        "                sentence = []\n",
        "\n",
        "        return threads_lis\n",
        "\n",
        "    def get_sentence_wise_preds(self, threads: List[List[List[int]]], \n",
        "                                      predictions: List[List[str]]) -> List[List[List[str]]]:\n",
        "        \"\"\"Splits the prediction corresponding to each thread, into predictions\n",
        "        for each sentence in the corresponding thread in \"threads\" list.\n",
        "        Args:\n",
        "            threads:      A list of threads, where each thread consists of further \n",
        "                          lists corresponding to the various sentences in the\n",
        "                          thread. [As output by self.preprocess()]\n",
        "            predictions:  A list of predictions for each thread, in the threads\n",
        "                          list. Each prediciton consists of a list of componenet \n",
        "                          types corresponding to each token in a thread.\n",
        "        Returns:\n",
        "            The predictions list, with each prediction split into predictions \n",
        "            corresponding to the sentences in the corresponding thread specified\n",
        "            in the threads list. \n",
        "        \"\"\"\n",
        "        sentence_wise_preds = []\n",
        "        for i, thread in enumerate(threads):\n",
        "            next_sentence_beg = 0\n",
        "            sentence_wise_preds.append([])\n",
        "            for sentence in thread:\n",
        "                sentence_wise_preds[i].append(\n",
        "                    predictions[i][next_sentence_beg:next_sentence_beg+len(sentence)])\n",
        "                next_sentence_beg += len(sentence)\n",
        "        return sentence_wise_preds\n",
        "    \n",
        "    def update_state(self, pred_sentence: List[str], ref_sentence: List[str]) -> None:\n",
        "        \"\"\"Updates the various information maintained for the computation of\n",
        "        Krippendorff's alpha, based on the predictions(pred_sentence) and \n",
        "        references(ref_sentence) provided for a particular sentence, in some \n",
        "        thread.\n",
        "        \"\"\"\n",
        "        self.total_sentences += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence:\n",
        "            self.pred_has_claim += 1\n",
        "            if 'B-C' in ref_sentence:\n",
        "                self.ref_has_claim += 1\n",
        "                self.claim_wise_agreement += 1\n",
        "            else:\n",
        "                self.claim_wise_disagreement += 1\n",
        "            \n",
        "        elif 'B-C' in ref_sentence:\n",
        "            self.ref_has_claim += 1\n",
        "            self.claim_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.claim_wise_agreement += 1\n",
        "        \n",
        "        if 'B-P' in pred_sentence:\n",
        "            self.pred_has_premise += 1\n",
        "            if 'B-P' in ref_sentence:\n",
        "                self.ref_has_premise += 1\n",
        "                self.premise_wise_agreement += 1\n",
        "            else:\n",
        "                self.premise_wise_disagreement += 1\n",
        "\n",
        "        elif 'B-P' in ref_sentence:\n",
        "            self.ref_has_premise += 1\n",
        "            self.premise_wise_disagreement += 1\n",
        "        \n",
        "        else:\n",
        "            self.premise_wise_agreement += 1\n",
        "        \n",
        "        if 'B-C' in pred_sentence and 'B-P' in pred_sentence:\n",
        "            self.has_both_pred += 1\n",
        "        \n",
        "        if 'B-C' in ref_sentence and 'B-P' in ref_sentence:\n",
        "            self.has_both_ref += 1\n",
        "        \n",
        "        if 'B-C' not in pred_sentence and 'B-P' not in pred_sentence:\n",
        "            self.has_none_pred += 1\n",
        "        \n",
        "        if 'B-C' not in ref_sentence and 'B-P' not in ref_sentence:\n",
        "            self.has_none_ref += 1\n",
        "        return\n",
        "\n",
        "    def add_batch(self, predictions: List[List[str]], \n",
        "                  references: List[List[str]], \n",
        "                  tokenized_threads: List[List[int]]) -> None:\n",
        "        \"\"\"Add a batch of predictions and references for the computation of \n",
        "        Krippendorff's alpha.\n",
        "        Args:\n",
        "            predictions:      A list of predictions for each thread, in the \n",
        "                              threads list. Each prediciton consists of a list \n",
        "                              of component types corresponding to each token in \n",
        "                              a thread.\n",
        "            references:       Same structure as predictions, but consisting of \n",
        "                              acutal gold labels, instead of predicted ones.\n",
        "            tokenized_thread: A list of all threads in a batch. A thread is a \n",
        "                              list of integers corresponding to token_ids of the\n",
        "                              tokens in the thread.\n",
        "        \"\"\"\n",
        "        threads = self.preprocess(tokenized_threads)\n",
        "        \n",
        "        sentence_wise_preds = self.get_sentence_wise_preds(threads, predictions)\n",
        "        sentence_wise_refs = self.get_sentence_wise_preds(threads, references)\n",
        "\n",
        "        for pred_thread, ref_thread in zip(sentence_wise_preds, sentence_wise_refs):\n",
        "            for pred_sentence, ref_sentence in zip(pred_thread, ref_thread):\n",
        "                self.update_state(pred_sentence, ref_sentence)\n",
        "\n",
        "    def compute(self, print_additional: bool=True) -> None:\n",
        "        \"\"\"Prints out the metric, for the batched added till now. And then \n",
        "        resets all data being maintained by the metric. \n",
        "        Args:\n",
        "            print_additional:   If True, will print all the data being \n",
        "                                maintained instead of just the Krippendorff's \n",
        "                                alphas for claims and premises.\n",
        "        \"\"\"\n",
        "        print(\"Sentence level Krippendorff's alpha for Claims: \", 1-(self.claim_wise_disagreement/(self.claim_wise_agreement+self.claim_wise_disagreement))/0.5)\n",
        "        print(\"Sentence level Krippendorff's alpha for Premises: \", 1-(self.premise_wise_disagreement/(self.premise_wise_agreement+self.premise_wise_disagreement))/0.5)\n",
        "        \n",
        "        if print_additional:\n",
        "            print(\"Additional attributes: \")\n",
        "            print(\"\\tTotal Sentences:\", self.total_sentences)\n",
        "            print(\"\\tPrediction setences having claims:\", self.pred_has_claim)\n",
        "            print(\"\\tPrediction sentences having premises:\", self.pred_has_premise)\n",
        "            print(\"\\tReference setences having claims:\", self.ref_has_claim)\n",
        "            print(\"\\tReference sentences having premises:\", self.ref_has_premise)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tPrediction Sentence having both claim and premise:\", self.has_both_pred)\n",
        "            print(\"\\tPrediction Sentence having neither claim nor premise:\", self.has_none_pred)\n",
        "            print(\"\\tReference Sentence having both claim and premise:\", self.has_both_ref)\n",
        "            print(\"\\tReference Sentence having neither claim nor premise:\", self.has_none_ref)\n",
        "            print(\"\\n\")\n",
        "            print(\"\\tSentences having claim in both reference and prediction:\", self.claim_wise_agreement)\n",
        "            print(\"\\tSentences having claim in only one of reference or prediction:\", self.claim_wise_disagreement)\n",
        "            print(\"\\tSentences having premise in both reference and prediction:\", self.premise_wise_agreement)\n",
        "            print(\"\\tSentences having premise in only one of reference or prediction:\", self.premise_wise_disagreement)\n",
        "        self.__init__()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYRNv0wBWtFd"
      },
      "source": [
        "metric = krip_alpha()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjf6BxMkfm3R"
      },
      "source": [
        "### Define & Load Tokenizer, Model, Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6wcsqmllnfRB"
      },
      "source": [
        "import torch\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45DeXxCDS_id",
        "outputId": "02da6412-cc5e-4dff-c492-60cf4f4d9173"
      },
      "source": [
        "device"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v938tu5rydoT"
      },
      "source": [
        "#### Load Model/Tokenizer from HF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tHRkCQOZu6HS"
      },
      "source": [
        "model_version = 'allenai/longformer-base-4096'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cB-7M9t0HP"
      },
      "source": [
        "%%capture\n",
        "from transformers import LongformerTokenizer, AutoModel\n",
        "tokenizer = LongformerTokenizer.from_pretrained(model_version)\n",
        "transformer_model = AutoModel.from_pretrained(model_version).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0L4IWXMOymZk"
      },
      "source": [
        "#### Or load them from pretrained files..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNf-8LQVyliT",
        "outputId": "b7178e86-6229-425b-9813-75a956d0dc3e"
      },
      "source": [
        "from transformers import LongformerTokenizer, LongformerModel\n",
        "\n",
        "tokenizer = LongformerTokenizer.from_pretrained('./2epoch_complete/tokenizer/')\n",
        "transformer_model = LongformerModel.from_pretrained('./2epoch_complete/model/').to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at ./2epoch_complete/model/ were not used when initializing LongformerModel: ['lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing LongformerModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerModel were not initialized from the model checkpoint at ./2epoch_complete/model/ and are newly initialized: ['longformer.pooler.dense.weight', 'longformer.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEV4yTy11zUm"
      },
      "source": [
        "import torch.nn as nn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP4PwtV9zai9"
      },
      "source": [
        "#### To add extra token type embeddings..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ajTzrbzkwbT"
      },
      "source": [
        "def resize_token_type_embeddings(transformer_model, new_size):\n",
        "    old_embeddings = transformer_model.embeddings.token_type_embeddings.weight\n",
        "    old_size, hidden_dim = old_embeddings.shape\n",
        "    transformer_model.embeddings.token_type_embeddings = nn.Embedding(new_size, hidden_dim, device=transformer_model.device)\n",
        "    with torch.no_grad():\n",
        "        transformer_model.embeddings.token_type_embeddings.weight[:old_size] = old_embeddings\n",
        "\n",
        "resize_token_type_embeddings(transformer_model, 2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeHJiT0sjXid"
      },
      "source": [
        "transformer_model.config.type_vocab_size = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRZ89akNzoMo"
      },
      "source": [
        "#### Load in discourse markers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3p6dGA83cGVS"
      },
      "source": [
        "with open('./Discourse_Markers.txt') as f:\n",
        "    discourse_markers = [dm.strip() for dm in f.readlines()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6bQWcCd1p1G"
      },
      "source": [
        "%%capture\n",
        "from arg_mining.datasets.cmv_modes import load_dataset, data_config"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5O1Wm7Dzqqe"
      },
      "source": [
        "#### Add special tokens to tokenizer and model vocab, if not already there"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvkOZsiCzl1k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5f97bb8-47df-4a97-ce8a-3aa10ed8cfae"
      },
      "source": [
        "tokenizer.add_tokens(data_config[\"special_tokens\"])\n",
        "\n",
        "transformer_model.resize_token_embeddings(len(tokenizer))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Embedding(50280, 768)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nh32PtIEz1mF"
      },
      "source": [
        "#### Function to get train, test data (50/50 split currently)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTCeCgbLxVyQ"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=50,\n",
        "                                                              test_sz=50,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yi_8gVbufzoX"
      },
      "source": [
        "### Define layers for a Linear-Chain-CRF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "seBwkZdByesM"
      },
      "source": [
        "from allennlp.modules.conditional_random_field import ConditionalRandomField as crf\n",
        "\n",
        "ac_dict = data_config[\"arg_components\"]\n",
        "\n",
        "allowed_transitions =([(ac_dict[\"B-C\"], ac_dict[\"I-C\"]), \n",
        "                       (ac_dict[\"B-P\"], ac_dict[\"I-P\"])] + \n",
        "                      [(ac_dict[\"I-C\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-C\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"I-P\"], ac_dict[ct]) \n",
        "                        for ct in [\"I-P\", \"B-C\", \"B-P\", \"O\"]] +\n",
        "                      [(ac_dict[\"O\"], ac_dict[ct]) \n",
        "                        for ct in [\"O\", \"B-C\", \"B-P\"]])\n",
        "                    \n",
        "linear_layer = nn.Linear(transformer_model.config.hidden_size,\n",
        "                         len(ac_dict)).to(device)\n",
        "\n",
        "crf_layer = crf(num_tags=len(ac_dict),\n",
        "                constraints=allowed_transitions,\n",
        "                include_start_end_transitions=False).to(device)\n",
        "\n",
        "cross_entropy_layer = nn.CrossEntropyLoss(weight=torch.log(torch.tensor([3.3102, 61.4809, 3.6832, 49.6827, 2.5639], \n",
        "                                                                        device=device)), reduction='none')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iUicsK33f9d7"
      },
      "source": [
        "### Global Attention Mask Utility for Longformer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zOSo7p2I5mOi"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def get_global_attention_mask(tokenized_threads: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Returns an attention mask, with 1 where there are [USER{i}] tokens and \n",
        "    0 elsewhere.\n",
        "    \"\"\"\n",
        "    mask = np.zeros_like(tokenized_threads)\n",
        "    for user_token in [\"UNU\"]+[f\"[USER{i}]\" for i in range(data_config[\"max_users\"])]:\n",
        "        user_token_id = tokenizer.encode(user_token)[1:-1]\n",
        "        mask = np.where(tokenized_threads==user_token_id, 1, mask)\n",
        "    return np.array(mask, dtype=bool)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bp4PLQihf5CT"
      },
      "source": [
        "### Loss and Prediction Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IL3u8eH1rjZe"
      },
      "source": [
        "from typing import Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuJ5aryW9tUC"
      },
      "source": [
        "def compute(batch: Tuple[torch.Tensor, torch.Tensor, torch.Tensor],\n",
        "            preds: bool=False, cross_entropy: bool=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        batch:  A tuple having tokenized thread of shape [batch_size, seq_len],\n",
        "                component type labels of shape [batch_size, seq_len], and a global\n",
        "                attention mask for Longformer, of the same shape.\n",
        "        \n",
        "        preds:  If True, returns a List(of batch_size size) of Tuples of form \n",
        "                (tag_sequence, viterbi_score) where the tag_sequence is the \n",
        "                viterbi-decoded sequence, for the corresponding sample in the batch.\n",
        "        \n",
        "        cross_entropy:  This argument will only be used if preds=False, i.e., if \n",
        "                        loss is being calculated. If True, then cross entropy loss\n",
        "                        will also be added to the output loss.\n",
        "    \n",
        "    Returns:\n",
        "        Either the predicted sequences with their scores for each element in the batch\n",
        "        (if preds is True), or the loss value summed over all elements of the batch\n",
        "        (if preds is False).\n",
        "    \"\"\"\n",
        "    tokenized_threads, token_type_ids, comp_type_labels, global_attention_mask = batch\n",
        "    \n",
        "    pad_mask = torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0)\n",
        "    \n",
        "    logits = linear_layer(transformer_model(input_ids=tokenized_threads,\n",
        "                                            attention_mask=pad_mask,\n",
        "                                            global_attention_mask=global_attention_mask).last_hidden_state)\n",
        "    \n",
        "    if preds:\n",
        "        return crf_layer.viterbi_tags(logits, pad_mask)\n",
        "    \n",
        "    log_likelihood = crf_layer(logits, comp_type_labels, pad_mask)\n",
        "    \n",
        "    if cross_entropy:\n",
        "        logits = logits.reshape(-1, logits.shape[-1])\n",
        "        \n",
        "        pad_mask, comp_type_labels = pad_mask.reshape(-1), comp_type_labels.reshape(-1)\n",
        "        \n",
        "        ce_loss = torch.sum(pad_mask*cross_entropy_layer(logits, comp_type_labels))\n",
        "        \n",
        "        return ce_loss - log_likelihood\n",
        "\n",
        "    return -log_likelihood"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ot6lkPCsgEY4"
      },
      "source": [
        "### Define optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-yfpzEMBGra"
      },
      "source": [
        "from itertools import chain\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(params = chain(transformer_model.parameters(),\n",
        "                                      linear_layer.parameters(),\n",
        "                                      crf_layer.parameters()),\n",
        "                       lr = 2e-5,)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdSWnkO8gLD6"
      },
      "source": [
        "### Training And Evaluation Loops"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jTNaP3kLaN2X"
      },
      "source": [
        "def train(dataset):\n",
        "    accumulate_over = 4\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    for i, (tokenized_threads, masked_threads, comp_type_labels, _ ) in enumerate(dataset):\n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads),\n",
        "                                             device=device, dtype=torch.int32)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0), \n",
        "                                         device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0), \n",
        "                                      device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0), \n",
        "                                        device=device, dtype=torch.long)\n",
        "        \n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        loss = compute((tokenized_threads,\n",
        "                        torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                        comp_type_labels, \n",
        "                        global_attention_mask))/data_config[\"batch_size\"]\n",
        "        \n",
        "        print(\"Loss: \", loss)\n",
        "        loss.backward()\n",
        "        \n",
        "        if i%accumulate_over==accumulate_over-1:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7BpSI83cU24"
      },
      "source": [
        "def evaluate(dataset, metric):\n",
        "    \n",
        "    int_to_labels = {v:k for k, v in ac_dict.items()}\n",
        "    \n",
        "    for tokenized_threads, masked_threads, comp_type_labels, _ in dataset:\n",
        "        \n",
        "        global_attention_mask = torch.tensor(get_global_attention_mask(tokenized_threads), \n",
        "                                             device=device)\n",
        "        \n",
        "        #Remove Device Axis and cast to PyTorch tensor\n",
        "        tokenized_threads = torch.tensor(np.squeeze(tokenized_threads, axis=0),\n",
        "                                        device=device)\n",
        "        masked_threads = torch.tensor(np.squeeze(masked_threads, axis=0),\n",
        "                                     device=device)\n",
        "        comp_type_labels = torch.tensor(np.squeeze(comp_type_labels, axis=0),\n",
        "                                        device=device)\n",
        "        global_attention_mask = torch.squeeze(global_attention_mask, dim=0)\n",
        "        \n",
        "        preds = compute((tokenized_threads,\n",
        "                         torch.where(masked_threads==tokenizer.mask_token_id, 1, 0), \n",
        "                         comp_type_labels,\n",
        "                         global_attention_mask),\n",
        "                        preds=True)\n",
        "        \n",
        "        lengths = torch.sum(torch.where(tokenized_threads!=tokenizer.pad_token_id, 1, 0), \n",
        "                            axis=-1)\n",
        "        \n",
        "        preds = [ [int_to_labels[pred] for pred in pred[0][:lengths[i]]]\n",
        "                  for i, pred in enumerate(preds)\n",
        "                ]\n",
        "        \n",
        "        refs = [ [int_to_labels[ref] for ref in labels[:lengths[i]]]\n",
        "                 for i, labels in enumerate(comp_type_labels.cpu().tolist())\n",
        "               ]\n",
        "        \n",
        "        metric.add_batch(predictions=preds, \n",
        "                         references=refs,)\n",
        "                         #tokenized_threads=tokenized_threads.cpu().tolist())\n",
        "    \n",
        "    print(metric.compute())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZV6rnIQgOYA"
      },
      "source": [
        "### Final Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0O2O5qCufGwA"
      },
      "source": [
        "n_epochs = 35"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30xcK9sOgX44",
        "outputId": "c1fc1a1c-6368-45ae-9f40-58c5e313fca9"
      },
      "source": [
        "for epoch in range(n_epochs):\n",
        "    print(f\"------------EPOCH {epoch+1}---------------\")\n",
        "    train_dataset, _, test_dataset = get_datasets()\n",
        "    train(train_dataset)\n",
        "    evaluate(test_dataset, metric)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------EPOCH 1---------------\n",
            "Loss:  tensor(3133.3960, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2263.3823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2053.5234, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2630.0054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3522.3733, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2686.1143, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3312.4844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3093.7793, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2862.2871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2841.2378, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2574.4023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2888.2661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3627.6550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2377.2776, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2978.6338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(909.5702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1156.0881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1936.9268, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1109.9921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1888.7828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2252.1562, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1852.0356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2761.9751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(6554.6602, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3923.2786, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1326.9871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1268.2356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2027.6323, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2973.5498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.04227212681638045, 'recall': 0.0427807486631016, 'f1': 0.04252491694352159, 'number': 748}, 'P': {'precision': 0.0533596837944664, 'recall': 0.052123552123552123, 'f1': 0.05273437499999999, 'number': 1036}, 'overall_precision': 0.04861503674392312, 'overall_recall': 0.04820627802690583, 'overall_f1': 0.048409794539825496, 'overall_accuracy': 0.5010660980810234}\n",
            "------------EPOCH 2---------------\n",
            "Loss:  tensor(2168.9014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1697.8115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1570.4128, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1956.6561, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2574.1807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1969.6017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2453.8645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2459.7539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2229.7915, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2269.1582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2072.5427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2328.6855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2926.0198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1825.3198, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2257.7017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(769.5688, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(976.5343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1643.5424, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(954.8472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1581.6404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1937.9954, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1453.5582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2285.5420, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5433.4438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3236.6230, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1159.5155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1096.2883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1706.7832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2571.8008, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.04642409033877039, 'recall': 0.09893048128342247, 'f1': 0.0631938514090521, 'number': 748}, 'P': {'precision': 0.09309309309309309, 'recall': 0.029922779922779922, 'f1': 0.045288531775018265, 'number': 1036}, 'overall_precision': 0.054488842760768035, 'overall_recall': 0.05885650224215247, 'overall_f1': 0.056588520614389654, 'overall_accuracy': 0.45797542897756116}\n",
            "------------EPOCH 3---------------\n",
            "Loss:  tensor(1861.2910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1526.7583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1442.6299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1819.3597, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2505.0002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1881.0007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2321.3889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2362.8352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1896.9744, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2033.9037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1753.0524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1945.9583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2607.8203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1569.9431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1885.0610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(697.9023, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(889.8855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1592.9536, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(907.8386, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1441.8857, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1879.3857, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1340.2209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2244.8672, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(5196.2979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3223.8501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1149.5615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1071.3105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1641.4368, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2481.0806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.21792392005157962, 'recall': 0.45187165775401067, 'f1': 0.29404088734232275, 'number': 748}, 'P': {'precision': 0.40625, 'recall': 0.25096525096525096, 'f1': 0.31026252983293556, 'number': 1036}, 'overall_precision': 0.27293473299863075, 'overall_recall': 0.3352017937219731, 'overall_f1': 0.3008805031446541, 'overall_accuracy': 0.4968423190171591}\n",
            "------------EPOCH 4---------------\n",
            "Loss:  tensor(1655.2423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1453.7175, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1328.4091, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1709.5100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1949.7896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1570.2349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2044.7878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1927.6831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1783.0746, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1832.2026, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1621.2076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1807.5844, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2484.8760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1641.3519, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1827.4272, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(729.4172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(865.2678, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1442.2520, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(785.7646, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1183.0645, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1631.2861, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1205.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1859.0754, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(4635.7891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2614.5544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1009.9185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(946.6257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1468.1406, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2153.3848, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.24015748031496062, 'recall': 0.4077540106951872, 'f1': 0.3022794846382557, 'number': 748}, 'P': {'precision': 0.3037608486017358, 'recall': 0.30405405405405406, 'f1': 0.3039073806078148, 'number': 1036}, 'overall_precision': 0.26874729085392285, 'overall_recall': 0.3475336322869955, 'overall_f1': 0.30310437545832314, 'overall_accuracy': 0.5470403086607778}\n",
            "------------EPOCH 5---------------\n",
            "Loss:  tensor(1416.3569, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1202.5294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1063.9385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1408.7830, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1691.8748, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1410.3092, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1943.3563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1818.4016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1586.9996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1671.5183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1360.2324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1647.9834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2159.6394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1344.4214, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1535.1931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(618.1064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(723.9438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1287.7000, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(678.2775, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1036.2799, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1424.3250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(944.0559, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1646.5619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3909.1782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2112.7886, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(917.8820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(840.2329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1298.9880, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1867.8712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.1390168970814132, 'recall': 0.24197860962566844, 'f1': 0.17658536585365853, 'number': 748}, 'P': {'precision': 0.15554026152787337, 'recall': 0.21814671814671815, 'f1': 0.18159903575733227, 'number': 1036}, 'overall_precision': 0.14773139745916516, 'overall_recall': 0.2281390134529148, 'overall_f1': 0.17933465521039876, 'overall_accuracy': 0.6023149558330795}\n",
            "------------EPOCH 6---------------\n",
            "Loss:  tensor(1077.0343, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(840.6055, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(731.4926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1051.8923, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1415.9363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1153.2296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1604.1904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1610.7751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1371.3884, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1468.9509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1126.8972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1285.3790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1830.0491, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1158.6329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1295.9587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(556.6515, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(611.3702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1183.2192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(576.7147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(844.8881, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1158.9996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(761.7866, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1536.6106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3233.3120, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1569.9351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(773.5100, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(687.1002, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1086.2471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1575.4265, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.17083692838654013, 'recall': 0.2647058823529412, 'f1': 0.2076560041950708, 'number': 748}, 'P': {'precision': 0.18745644599303135, 'recall': 0.25965250965250963, 'f1': 0.2177256171590449, 'number': 1036}, 'overall_precision': 0.1800308404009252, 'overall_recall': 0.2617713004484305, 'overall_f1': 0.21333942439470077, 'overall_accuracy': 0.6325718347040309}\n",
            "------------EPOCH 7---------------\n",
            "Loss:  tensor(881.1471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(571.2297, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(502.5132, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(757.8911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1258.3933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(963.2832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1425.7164, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1389.4430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1117.9968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1248.6560, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(918.8557, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1017.1868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1439.2615, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(903.8609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1113.7032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(459.7931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(506.4021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1025.1229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(499.3768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(669.7839, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(900.9098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(638.8565, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1370.3534, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2549.4985, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1072.8872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(555.9680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(468.8962, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(820.0712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1129.5891, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.32160804020100503, 'recall': 0.42780748663101603, 'f1': 0.36718301778542745, 'number': 748}, 'P': {'precision': 0.4923224568138196, 'recall': 0.4951737451737452, 'f1': 0.49374398460057745, 'number': 1036}, 'overall_precision': 0.40893470790378006, 'overall_recall': 0.4669282511210762, 'overall_f1': 0.4360115153101282, 'overall_accuracy': 0.6612854096862626}\n",
            "------------EPOCH 8---------------\n",
            "Loss:  tensor(606.0383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(396.3486, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(353.3338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(526.8547, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1002.6518, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(649.2563, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1094.4076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1110.3431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(863.0522, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1004.4233, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(692.0711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(714.6283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1076.7141, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(643.8715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(803.8883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(335.7291, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(399.2805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(721.0080, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(345.7014, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(494.9548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(680.1326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(523.7949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1118.6702, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(2023.3264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(724.1572, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(405.4582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(325.2823, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(680.1728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(896.4855, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.41903171953255425, 'recall': 0.3355614973262032, 'f1': 0.3726800296956199, 'number': 748}, 'P': {'precision': 0.5013736263736264, 'recall': 0.7046332046332047, 'f1': 0.5858747993579454, 'number': 1036}, 'overall_precision': 0.47737226277372263, 'overall_recall': 0.5498878923766816, 'overall_f1': 0.5110705912998177, 'overall_accuracy': 0.687054523301858}\n",
            "------------EPOCH 9---------------\n",
            "Loss:  tensor(466.4465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(379.3619, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(272.0320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(460.5723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(831.6931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(369.4779, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(770.4531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(870.5574, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(800.6047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(860.7698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(569.0052, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(705.7086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1025.6455, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(580.7257, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(770.8383, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(293.3328, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(355.0950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(556.6750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.7661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(448.8472, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(604.7223, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(379.4662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(841.5356, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1718.1898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(457.3872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(280.3477, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(229.0624, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(585.4255, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1033.9938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3353159851301115, 'recall': 0.6029411764705882, 'f1': 0.43096034400382227, 'number': 748}, 'P': {'precision': 0.5292338709677419, 'recall': 0.5067567567567568, 'f1': 0.5177514792899408, 'number': 1036}, 'overall_precision': 0.41762943945228925, 'overall_recall': 0.547085201793722, 'overall_f1': 0.4736714389711235, 'overall_accuracy': 0.5978881104680678}\n",
            "------------EPOCH 10---------------\n",
            "Loss:  tensor(659.1189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(383.3036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.8898, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(571.5183, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(812.3530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(598.3828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1492.7590, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1291.1779, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(789.3438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(761.6281, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(598.2465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(765.0845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(779.9177, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(452.7592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(663.1937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.6314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(419.1027, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(813.5531, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(336.7189, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(501.1863, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1108.0310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(612.2526, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(928.9889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(3937.8594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(637.1486, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(223.8452, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.1968, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(494.4029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(576.6310, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3645364536453645, 'recall': 0.5414438502673797, 'f1': 0.43571812802582033, 'number': 748}, 'P': {'precision': 0.49493243243243246, 'recall': 0.5656370656370656, 'f1': 0.5279279279279279, 'number': 1036}, 'overall_precision': 0.4318082788671024, 'overall_recall': 0.5554932735426009, 'overall_f1': 0.4859034076979652, 'overall_accuracy': 0.6108640471113819}\n",
            "------------EPOCH 11---------------\n",
            "Loss:  tensor(527.0022, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(354.7739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(306.1097, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(545.7479, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(863.7637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(563.5950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1405.0497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1217.1809, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(905.9341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(887.8445, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(878.7173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1275.5129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1272.1932, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(591.5078, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(888.8734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(315.7906, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(329.2262, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(651.8203, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(360.0084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(426.9947, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(559.9937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(330.4404, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(811.2425, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1782.8267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(460.1925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.0548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(213.9197, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(464.4094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(707.4814, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3813868613138686, 'recall': 0.27941176470588236, 'f1': 0.3225308641975309, 'number': 748}, 'P': {'precision': 0.46894904458598724, 'recall': 0.5685328185328186, 'f1': 0.5139616055846422, 'number': 1036}, 'overall_precision': 0.442350332594235, 'overall_recall': 0.44730941704035876, 'overall_f1': 0.44481605351170567, 'overall_accuracy': 0.6895116255457406}\n",
            "------------EPOCH 12---------------\n",
            "Loss:  tensor(405.2673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(288.5049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(278.3734, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(494.8944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(969.3146, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(531.3638, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(875.2604, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1219.2751, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(550.6952, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(598.1974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(503.2242, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(562.0279, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(597.7995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(420.3054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(441.5673, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(250.2511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(272.5728, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(501.2415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(281.9092, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(290.1241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.9807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(359.8889, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(686.7488, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1255.1548, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(282.3974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(147.3468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(141.3349, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(395.4637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(343.0245, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.43982808022922637, 'recall': 0.410427807486631, 'f1': 0.42461964038727523, 'number': 748}, 'P': {'precision': 0.5110957004160888, 'recall': 0.7113899613899614, 'f1': 0.5948345439870864, 'number': 1036}, 'overall_precision': 0.48785046728971965, 'overall_recall': 0.5852017937219731, 'overall_f1': 0.5321100917431193, 'overall_accuracy': 0.679013097776424}\n",
            "------------EPOCH 13---------------\n",
            "Loss:  tensor(195.3157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(171.9977, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(166.7782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(285.2653, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(677.8527, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(257.7130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(718.1276, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(834.2408, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(564.4851, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(521.2900, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(326.7299, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(519.3339, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(545.0812, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(337.4829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(408.6729, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(151.2726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(160.0978, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(250.7951, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(166.5317, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(186.9127, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.2173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(210.0316, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(471.5594, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(695.8068, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(268.1147, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.9296, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.7441, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(383.1794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(512.9551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3554933519944017, 'recall': 0.679144385026738, 'f1': 0.46669728984841524, 'number': 748}, 'P': {'precision': 0.6048265460030166, 'recall': 0.38706563706563707, 'f1': 0.4720423778693349, 'number': 1036}, 'overall_precision': 0.4345124282982792, 'overall_recall': 0.5095291479820628, 'overall_f1': 0.4690402476780186, 'overall_accuracy': 0.611006193522185}\n",
            "------------EPOCH 14---------------\n",
            "Loss:  tensor(386.2994, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(150.3032, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(249.1403, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(277.2745, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(878.0637, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.5668, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(664.9467, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(537.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(258.0901, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(251.3123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(150.9644, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(255.7415, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(443.2628, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(244.7798, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(352.4034, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(126.5805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(287.9209, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(451.3524, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(402.6923, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(366.7314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(414.7076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.1657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(600.1489, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1442.2797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(215.9382, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.9043, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(98.6794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(272.2509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(181.4610, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3219814241486068, 'recall': 0.6951871657754011, 'f1': 0.44011849344054166, 'number': 748}, 'P': {'precision': 0.6190476190476191, 'recall': 0.32625482625482627, 'f1': 0.427307206068268, 'number': 1036}, 'overall_precision': 0.39703840814437763, 'overall_recall': 0.48094170403587444, 'overall_f1': 0.43498098859315587, 'overall_accuracy': 0.5904761904761905}\n",
            "------------EPOCH 15---------------\n",
            "Loss:  tensor(187.4316, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.2512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(128.2387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(191.0790, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(883.0217, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(443.5407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(1111.7136, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(832.6221, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(579.2313, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(703.7525, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(452.6056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(742.8004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(510.9991, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(299.7135, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(362.4109, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(174.1589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(108.7724, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(134.5902, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.8769, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(117.4650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(218.5227, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.2252, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(284.6808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(546.9186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(308.3743, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(152.0438, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.2595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(372.5048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(472.6888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.5201072386058981, 'recall': 0.25935828877005346, 'f1': 0.3461195361284567, 'number': 748}, 'P': {'precision': 0.496309963099631, 'recall': 0.778957528957529, 'f1': 0.6063110443275732, 'number': 1036}, 'overall_precision': 0.5007503751875938, 'overall_recall': 0.5610986547085202, 'overall_f1': 0.529209621993127, 'overall_accuracy': 0.7014925373134329}\n",
            "------------EPOCH 16---------------\n",
            "Loss:  tensor(200.1999, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(332.6157, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(257.5363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(355.9835, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(817.3849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(239.9725, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(532.4911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(751.3137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(530.7571, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(331.3582, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(218.9329, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(303.7104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(220.2402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(161.5426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(214.1639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(100.5806, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.4819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(113.1007, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.5011, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(107.2042, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(213.8858, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(104.6330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(251.4909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(523.1340, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(153.6632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.9423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.9070, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(217.6530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(230.8094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.35852981969486825, 'recall': 0.6911764705882353, 'f1': 0.47214611872146117, 'number': 748}, 'P': {'precision': 0.6185567010309279, 'recall': 0.3474903474903475, 'f1': 0.4449938195302844, 'number': 1036}, 'overall_precision': 0.433300395256917, 'overall_recall': 0.4915919282511211, 'overall_f1': 0.460609243697479, 'overall_accuracy': 0.6145598537922632}\n",
            "------------EPOCH 17---------------\n",
            "Loss:  tensor(132.4066, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.2726, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.3106, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.9655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(325.8445, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.8158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(307.5767, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(260.7993, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(240.1436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(248.2332, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(106.1130, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(242.5760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(281.9426, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(237.1037, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(281.6321, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(148.3314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.3842, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(169.7141, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(91.1327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(131.0636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(168.7283, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.3812, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(216.6312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(396.2200, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(120.3639, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.9229, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.8934, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(93.6927, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.2118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.41797752808988764, 'recall': 0.49732620320855614, 'f1': 0.4542124542124542, 'number': 748}, 'P': {'precision': 0.5347280334728034, 'recall': 0.6167953667953668, 'f1': 0.5728372926938593, 'number': 1036}, 'overall_precision': 0.4848920863309353, 'overall_recall': 0.5667040358744395, 'overall_f1': 0.5226156629620058, 'overall_accuracy': 0.6927200731038684}\n",
            "------------EPOCH 18---------------\n",
            "Loss:  tensor(51.4401, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(84.9974, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.5928, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.4481, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(382.2015, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(181.0914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(273.5657, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.2517, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(99.3402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.5749, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(175.6228, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(167.9877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(112.2318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(162.9980, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.1802, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(54.3992, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(85.2705, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(102.5794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.1716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(118.9506, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.5877, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(188.0656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(319.8894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(104.5878, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.5471, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.3450, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(78.5931, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.6623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.42604298356510745, 'recall': 0.4505347593582888, 'f1': 0.437946718648473, 'number': 748}, 'P': {'precision': 0.5331230283911672, 'recall': 0.6525096525096525, 'f1': 0.5868055555555556, 'number': 1036}, 'overall_precision': 0.4919864011656144, 'overall_recall': 0.5678251121076233, 'overall_f1': 0.5271922976841009, 'overall_accuracy': 0.6981013300842726}\n",
            "------------EPOCH 19---------------\n",
            "Loss:  tensor(39.9280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.4760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.1123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.3570, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(263.2956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.2327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(122.5062, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(182.7345, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(127.2732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.9760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.1029, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.3911, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.6661, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.7828, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(106.8402, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.2241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.6838, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.9017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4903, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.1051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.9795, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.6140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(136.7449, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(183.6499, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.1895, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.2778, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.6019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.1796, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.8939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3882149046793761, 'recall': 0.5989304812834224, 'f1': 0.47108307045215564, 'number': 748}, 'P': {'precision': 0.5846817691477886, 'recall': 0.5231660231660231, 'f1': 0.5522159959246051, 'number': 1036}, 'overall_precision': 0.4757328207592504, 'overall_recall': 0.554932735426009, 'overall_f1': 0.5122897800776197, 'overall_accuracy': 0.6668088130774698}\n",
            "------------EPOCH 20---------------\n",
            "Loss:  tensor(38.4730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.2681, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.8585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(76.5294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(163.3655, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.3815, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(90.6650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(116.3111, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(110.7451, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.2692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.2074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(95.5142, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(86.2819, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(106.8950, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.1095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7270, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.2463, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.5587, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.4059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.6544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.5508, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(123.5614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(159.8105, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(73.1824, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7527, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5810, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.0648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.2074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39388489208633093, 'recall': 0.5855614973262032, 'f1': 0.4709677419354839, 'number': 748}, 'P': {'precision': 0.5695159629248198, 'recall': 0.5337837837837838, 'f1': 0.5510712506228201, 'number': 1036}, 'overall_precision': 0.4757561209793567, 'overall_recall': 0.5554932735426009, 'overall_f1': 0.5125420222394621, 'overall_accuracy': 0.6686770230480252}\n",
            "------------EPOCH 21---------------\n",
            "Loss:  tensor(32.5711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.9327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.8617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(69.1016, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(124.1305, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.7805, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(62.8867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.8512, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.5370, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.2330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.2140, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.8699, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.2914, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.8580, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(79.2099, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.3294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.4492, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.5314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3493, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.6410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.1656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.4199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(83.0712, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(125.3896, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(61.8617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.8937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.7971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.8841, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.2723, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39503619441571874, 'recall': 0.5106951871657754, 'f1': 0.4454810495626822, 'number': 748}, 'P': {'precision': 0.5383244206773619, 'recall': 0.583011583011583, 'f1': 0.5597775718257646, 'number': 1036}, 'overall_precision': 0.4719961704164672, 'overall_recall': 0.5526905829596412, 'overall_f1': 0.5091660211722179, 'overall_accuracy': 0.6806985480759468}\n",
            "------------EPOCH 22---------------\n",
            "Loss:  tensor(23.3387, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.7363, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.3537, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.7351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(94.6280, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.1706, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.2390, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.5431, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(67.0315, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7940, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.8568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(70.2558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.8119, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(72.9871, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.3224, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.8849, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.1787, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2642, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.3660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.6040, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.5462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.2834, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(109.3285, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.0194, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9711, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1692, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.4901, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3970893970893971, 'recall': 0.5106951871657754, 'f1': 0.44678362573099417, 'number': 748}, 'P': {'precision': 0.5487588652482269, 'recall': 0.5974903474903475, 'f1': 0.5720887245841034, 'number': 1036}, 'overall_precision': 0.4789473684210526, 'overall_recall': 0.5610986547085202, 'overall_f1': 0.5167785234899329, 'overall_accuracy': 0.6808813077469794}\n",
            "------------EPOCH 23---------------\n",
            "Loss:  tensor(21.9079, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.3848, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.7330, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.9612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.4312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.2683, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(57.4464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.6873, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.4808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.6357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.4716, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.8341, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(63.5961, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7921, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.2803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7762, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.0912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.3264, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(97.7975, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.1807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.4294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6658, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.6272, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.4272, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39296187683284456, 'recall': 0.5374331550802139, 'f1': 0.45398080180688877, 'number': 748}, 'P': {'precision': 0.5472222222222223, 'recall': 0.5704633204633205, 'f1': 0.558601134215501, 'number': 1036}, 'overall_precision': 0.47218259629101283, 'overall_recall': 0.5566143497757847, 'overall_f1': 0.5109338821713403, 'overall_accuracy': 0.6724743628794801}\n",
            "------------EPOCH 24---------------\n",
            "Loss:  tensor(20.3811, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(51.6589, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.7123, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.5104, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(71.0845, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.6888, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2979, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.8919, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.6760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.2464, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.5971, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.5869, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.4715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.2739, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(56.2632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0894, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9650, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1800, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.0731, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.4586, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.6747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(89.3437, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.7636, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1554, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.1626, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.7905, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.39923224568138194, 'recall': 0.5561497326203209, 'f1': 0.46480446927374297, 'number': 748}, 'P': {'precision': 0.5545023696682464, 'recall': 0.5646718146718147, 'f1': 0.5595408895265422, 'number': 1036}, 'overall_precision': 0.47734859322842155, 'overall_recall': 0.5610986547085202, 'overall_f1': 0.5158464313321309, 'overall_accuracy': 0.6696111280333029}\n",
            "------------EPOCH 25---------------\n",
            "Loss:  tensor(18.7338, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.8192, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.0318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.5949, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(64.0208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.9573, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.6750, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.8856, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.2641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8907, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.0634, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3609, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.9482, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9825, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.9774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.8186, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.6245, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.8241, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.6059, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.0351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3287, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.4051, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(82.3376, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.3680, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.0353, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3690, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.6556, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3945841392649903, 'recall': 0.5454545454545454, 'f1': 0.45791245791245794, 'number': 748}, 'P': {'precision': 0.5553453169347209, 'recall': 0.5666023166023166, 'f1': 0.5609173435260392, 'number': 1036}, 'overall_precision': 0.47584887613582016, 'overall_recall': 0.5577354260089686, 'overall_f1': 0.5135483870967742, 'overall_accuracy': 0.6697329678139913}\n",
            "------------EPOCH 26---------------\n",
            "Loss:  tensor(17.4605, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.2628, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.3122, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.4598, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.7137, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9807, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.0138, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8076, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.7077, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8585, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7946, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.1307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.4995, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.3770, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2278, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8693, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3247, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.1292, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.7659, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.9385, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.2122, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.5553, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(74.6434, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.3701, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.1972, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4064, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.8061, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.9322, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3965183752417795, 'recall': 0.5481283422459893, 'f1': 0.4601571268237935, 'number': 748}, 'P': {'precision': 0.5561797752808989, 'recall': 0.5733590733590733, 'f1': 0.564638783269962, 'number': 1036}, 'overall_precision': 0.4776403425309229, 'overall_recall': 0.5627802690582959, 'overall_f1': 0.51672671127123, 'overall_accuracy': 0.6715605645243172}\n",
            "------------EPOCH 27---------------\n",
            "Loss:  tensor(16.5520, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(44.9360, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7528, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.6389, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.2957, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.2853, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2774, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.4771, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.9617, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5207, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.9783, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0193, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.2366, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.9410, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.5632, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.9429, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2208, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.5782, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4768, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4048, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4578, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2604, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(66.1929, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.6498, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.5357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.4028, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.6996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6019, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.394990366088632, 'recall': 0.5481283422459893, 'f1': 0.4591265397536394, 'number': 748}, 'P': {'precision': 0.5556594948550047, 'recall': 0.5733590733590733, 'f1': 0.5643705463182898, 'number': 1036}, 'overall_precision': 0.47650688182249645, 'overall_recall': 0.5627802690582959, 'overall_f1': 0.5160627088152145, 'overall_accuracy': 0.671195045182252}\n",
            "------------EPOCH 28---------------\n",
            "Loss:  tensor(15.8436, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.3535, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.5354, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.8753, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(48.4205, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.7095, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.4427, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.3516, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7760, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.5652, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.6588, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4671, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.8081, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.1502, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.1789, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.7148, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0244, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.0004, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2925, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.8500, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.0290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.3352, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(59.9457, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.3324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(9.0167, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.4324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9430, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.4538, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3915547024952015, 'recall': 0.5454545454545454, 'f1': 0.45586592178770946, 'number': 748}, 'P': {'precision': 0.5571161048689138, 'recall': 0.5743243243243243, 'f1': 0.5655893536121672, 'number': 1036}, 'overall_precision': 0.47535545023696685, 'overall_recall': 0.5622197309417041, 'overall_f1': 0.5151515151515151, 'overall_accuracy': 0.6713168849629404}\n",
            "------------EPOCH 29---------------\n",
            "Loss:  tensor(15.1539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.3916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4583, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4803, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(45.0773, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3641, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1614, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.5381, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.5633, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.1098, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.1325, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.3643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.4240, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3403, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.7616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9963, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.3631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.0616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.4326, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.3664, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.1695, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1532, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(81.3054, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(55.1544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.1125, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.7017, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.0115, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.37281553398058254, 'recall': 0.5133689839572193, 'f1': 0.4319460067491564, 'number': 748}, 'P': {'precision': 0.5200356188780053, 'recall': 0.5637065637065637, 'f1': 0.5409911996294581, 'number': 1036}, 'overall_precision': 0.44960520204366, 'overall_recall': 0.5426008968609866, 'overall_f1': 0.4917449834899669, 'overall_accuracy': 0.671195045182252}\n",
            "------------EPOCH 30---------------\n",
            "Loss:  tensor(18.8357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.9318, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.7643, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.9118, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.5937, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.3943, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.2757, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(34.7656, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.8597, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.0041, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.3631, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.3089, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3558, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.8320, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4868, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.4476, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.6620, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.7611, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.6394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.8938, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.2407, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.2740, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(47.7314, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(8.8056, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6917, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.4747, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.8293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3572815533980582, 'recall': 0.4919786096256685, 'f1': 0.41394825646794153, 'number': 748}, 'P': {'precision': 0.49955634427684115, 'recall': 0.5434362934362934, 'f1': 0.5205732778548312, 'number': 1036}, 'overall_precision': 0.4316179879462216, 'overall_recall': 0.5218609865470852, 'overall_f1': 0.4724689165186501, 'overall_accuracy': 0.6679662909940095}\n",
            "------------EPOCH 31---------------\n",
            "Loss:  tensor(18.2155, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.5090, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1649, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.9539, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.5172, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5821, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.8253, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.8351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.5039, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.9423, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7513, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.6187, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.8433, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.2698, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(50.7616, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.3124, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.9832, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1394, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3497, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.1953, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.0301, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.3916, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.4036, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(87.9612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.0544, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(11.2312, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.9910, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7047, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.8478, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3752362948960302, 'recall': 0.5307486631016043, 'f1': 0.43964562569213733, 'number': 748}, 'P': {'precision': 0.5321691176470589, 'recall': 0.5588803088803089, 'f1': 0.5451977401129944, 'number': 1036}, 'overall_precision': 0.4547996272134203, 'overall_recall': 0.547085201793722, 'overall_f1': 0.4966921119592876, 'overall_accuracy': 0.6658137882018479}\n",
            "------------EPOCH 32---------------\n",
            "Loss:  tensor(20.6831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.1831, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.2290, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.9530, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.6351, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.8460, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.3074, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.6357, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.5033, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3218, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.9094, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.7185, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.2049, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(49.7926, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.6797, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(16.8181, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2843, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.1057, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.7730, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.2468, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.3965, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(37.5882, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(80.3509, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.2446, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.5420, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6827, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6996, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.8662, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.390715667311412, 'recall': 0.5401069518716578, 'f1': 0.4534231200897868, 'number': 748}, 'P': {'precision': 0.544362292051756, 'recall': 0.5685328185328186, 'f1': 0.5561850802644004, 'number': 1036}, 'overall_precision': 0.46928166351606804, 'overall_recall': 0.5566143497757847, 'overall_f1': 0.5092307692307692, 'overall_accuracy': 0.6700984871560565}\n",
            "------------EPOCH 33---------------\n",
            "Loss:  tensor(22.6295, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.1392, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.3250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8030, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(46.9872, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.1184, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.8939, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.2462, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.0829, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.9199, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.6551, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.1808, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(60.1568, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.8021, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(42.6384, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.9191, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.2904, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(22.2267, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.3986, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(28.2623, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2984, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(38.5173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(65.3176, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.0324, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.9359, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2593, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.9883, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.6948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.38976377952755903, 'recall': 0.5294117647058824, 'f1': 0.44897959183673464, 'number': 748}, 'P': {'precision': 0.5461254612546126, 'recall': 0.5714285714285714, 'f1': 0.558490566037736, 'number': 1036}, 'overall_precision': 0.4704761904761905, 'overall_recall': 0.5538116591928252, 'overall_f1': 0.5087538619979401, 'overall_accuracy': 0.6716620976748908}\n",
            "------------EPOCH 34---------------\n",
            "Loss:  tensor(22.2794, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.2319, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.5956, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(27.3852, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(43.7250, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.6660, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.1909, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.8575, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.6084, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.7129, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(12.7340, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.3867, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(53.6521, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.5761, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.7294, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.2533, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.5126, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.7469, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.2249, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(23.1158, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.4327, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.6235, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.8988, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(58.7307, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(40.7715, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(15.3131, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.6804, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.2248, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.4944, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3901477832512315, 'recall': 0.5294117647058824, 'f1': 0.4492342597844583, 'number': 748}, 'P': {'precision': 0.5472043996333639, 'recall': 0.5762548262548263, 'f1': 0.5613540197461213, 'number': 1036}, 'overall_precision': 0.47150997150997154, 'overall_recall': 0.5566143497757847, 'overall_f1': 0.5105398457583548, 'overall_accuracy': 0.6711747385521373}\n",
            "------------EPOCH 35---------------\n",
            "Loss:  tensor(21.7550, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(33.1201, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.4595, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(26.0941, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(41.2293, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.9707, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(32.3275, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(24.9948, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(29.7648, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(30.7511, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(10.6542, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.4465, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.4987, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.2612, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(36.8456, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.9173, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(13.0820, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.4009, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(20.5933, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(21.8122, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(25.0086, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(19.0785, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(35.7222, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(52.2801, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(39.2592, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(14.8732, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(17.5031, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(18.7501, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "Loss:  tensor(31.5912, device='cuda:0', grad_fn=<DivBackward0>)\n",
            "{'C': {'precision': 0.3896484375, 'recall': 0.5334224598930482, 'f1': 0.4503386004514673, 'number': 748}, 'P': {'precision': 0.5508707607699358, 'recall': 0.5801158301158301, 'f1': 0.5651151857075692, 'number': 1036}, 'overall_precision': 0.4728132387706856, 'overall_recall': 0.5605381165919282, 'overall_f1': 0.5129520389843549, 'overall_accuracy': 0.6718245507158087}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mi7LRmfIYbR"
      },
      "source": [
        "### Rough -- Checking dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jD9kGw_svbXM",
        "outputId": "b2838b14-a51e-4129-bb8c-20b274bb7a1c"
      },
      "source": [
        "\" \".join(\" mY name is \".split())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mY name is'"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6-oVkplUCJh"
      },
      "source": [
        "def get_datasets():\n",
        "    train_dataset, valid_dataset, test_dataset = load_dataset(tokenizer=tokenizer,\n",
        "                                                              train_sz=80,\n",
        "                                                              test_sz=20,\n",
        "                                                              mask_tokens=discourse_markers)\n",
        "    return train_dataset, valid_dataset, test_dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3qMGQ5GOzbn"
      },
      "source": [
        "train_dataset, _, test_dataset = get_datasets()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bzdpyzvKIfM8",
        "outputId": "8531f009-4bed-4ea9-daa4-d76d023d52e9"
      },
      "source": [
        "for tokenized_threads, masked_threads, comp_type_labels, _ in test_dataset:\n",
        "    tokenized_threads, masked_threads, comp_type_labels = tokenized_threads[0], masked_threads[0], comp_type_labels[0]\n",
        "    for tokenized_thread, masked_thread, comp_type_label in zip(tokenized_threads, masked_threads, comp_type_labels):\n",
        "        print(comp_type_label[:100])\n",
        "        print(tokenized_thread[:100])\n",
        "        print(tokenizer.decode(tokenized_thread[:500]))\n",
        "        start, end = 0, 0\n",
        "        prev_type = \"other\"\n",
        "        i = 0\n",
        "        while i<tokenized_thread.shape[0]:\n",
        "            if comp_type_label[i]==ac_dict[\"O\"]:\n",
        "                if prev_type==\"other\":\n",
        "                    end += 1\n",
        "                else:\n",
        "                    print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                    print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                    start = i\n",
        "                    end = i\n",
        "                    prev_type=\"other\"\n",
        "                \n",
        "            if comp_type_label[i] in [ac_dict[\"B-C\"], ac_dict[\"B-P\"]]:\n",
        "                print(\"Component: \", tokenizer.decode(tokenized_thread[start:end+1]), \" of type: \", prev_type, tokenized_thread[start:end+1])\n",
        "                print(\"Masked Component: \", tokenizer.decode(masked_thread[start:end+1]), \" of type: \", prev_type, masked_thread[start:end+1])\n",
        "                start = i\n",
        "                end = i\n",
        "                prev_type = \"Claim\" if comp_type_label[i]==ac_dict[\"B-C\"] else \"Premise\"\n",
        "            \n",
        "            if comp_type_label[i] in [ac_dict[\"I-C\"], ac_dict[\"I-P\"]]:\n",
        "                end += 1\n",
        "            \n",
        "            i+=1\n",
        "        break\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0 1 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 2\n",
            " 2 2 2 2 0 0 0 0 1 2 2 2 2 2 2 2 2 2 2 2 2 3 4 4 4 4]\n",
            "[    0 18814   846    35  7978     9  1901    16   145   551   350   444\n",
            " 50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268\n",
            "   100  1819   923    84   481  1901    53     7   162  1437  8585    16\n",
            "    10   699   516   227 20203   110    78  8322   235    36  1437    22\n",
            "   270  1284 29384   328]\n",
            "<s>CMV: Freedom of speech is being taken too far [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE] I certainly value our free speech but to me there is a clear line between exercising your first amendment right (  \" President Obama sucks! \" etc ) and doing things that are known to be offensive to other cultures (  Satirical cartoons of prophets, assassinating leaders, etc ). [NEWLINE] [NEWLINE]  Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE] Sure, but that doesn't mean we condone the bully's actions and don't punish the bullies for acting. [NEWLINE] We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments. [NEWLINE]  Complete freedom in the expression of any idea, offensive or not, is a major element of that world. [NEWLINE] [NEWLINE]  If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us. [NEWLINE] [NEWLINE] [USER0] [NEWLINE] I certainly agree with your points - I didn't mean to imply that I was only for * * some * * freedom of speech. [NEWLINE] I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset. [NEWLINE] [NEWLINE]  Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"  [NEWLINE] Is that so hard? [NEWLINE] [NEWLINE] </s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n",
            "Component:  <s>CM  of type:  other [    0 18814]\n",
            "Masked Component:  <s>CM  of type:  other [    0 18814]\n",
            "Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Masked Component:  CMV: Freedom of speech is being taken too far  of type:  Claim [18814   846    35  7978     9  1901    16   145   551   350   444]\n",
            "Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Masked Component:  [USER0] [NEWLINE] [NEWLINE] In the last few weeks we've had two huge events happen in the world, both of which were caused by matters relating to \" freedom of speech. \" The first being the hacking of Sony over The Interview, and today the shooting at the offices of a satirical magazine in Paris. [NEWLINE]  of type:  other [50270 50268 50268  1121     5    94   367   688    52   348    56    80\n",
            "  1307  1061  1369    11     5   232     6   258     9    61    58  1726\n",
            "    30  3510  8941     7    22  3519     9  1901     4    22    20    78\n",
            "   145     5 11597     9  6366    81    20 21902     6     8   452     5\n",
            "  1094    23     5  4088     9    10 33937  4320    11  2201     4 50268]\n",
            "Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Masked Component:  I certainly value our free speech  of type:  Claim [ 100 1819  923   84  481 1901]\n",
            "Component:   but to me   of type:  other [  53    7  162 1437]\n",
            "Masked Component:  <mask> to me   of type:  other [50264     7   162  1437]\n",
            "Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Masked Component:  there is a clear line between exercising your first amendment right (   of type:  Claim [ 8585    16    10   699   516   227 20203   110    78  8322   235    36\n",
            "  1437]\n",
            "Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Masked Component:   \" President Obama sucks! \" etc  of type:  Premise [   22   270  1284 29384   328    22  4753]\n",
            "Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Masked Component:   ) and doing things that are known to be offensive to other cultures  of type:  Claim [ 4839     8   608   383    14    32   684     7    28  2555     7    97\n",
            " 13426]\n",
            "Component:   (   of type:  other [  36 1437]\n",
            "Masked Component:   (   of type:  other [  36 1437]\n",
            "Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Masked Component:   Satirical cartoons of prophets, assassinating leaders, etc  of type:  Premise [ 8918   853  3569 32162     9 43346     6 39257 15647   917     6  4753]\n",
            "Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Masked Component:   ). [NEWLINE] [NEWLINE]  of type:  other [32801 50268 50268]\n",
            "Component:   Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111    53   114    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Masked Component:   Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome.   of type:  Claim [ 6259    42    16    10  1099 33460   111 50264 50264    47   224   402\n",
            " 32726  2787     7    10 23934     8    47   120   110  8446  5836     6\n",
            "    47   197    33  5291    14  4258     4  1437]\n",
            "Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy - but if you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            "    53   114    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Masked Component:  [NEWLINE] [USER1] [NEWLINE] [NEWLINE] [STARTQ] Perhaps this is a bad analogy -<mask><mask> you say something antagonizing to a bully and you get your ass kicked, you should have anticipated that outcome. [ENDQ] [NEWLINE]  of type:  other [50268 50271 50268 50268 50265 32458    42    16    10  1099 33460   111\n",
            " 50264 50264    47   224   402 32726  2787     7    10 23934     8    47\n",
            "   120   110  8446  5836     6    47   197    33  5291    14  4258     4\n",
            " 50266 50268]\n",
            "Component:  Sure  of type:  Claim [32541]\n",
            "Masked Component:  Sure  of type:  Claim [32541]\n",
            "Component:  , but   of type:  other [   6   53 1437]\n",
            "Masked Component:  ,<mask>   of type:  other [    6 50264  1437]\n",
            "Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Masked Component:  that doesn't mean we condone the bully's actions and don't punish the bullies for acting  of type:  Claim [ 6025   630    75  1266    52 35005     5 23934    18  2163     8   218\n",
            "    75 15392     5 33969    13  3501]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Masked Component:  We want to live in a world without bullies, whether those bullies act on behalf of beliefs, religions, countries, or even out own governments  of type:  Premise [  170   236     7   697    11    10   232   396 33969     6   549   167\n",
            " 33969  1760    15  4137     9  9734     6 24664     6   749     6    50\n",
            "   190    66   308  3233]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Masked Component:   Complete freedom in the expression of any idea, offensive or not, is a major element of that world  of type:  Premise [18337  3519    11     5  8151     9   143  1114     6  2555    50    45\n",
            "     6    16    10   538  7510     9    14   232]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   If you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [ 318   47  236    7 1744    5 3519    7 1994   13  143    9  201    6\n",
            "   47   33    7 1744    5 3519    7 1994   13   70    9  201]\n",
            "Masked Component:  <mask> you want to protect the freedom to speak for any of us, you have to protect the freedom to speak for all of us  of type:  Claim [50264    47   236     7  1744     5  3519     7  1994    13   143     9\n",
            "   201     6    47    33     7  1744     5  3519     7  1994    13    70\n",
            "     9   201]\n",
            "Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE] [USER0] [NEWLINE]  of type:  other [    4 50268 50268 50270 50268]\n",
            "Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Masked Component:  I certainly agree with your points  of type:  Claim [ 100 1819 2854   19  110  332]\n",
            "Component:   -   of type:  other [ 111 1437]\n",
            "Masked Component:   -   of type:  other [ 111 1437]\n",
            "Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Masked Component:  I didn't mean to imply that I was only for * * some * * freedom of speech  of type:  Claim [  100   399    75  1266     7 25696    14    38    21   129    13  1009\n",
            "  1009   103  1009  1009  3519     9  1901]\n",
            "Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Masked Component:  . [NEWLINE]  of type:  other [    4 50268]\n",
            "Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech when certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901    77  1402  1134\n",
            "   120  4904]\n",
            "Masked Component:  I just think lately there's been a lot of antagonistic material being published and everyone cries freedom of speech<mask> certain groups get upset  of type:  Claim [  100    95   206 12056    89    18    57    10   319     9 32726  5580\n",
            "  1468   145  1027     8   961 25355  3519     9  1901 50264  1402  1134\n",
            "   120  4904]\n",
            "Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Masked Component:  . [NEWLINE] [NEWLINE]  of type:  other [    4 50268 50268]\n",
            "Component:   Why can't America be the bigger person and say \" Ok, we won't publish certain types of material, not because we're afraid of you but because we respect your views. \"   of type:  Premise [ 2612    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45   142\n",
            "    52   214  6023     9    47    53   142    52  2098   110  2728     4\n",
            "    22  1437]\n",
            "Masked Component:  <mask> can't America be the bigger person and say \" Ok, we won't publish certain types of material, not<mask> we're afraid of you<mask><mask> we respect your views. \"   of type:  Premise [50264    64    75   730    28     5  2671   621     8   224    22  5148\n",
            "     6    52   351    75 10732  1402  3505     9  1468     6    45 50264\n",
            "    52   214  6023     9    47 50264 50264    52  2098   110  2728     4\n",
            "    22  1437]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZnb1Nz-MX4u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2601bebd-3989-40a5-af7d-91d578651823"
      },
      "source": [
        "import re\n",
        "re.sub(r\"\\s*</claim>([^\\s])\", r\"</claim> \\1\", \"<claim>my name is </claim>jeevesh.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<claim>my name is</claim> jeevesh.'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d5UVJb5jy178"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}